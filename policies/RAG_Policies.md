# RAG System Policy Document

## Introduction

This document outlines the policies and procedures related to the operation of the Retriever-And-Generator (RAG) system. The RAG system is designed to provide accurate, relevant, and helpful responses to user queries by leveraging a two-pipeline architecture: Ingestion and Retrieval.

## Pipeline Descriptions

### 1. Ingestion Pipeline

In the Ingestion Pipeline, documents are split into smaller chunks, which are then stored in a vector database. This database is structured in such a way that semantically similar chunks are positioned closer to each other in the vector space. This organization facilitates more accurate and relevant retrieval of information in response to user queries.

#### Policy-1

- All documents ingested into the system must be accurate and up-to-date to ensure the reliability of the information provided.
- Document chunks must be clearly delineated and stored with appropriate metadata for easy retrieval and reference.

### 2. Retrieval Pipeline

In the Retrieval Pipeline, user queries are mapped onto the vector space, identifying the nearest vector/chunks in the vector database. These relevant chunks, along with the original question, are then passed to a Large Language Model (LLM) for synthesizing a coherent and informative response.

#### Policy-2

- The retrieval process must prioritize semantic relevance to ensure that the information retrieved closely aligns with the user's query.
- The LLM must generate answers that are directly based on the information contained within the retrieved document chunks to maintain honesty and accuracy.

## Honesty and Helpfulness Guidelines

To adhere to our core principles of Honesty and Helpfulness, the following guidelines are established:

### Honesty

The response provided by the LLM should be directly related to the retrieved document chunks, ensuring that the information is grounded in the source material.
The system must avoid generating speculative or unsubstantiated answers.

### Helpfulness

The answers generated by the LLM should be relevant and directly responsive to the user's question.
Along with the answer, the system should provide the retrieved chunks and at least one relevance score, helping users understand the basis of the response and its relevance to their query.

#### Policy-3

- Users must be informed of the source of the information provided in the response to ensure transparency.
- The system should include a mechanism for users to provide feedback on the relevance and accuracy of the responses, facilitating continuous improvement.
