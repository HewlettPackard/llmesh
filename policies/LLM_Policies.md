# LLM System Policy Document

## Introduction

This document establishes the policies and procedures for the operation of Large Language Models (LLM) within our system. The LLM is deployed to provide insightful, accurate, and contextually relevant responses based on user inputs. To maintain a high standard of quality and safety, this system incorporates guardrails that ensure all interactions with the LLM adhere to the Harmless principle, minimizing the risk of generating harmful or inappropriate content.

## LLM Operational Guidelines

### Guardrails Implementation

Guardrails are essential mechanisms put in place to ensure that both the prompts provided to the LLM and the content generated in response are aligned with our Harmless principles. These principles are designed to prevent the dissemination of harmful, misleading, or otherwise inappropriate information.

#### Policy-1

- All prompts passed to the LLM must be screened to prevent the submission of harmful, offensive, or inappropriate content.
- The LLM must be configured to reject or modify prompts that could lead to the generation of harmful responses.
- Responses generated by the LLM must be routinely monitored and reviewed to ensure they adhere to safety and content guidelines.

### Content Generation

The LLM is responsible for generating responses that are not only relevant and informative but also safe and appropriate for the user. This responsibility is paramount to maintaining the integrity and trustworthiness of the system.

#### Policy-2

- Responses must be factual, unbiased, and free from harmful content, including hate speech, biases, and misinformation.
- The LLM should provide sources or suggest further reading where possible to enhance the credibility and usefulness of the information provided.

### Harmless Principles Guidelines

To uphold the Harmless principles effectively, the following guidelines have been established:
The LLM should avoid generating content that could be considered harmful, offensive, or divisive.
Responses should be constructed with a focus on neutrality, respect, and factual accuracy.

#### Policy-3

- Any detected violation of the Harmless principles should be addressed immediately, with the offending content removed or corrected.
- Users should have the ability to report concerns or harmful responses, triggering a review process.

### Optional: LLM as a Reasoning Engine

In scenarios where the LLM serves as a Reasoning Engine, particularly in the orchestration of various tools, its utilization should be specifically restricted to those contexts.

#### Policy-4

- The LLM should be configured to recognize and respond appropriately to queries directly related to the toolset's functionalities.
- If a query is unrelated to the tool's specific functions, the LLM should politely decline to answer, guiding the user back to relevant topics.
- This selective response mechanism ensures the LLM remains focused on its intended purpose and avoids engaging in discussions outside its operational scope.
