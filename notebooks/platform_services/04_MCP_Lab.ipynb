{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1051b6",
   "metadata": {},
   "source": [
    "# Lab: Model Context Protocol (MCP)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Model Context Protocol (MCP) provides a standardized approach for communication between AI-powered applications, particularly Large Language Models (LLMs). This is to facilitate easier integrations between various frameworks, applications, and model services as they continue to grow in scale/complexity. \n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "\n",
    "MCP is designed to define a clear structure for managing LLM **prompts**, contextual information (which can include references to **tools** and **resources**), and data exchange when an application (a client) communicates with a model service (a server). This is not the complete specification of the protocol, but a simplified overview to illustrate the key concepts.\n",
    "\n",
    "Key objectives and potential benefits of using a protocol like MCP include:\n",
    "\n",
    "* **Improved Interoperability**: Facilitating connections between applications and a variety of model backends.\n",
    "* **Standardized Patterns**: Promoting common communication methods for leveraging **prompts**, requesting **tool** executions, and receiving model-generated content.\n",
    "* **Rich Context & Instruction Handling**: Providing defined ways to pass not just conversational history, but also detailed **prompts**, and specifications for how models should use available **tools** or access **resources**.\n",
    "* **Extensibility**: Allowing for future enhancements or specialized data exchanges within the protocol's framework.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "MCP interactions use a client-server architecture:\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/mcp-arch.png\" alt=\"MCP Architecture\" width=\"700\">\n",
    "</div>\n",
    "\n",
    "1.  **Client**: The component within a host application (such as an LLM-powered app or IDE) that communicates with the MCP server. The client forms and sends requests (such as tool invocations, resource lookups, or prompt template requests) to the server and processes the responses. The client typically acts on behalf of the application's AI model, enabling it to access external tools and resources via the MCP protocol.\n",
    "2.  **Server**: A service that exposes tools, resources, and prompt templates via MCP. It listens for client requests, manages tool execution or resource access as defined by the protocol, and returns structured responses. The MCP server does not typically host or run AI models itself; instead, it provides external capabilities that can be used by models running in the clientâ€™s host application.\n",
    "3.  **Transport Layer**: The underlying mechanism for message transmission. MCP defines the structure and interaction flow, with flexibility in transport mechanisms. MCP supports:\n",
    "    * Standard Input/Output (STDIO)\n",
    "    * Streamable HTTP (with optional Server-Sent Events)\n",
    "\n",
    "This separation allows flexibility in choosing a transport suitable for the application's environment.\n",
    "\n",
    "**Note:** The protocol itself does not define this but MCP Hosts are discrete components actually implementing the protocol. A MCP Host is the entity that includes server(s) and/or client(s) and can be implemented in any programming language. Servers and client connections can/often-are maintained within the same MCP host.\n",
    "\n",
    "#### Components\n",
    "\n",
    "MCP interactions involve several key elements and types of information:\n",
    "\n",
    "1.  **Messages**: Defined units of information exchanged between client and server using JSON-RPC format. Common patterns include:\n",
    "    * Request messages from client to server\n",
    "    * Response messages from server to client\n",
    "    * Notification messages that don't require responses\n",
    "    * Error messages for communicating issues\n",
    "\n",
    "2.  **Prompts**: The core input that guides the model's behavior. MCP facilitates powerful prompt management.\n",
    "\n",
    "3.  **Tools & Resources**: Mechanisms by which a model can interact with external systems or data:\n",
    "    * **Tools**: Pre-defined functions or capabilities that the model can be instructed to use (e.g., calculators, search engines, database query functions)\n",
    "    * **Resources**: External data sources or knowledge bases that the model might need to access to fulfill a request\n",
    "    * *Note: The specifics of how tools and resources are defined and invoked can vary*\n",
    "\n",
    "#### Protocol (Interaction Flow)\n",
    "\n",
    "A typical interaction sequence in MCP includes:\n",
    "\n",
    "1.  **Connection**: The client connects to the MCP server via the chosen transport mechanism.\n",
    "2.  **Initialization**: Client and server exchange initialization messages to establish capabilities and protocol version.\n",
    "3.  **Request**: The client sends a request message with the **prompt**, relevant context, parameters, and any **tool** or **resource** specifications.\n",
    "4.  **Processing**: The server processes the request, which may involve:\n",
    "    * Interpreting the prompt\n",
    "    * Invoking tools as specified\n",
    "    * Accessing resources as needed\n",
    "    * Generating model responses\n",
    "5.  **Response**: The server returns the response, either as a complete message or (if using Streamable HTTP) potentially as a stream of partial responses.\n",
    "6.  **Error Handling**: If issues arise, appropriate error messages are exchanged.\n",
    "7.  **Disconnection**: The connection may be closed or kept alive for subsequent interactions, depending on the transport and use case.\n",
    "\n",
    "This structured approach helps create more predictable and maintainable integrations with AI models, encompassing complex interactions involving prompts, tools, and resources while maintaining flexibility in implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8976a",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    "- Understand the core principles of the Model Context Protocol (MCP) and why it's valuable for LLM applications\n",
    "- Explore different transport mechanisms for MCP implementation (STDIO, SSE, and HTTP)\n",
    "- Implement basic MCP clients and servers using Python\n",
    "- Learn how to integrate LLMs with MCP\n",
    "- Gain practical experience with standardized protocols for AI communication\n",
    "\n",
    "This lab provides hands-on experience with an emerging standard in the AI ecosystem, giving you the skills to build more interoperable and flexible LLM-powered applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e09592",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "The following bullets must be ran prior to executing notebooks for running this lab:\n",
    "  1. uv installed and available on PATH\n",
    "      - Linux/MacOS:\n",
    "          - `curl -sSL https://get.uv.dev | sh`\n",
    "          - `source ~/.bashrc`\n",
    "          - `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
    "          - `source $HOME/.local/bin/env`\n",
    "\n",
    "      - Windows:\n",
    "          - `powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"`\n",
    "          - Completley close and re-open VSCode, essentailly just restart terminal.\n",
    "              - In my experience doing a terminal restart without closing VSCode does not work.\n",
    "\n",
    "  2. Python 3.12 installed and venv created\n",
    "      - `uv python install 3.12`\n",
    "      - `uv venv -p 3.12`\n",
    "      - Activate the venv:\n",
    "          - Linux/MacOS:\n",
    "              - `source .venv/bin/activate`\n",
    "          - Windows:\n",
    "              - `.\\.venv\\Scripts\\activate`\n",
    "      - `uv pip install ipykernel`\n",
    "\n",
    "  3. (Optional) In order to run the final labe section you need npx installed which requires Node.js (and npm if on linux)\n",
    "      - Windows:\n",
    "          - Simply use installer and include extra tools checkbox when installing Node.js https://nodejs.org/en/download.\n",
    "      - Linux/MacOS:\n",
    "          - Google how to install Node.js on your OS as there are many different ways to do it.\n",
    "\n",
    "  4. Select the venv as the notebook kernel\n",
    "  <div align=\"left\">\n",
    "    <img src=\"pictures/kernel.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "  </div>\n",
    "\n",
    "  \n",
    "\n",
    "**MUST restart Juypter kernel if automated install dependencies cell is ran**\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/restart.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafda488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31merror\u001b[39m\u001b[0m: /home/kinsy/llmesh/notebooks does not appear to be a Python project, as neither `pyproject.toml` nor `setup.py` are present in the directory\n",
      "Kernel will shut down to apply new pip installations, manual restart required.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"Install platform chat & mcp dependencies.\"\"\"\n",
    "!cd ../.. && uv pip install --quiet -e .[chat] mcp\n",
    "# Shut down the kernel so user must restart it to apply new pip installations.\n",
    "# This is a workaround for the fact that Jupyter does not automatically\n",
    "# pick up new installations in the current kernel.\n",
    "!echo \"Kernel will shut down to apply new pip installations, manual restart required.\"\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: atexit handler registered for automated cleanup on kernel exit.\n",
      "lablib.util process management functions are ready.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Setup background process manager for sse/http server demos.\"\"\"\n",
    "from notebooks.platform_services.lablib.bg_util import clear_port, start_background_process, kill_background_process, kill_all_background_processes\n",
    "from time import sleep\n",
    "import subprocess as sp\n",
    "print(\"lablib.util process management functions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping existing .env file.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set up the lab chat model.\"\"\"\n",
    "from notebooks.platform_services.lablib.env_util import set_services_env\n",
    "\n",
    "_, _, _ = set_services_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdd7bf",
   "metadata": {},
   "source": [
    "## STDIO Transport\n",
    "\n",
    "The STDIO (Standard Input/Output) transport is the primary MCP implementation that clients should support. It uses standard input and output streams to exchange MCP messages between client and server processes.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Subprocess Communication**: The client launches the MCP server as a subprocess\n",
    "- **Newline-Delimited Messages**: JSON-RPC messages are separated by newlines\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Process Launch**: The client launches the MCP server as a subprocess\n",
    "2. **Message Exchange**:\n",
    "   - The server reads JSON-RPC messages from its standard input (stdin)\n",
    "   - The server sends messages to its standard output (stdout)\n",
    "   - Messages are delimited by newlines and MUST NOT contain embedded newlines\n",
    "3. **Message Types**: Messages may be JSON-RPC requests, notifications, responses, or batched messages containing multiple requests/notifications\n",
    "4. **Logging**: The server MAY write UTF-8 strings to its standard error (stderr) for logging purposes, which clients MAY capture, forward, or ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef00f934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 31ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      "2025-05-12 18:06:56,243 - ATHON - DEBUG - Selected Langchain ChatOpenAI\n",
      "==================================================\n",
      "MCP Client-Server Demonstration\n",
      "==================================================\n",
      "\n",
      "==================== STEP: Initialization ====================\n",
      "Description: Establishing connection with MCP server\n",
      "\n",
      "âœ… Successfully connected to MCP server\n",
      "\n",
      "==================== STEP: Discovery ====================\n",
      "Description: Finding what the server offers\n",
      "\n",
      "\u001b[2;36m[05/12/25 18:06:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=820435;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=530867;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListToolsRequest                      \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=43137;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=542318;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListResourcesRequest                  \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=21423;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=804420;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListPromptsRequest                    \u001b[2m             \u001b[0m\n",
      "ðŸ”§ Found 2 tools: ['add', 'get_weather']\n",
      "ðŸ“„ Found 1 resources: [AnyUrl('config://settings')]\n",
      "ðŸ’¬ Found 1 prompts: ['system_prompt']\n",
      "\n",
      "==================== STEP: Tool Usage ====================\n",
      "Description: Calling different MCP tools to show functionality\n",
      "\n",
      "\n",
      "ðŸ”§ Using tool: add\n",
      "   Description: Add two numbers\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=921501;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=540704;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "\n",
      "--- Add Tool Response ---\n",
      "{'content': [{'annotations': None, 'text': '8.5', 'type': 'text'}],\n",
      " 'isError': False,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Basic addition using MCP tool', 'operation': '5 + 3.5'}\n",
      "--- End of Add Tool ---\n",
      "\n",
      "ðŸ”§ Using tool: get_weather\n",
      "   Description: Get current weather for a city\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=373005;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=115936;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "\n",
      "--- Weather Tool Response ---\n",
      "{'content': [{'annotations': None,\n",
      "              'text': 'Error executing tool get_weather: 1 validation error '\n",
      "                      'for get_weatherArguments\\n'\n",
      "                      'city\\n'\n",
      "                      '  Field required [type=missing, '\n",
      "                      \"input_value={'location': 'San Francisco'}, \"\n",
      "                      'input_type=dict]\\n'\n",
      "                      '    For further information visit '\n",
      "                      'https://errors.pydantic.dev/2.10/v/missing',\n",
      "              'type': 'text'}],\n",
      " 'isError': True,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Getting weather information', 'location': 'San Francisco'}\n",
      "--- End of Weather Tool ---\n",
      "\n",
      "==================== STEP: Resource Access ====================\n",
      "Description: Reading data from MCP resources\n",
      "\n",
      "\n",
      "ðŸ“„ Accessing resource: config://settings\n",
      "   Name: config://settings\n",
      "   Description: None\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=402781;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=636604;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ReadResourceRequest                   \u001b[2m             \u001b[0m\n",
      "\n",
      "--- Config Resource Response ---\n",
      "{'contents': [{'mimeType': 'text/plain',\n",
      "               'text': '{\\n'\n",
      "                       '  \"type\": \"text\",\\n'\n",
      "                       '  \"text\": \"{\\\\\"version\\\\\": \\\\\"1.0\\\\\", \\\\\"status\\\\\": '\n",
      "                       '\\\\\"active\\\\\"}\",\\n'\n",
      "                       '  \"annotations\": null\\n'\n",
      "                       '}',\n",
      "               'uri': AnyUrl('config://settings')}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Configuration data resource'}\n",
      "--- End of Config Resource ---\n",
      "\n",
      "==================== STEP: Prompt Usage ====================\n",
      "Description: Getting and using MCP prompts\n",
      "\n",
      "\n",
      "ðŸ’¬ Using prompt: system_prompt\n",
      "   Description: System prompt template for chat responses for one of two modes.\n",
      "\n",
      "        Mode 1: Translate user query to pirate language.\n",
      "        Mode 2: Answer the user query in pirate language.\n",
      "\n",
      "        Args:\n",
      "            user_query (str): The user's query.\n",
      "            mode (str): The mode of operation. Can be 'translate' or 'answer'.\n",
      "\n",
      "        Returns:\n",
      "            A list containing system instruction string and message objects.\n",
      "        \n",
      "\n",
      "   Trying mode: translate\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=223054;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=658378;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         GetPromptRequest                      \u001b[2m             \u001b[0m\n",
      "\n",
      "--- System Prompt (mode=translate) Response ---\n",
      "{'description': None,\n",
      " 'messages': [{'content': {'annotations': None,\n",
      "                           'text': 'You are a helpful assistant who is fluent '\n",
      "                                   \"in pirate language. Please 'translate' the \"\n",
      "                                   'following text.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'Hello, how are you today?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'Ahoy there, how be ye this fine day, '\n",
      "                                   'matey?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'I need to find a restaurant nearby.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': \"Arr, I be needin' to find a place to fill \"\n",
      "                                   'me belly nearby, savvy?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': \"What's the weather like today?\",\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Prompt in translate mode',\n",
      " 'message_count': 6,\n",
      " 'parameters': {'mode': 'translate',\n",
      "                'user_query': \"What's the weather like today?\"}}\n",
      "--- End of System Prompt (mode=translate) ---\n",
      "\n",
      "==================== STEP: MCP Prompt + LLM Integration ====================\n",
      "Description: Using MCP prompts directly with Language Model\n",
      "\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=229939;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=906579;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         GetPromptRequest                      \u001b[2m             \u001b[0m\n",
      "âœ… Retrieved prompt from MCP server\n",
      "   Prompt has 6 messages\n",
      "\n",
      "ðŸ“¤ Sending messages to LLM...\n",
      "   Converting 6 MCP messages to LangChain format\n",
      "2025-05-12 18:06:57,695 - ATHON - DEBUG - Prompt generated Arrr, I be needin' to secure a table fer supper, savvy?\n",
      "\n",
      "--- LLM Response Using MCP Prompt Response ---\n",
      "{'explanation': 'LLM processed the MCP prompt and generated a pirate '\n",
      "                'translation',\n",
      " 'llm_response': \"Arrr, I be needin' to secure a table fer supper, savvy?\",\n",
      " 'mcp_prompt': {'last_user_message': 'I need to book a dinner reservation',\n",
      "                'message_count': 6,\n",
      "                'name': 'system_prompt'}}\n",
      "--- End of LLM Response Using MCP Prompt ---\n",
      "\n",
      "==================== STEP: Tools + Prompts + LLM ====================\n",
      "Description: Complete workflow: MCP tools -> MCP prompts -> LLM\n",
      "\n",
      "1ï¸âƒ£ Getting data from MCP tool...\n",
      "\u001b[2;36m[05/12/25 18:06:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=865541;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=241147;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "   Tool result: 50.0\n",
      "\n",
      "2ï¸âƒ£ Getting translation prompt from MCP server...\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=534917;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=568093;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         GetPromptRequest                      \u001b[2m             \u001b[0m\n",
      "\n",
      "3ï¸âƒ£ Using MCP prompt with LLM...\n",
      "2025-05-12 18:06:58,289 - ATHON - DEBUG - Prompt generated The reckonin' be fifty, me hearty!\n",
      "\n",
      "--- Complete MCP Workflow Result Response ---\n",
      "{'explanation': 'Full workflow: MCP tool -> MCP prompt -> LLM processing',\n",
      " 'step_1_tool_result': '50.0',\n",
      " 'step_2_prompt_messages': 6,\n",
      " 'step_3_llm_result': \"The reckonin' be fifty, me hearty!\"}\n",
      "--- End of Complete MCP Workflow Result ---\n",
      "\n",
      "==================================================\n",
      "âœ… MCP Demo Complete!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the STDIO demo\"\"\"\n",
    "# Warning on windows the STDIO demo works but a exception that can be ignored is printed after the demo output\n",
    "!uv run lablib/mcp/stdio/client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdfe8d",
   "metadata": {},
   "source": [
    "## SSE Transport (Deprecated)\n",
    "\n",
    "Server-Sent Events (SSE) was originally used in MCP as part of the \"HTTP+SSE transport\" protocol, which has since been deprecated and replaced by the Streamable HTTP transport.\n",
    "\n",
    "### Important Note\n",
    "\n",
    "- The standalone \"HTTP+SSE transport\" from protocol version 2024-11-05 has been **deprecated**\n",
    "- SSE functionality has been **absorbed into** the new Streamable HTTP transport (as of protocol version 2025-03-26)\n",
    "- The Python SDK currently only supports SSE transport\n",
    "- This section covers the SSE implementation as used in current Python MCP implementations\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Server-to-Client Streaming**: Enables servers to push updates to clients over HTTP\n",
    "- **Event-Based Communication**: Uses SSE event format with `event`, `data`, and `id` fields\n",
    "- **HTTP-Compatible**: Works over standard HTTP connections\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Client establishes an HTTP connection requesting SSE stream\n",
    "2. Server sends initial `endpoint` event with connection details\n",
    "3. Server streams JSON-RPC messages as SSE events\n",
    "4. Client processes messages as they arrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2029b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Clearing port 8000...\n",
      "BackgroundProcessManager: No processes found using port 8000\n",
      "BackgroundProcessManager: Port 8000 was already clear.\n",
      "Starting SSE server: uv run lablib/mcp/sse/server.py\n",
      "BackgroundProcessManager: Started 'sse_server' (PID: 39275, PGID: 39275). Command: uv run lablib/mcp/sse/server.py\n",
      "BackgroundProcessManager: Process 'sse_server' started successfully. PID: 39275\n",
      "2025-05-12 18:07:09,603 - ATHON - DEBUG - Selected Langchain ChatOpenAI\n",
      "Connecting to SSE server at http://localhost:8000/math/sse...\n",
      "==================================================\n",
      "MCP Client-Server Demonstration\n",
      "==================================================\n",
      "\n",
      "==================== STEP: Initialization ====================\n",
      "Description: Establishing connection with MCP server\n",
      "\n",
      "âœ… Successfully connected to MCP server\n",
      "\n",
      "==================== STEP: Discovery ====================\n",
      "Description: Finding what the server offers\n",
      "\n",
      "ðŸ”§ Found 3 tools: ['add', 'get_weather', 'add_two']\n",
      "ðŸ“„ Found 1 resources: [AnyUrl('config://settings')]\n",
      "ðŸ’¬ Found 1 prompts: ['system_prompt']\n",
      "\n",
      "==================== STEP: Tool Usage ====================\n",
      "Description: Calling different MCP tools to show functionality\n",
      "\n",
      "\n",
      "ðŸ”§ Using tool: add\n",
      "   Description: Add two numbers\n",
      "\n",
      "--- Add Tool Response ---\n",
      "{'content': [{'annotations': None, 'text': '8.5', 'type': 'text'}],\n",
      " 'isError': False,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Basic addition using MCP tool', 'operation': '5 + 3.5'}\n",
      "--- End of Add Tool ---\n",
      "\n",
      "ðŸ”§ Using tool: get_weather\n",
      "   Description: Get current weather for a city\n",
      "\n",
      "--- Weather Tool Response ---\n",
      "{'content': [{'annotations': None,\n",
      "              'text': 'Error executing tool get_weather: 1 validation error '\n",
      "                      'for get_weatherArguments\\n'\n",
      "                      'city\\n'\n",
      "                      '  Field required [type=missing, '\n",
      "                      \"input_value={'location': 'San Francisco'}, \"\n",
      "                      'input_type=dict]\\n'\n",
      "                      '    For further information visit '\n",
      "                      'https://errors.pydantic.dev/2.10/v/missing',\n",
      "              'type': 'text'}],\n",
      " 'isError': True,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Getting weather information', 'location': 'San Francisco'}\n",
      "--- End of Weather Tool ---\n",
      "\n",
      "ðŸ”§ Using tool: add_two\n",
      "   Description: A simple add tool\n",
      "\n",
      "--- Add Two Tool Response ---\n",
      "{'content': [{'annotations': None, 'text': '2 + 2 = 4', 'type': 'text'}],\n",
      " 'isError': False,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Another addition tool variant', 'operation': '10 + 7'}\n",
      "--- End of Add Two Tool ---\n",
      "\n",
      "==================== STEP: Resource Access ====================\n",
      "Description: Reading data from MCP resources\n",
      "\n",
      "\n",
      "ðŸ“„ Accessing resource: config://settings\n",
      "   Name: config://settings\n",
      "   Description: None\n",
      "\n",
      "--- Config Resource Response ---\n",
      "{'contents': [{'mimeType': 'text/plain',\n",
      "               'text': '{\\n'\n",
      "                       '  \"type\": \"text\",\\n'\n",
      "                       '  \"text\": \"{\\\\\"version\\\\\": \\\\\"1.0\\\\\", \\\\\"status\\\\\": '\n",
      "                       '\\\\\"active\\\\\"}\",\\n'\n",
      "                       '  \"annotations\": null\\n'\n",
      "                       '}',\n",
      "               'uri': AnyUrl('config://settings')}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Configuration data resource'}\n",
      "--- End of Config Resource ---\n",
      "\n",
      "==================== STEP: Prompt Usage ====================\n",
      "Description: Getting and using MCP prompts\n",
      "\n",
      "\n",
      "ðŸ’¬ Using prompt: system_prompt\n",
      "   Description: System prompt template for chat responses for one of two modes.\n",
      "\n",
      "        Mode 1: Translate user query to pirate language.\n",
      "        Mode 2: Answer the user query in pirate language.\n",
      "\n",
      "        Args:\n",
      "            user_query (str): The user's query.\n",
      "            mode (str): The mode of operation. Can be 'translate' or 'answer'.\n",
      "\n",
      "        Returns:\n",
      "            A list containing system instruction string and message objects.\n",
      "        \n",
      "\n",
      "   Trying mode: translate\n",
      "\n",
      "--- System Prompt (mode=translate) Response ---\n",
      "{'description': None,\n",
      " 'messages': [{'content': {'annotations': None,\n",
      "                           'text': 'You are a helpful assistant who is fluent '\n",
      "                                   \"in pirate language. Please 'translate' the \"\n",
      "                                   'following text.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'Hello, how are you today?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'Ahoy there, how be ye this fine day, '\n",
      "                                   'matey?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'I need to find a restaurant nearby.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': \"Arr, I be needin' to find a place to fill \"\n",
      "                                   'me belly nearby, savvy?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': \"What's the weather like today?\",\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Prompt in translate mode',\n",
      " 'message_count': 6,\n",
      " 'parameters': {'mode': 'translate',\n",
      "                'user_query': \"What's the weather like today?\"}}\n",
      "--- End of System Prompt (mode=translate) ---\n",
      "\n",
      "==================== STEP: MCP Prompt + LLM Integration ====================\n",
      "Description: Using MCP prompts directly with Language Model\n",
      "\n",
      "âœ… Retrieved prompt from MCP server\n",
      "   Prompt has 6 messages\n",
      "\n",
      "ðŸ“¤ Sending messages to LLM...\n",
      "   Converting 6 MCP messages to LangChain format\n",
      "2025-05-12 18:07:10,712 - ATHON - DEBUG - Prompt generated Arrr, I be needin' to secure a fine table fer dinner, if ye please!\n",
      "\n",
      "--- LLM Response Using MCP Prompt Response ---\n",
      "{'explanation': 'LLM processed the MCP prompt and generated a pirate '\n",
      "                'translation',\n",
      " 'llm_response': \"Arrr, I be needin' to secure a fine table fer dinner, if ye \"\n",
      "                 'please!',\n",
      " 'mcp_prompt': {'last_user_message': 'I need to book a dinner reservation',\n",
      "                'message_count': 6,\n",
      "                'name': 'system_prompt'}}\n",
      "--- End of LLM Response Using MCP Prompt ---\n",
      "\n",
      "==================== STEP: Tools + Prompts + LLM ====================\n",
      "Description: Complete workflow: MCP tools -> MCP prompts -> LLM\n",
      "\n",
      "1ï¸âƒ£ Getting data from MCP tool...\n",
      "   Tool result: 50.0\n",
      "\n",
      "2ï¸âƒ£ Getting translation prompt from MCP server...\n",
      "\n",
      "3ï¸âƒ£ Using MCP prompt with LLM...\n",
      "2025-05-12 18:07:11,345 - ATHON - DEBUG - Prompt generated The reckonin' be 50.0, matey!\n",
      "\n",
      "--- Complete MCP Workflow Result Response ---\n",
      "{'explanation': 'Full workflow: MCP tool -> MCP prompt -> LLM processing',\n",
      " 'step_1_tool_result': '50.0',\n",
      " 'step_2_prompt_messages': 6,\n",
      " 'step_3_llm_result': \"The reckonin' be 50.0, matey!\"}\n",
      "--- End of Complete MCP Workflow Result ---\n",
      "\n",
      "==================================================\n",
      "âœ… MCP Demo Complete!\n",
      "==================================================\n",
      "BackgroundProcessManager: Attempting to terminate 'sse_server' (PID: 39275)...\n",
      "BackgroundProcessManager: Process group for 'sse_server' (PGID: 39275) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'sse_server'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the SSE demo\"\"\"\n",
    "# Clear port 8000 before starting the server\n",
    "clear_port(8000)\n",
    "\n",
    "sse_server_command = \"uv run lablib/mcp/sse/server.py\"\n",
    "sse_client_command = \"uv run lablib/mcp/sse/client.py\"\n",
    "\n",
    "print(f\"Starting SSE server: {sse_server_command}\")\n",
    "server_proc = start_background_process(\"sse_server\", sse_server_command)\n",
    "\n",
    "if not server_proc and server_proc.poll() is not None:\n",
    "    print(\"SSE server failed to start or exited prematurely. Client will not run.\")\n",
    "\n",
    "!uv run lablib/mcp/sse/client.py\n",
    "\n",
    "kill_background_process(\"sse_server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d436b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SSE server: uv run --active lablib/mcp/sse/server.py\n",
      "BackgroundProcessManager: Started 'sse_server' (PID: 39316, PGID: 39316). Command: uv run --active lablib/mcp/sse/server.py\n",
      "BackgroundProcessManager: Process 'sse_server' started successfully. PID: 39316\n",
      "Add the following to your llmesh/.vscode/mcp.json file:\n",
      "\n",
      "        \"my-test-mcp\": {\n",
      "            \"url\": \"http://localhost:8000/math/sse\"\n",
      "        }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the SSE Copilot demo\"\"\"\n",
    "sse_server_command = \"uv run --active lablib/mcp/sse/server.py\"\n",
    "\n",
    "print(f\"Starting SSE server: {sse_server_command}\")\n",
    "server_proc = start_background_process(\"sse_server\", sse_server_command)\n",
    "\n",
    "output = \"\"\"\n",
    "        \"my-test-mcp\": {\n",
    "            \"url\": \"http://localhost:8000/math/sse\"\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Add the following to your llmesh/.vscode/mcp.json file:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaf418fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Attempting to terminate 'sse_server' (PID: 39316)...\n",
      "BackgroundProcessManager: Process group for 'sse_server' (PGID: 39316) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'sse_server'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cleanup sse process\"\"\"\n",
    "kill_background_process(\"sse_server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb4c98",
   "metadata": {},
   "source": [
    "## Streamable HTTP Transport\n",
    "\n",
    "The Streamable HTTP transport is the current standard MCP transport mechanism, replacing the deprecated HTTP+SSE transport.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Current Standard**: The recommended transport mechanism for MCP\n",
    "- **HTTP-Based**: Uses standard HTTP POST and GET requests\n",
    "- **Optional Streaming**: Server can optionally use Server-Sent Events (SSE) for streaming responses\n",
    "- **Stateless or Stateful**: Supports both basic stateless servers and more feature-rich servers with sessions\n",
    "- **Multiple Connections**: Clients can maintain multiple concurrent connections\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Single Endpoint**: Server provides one HTTP endpoint that supports both POST and GET methods\n",
    "2. **Sending Messages**: \n",
    "   - Client sends JSON-RPC messages via HTTP POST to the MCP endpoint\n",
    "   - Client must include `Accept` header with both `application/json` and `text/event-stream`\n",
    "3. **Server Response Options**:\n",
    "   - **Simple Response**: Return `Content-Type: application/json` with a single JSON object\n",
    "   - **Streaming Response**: Return `Content-Type: text/event-stream` to initiate SSE streaming\n",
    "4. **Receiving Messages**: \n",
    "   - Client can issue HTTP GET to open SSE streams for server-initiated messages\n",
    "   - Server may send JSON-RPC requests and notifications via SSE\n",
    "5. **Session Management**: Server may optionally assign session IDs for stateful interactions\n",
    "\n",
    "**Note**: The MCP Inspector (shown below) provides a TypeScript client for testing Streamable HTTP connections.\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/inspector.png\" alt=\"Inspector screen shot\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10bf05e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Manually killing all registered processes...\n",
      "BackgroundProcessManager: Manual cleanup attempt complete.\n",
      "BackgroundProcessManager: Clearing port 8000...\n",
      "BackgroundProcessManager: No processes found using port 8000\n",
      "BackgroundProcessManager: Port 8000 was already clear.\n",
      "BackgroundProcessManager: Clearing port 6274...\n",
      "BackgroundProcessManager: No processes found using port 6274\n",
      "BackgroundProcessManager: Port 6274 was already clear.\n",
      "BackgroundProcessManager: Started 'mcp_server' (PID: 39509, PGID: 39509). Command: uv run lablib/mcp/streamable/server.py\n",
      "BackgroundProcessManager: Process 'mcp_server' started successfully. PID: 39509\n",
      "BackgroundProcessManager: Started 'mcp_inspector' (PID: 39511, PGID: 39511). Command: npx @modelcontextprotocol/inspector\n",
      "BackgroundProcessManager: Process 'mcp_inspector' started successfully. PID: 39511\n",
      "Starting MCP inspector and server... If inspector fails to start, try waiting for server setup or running the command manually.\n",
      "MCP inspector UI available at http://localhost:6274/\n",
      "MCP Streamable HTTP demo server available at http://localhost:8000/math/mcp\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the HTTP demo\"\"\"\n",
    "\n",
    "# Ensure background processes are killed before starting new ones\n",
    "kill_all_background_processes()\n",
    "\n",
    "# Ensure ports are clear before starting the server\n",
    "clear_port(8000)\n",
    "clear_port(6274)\n",
    "\n",
    "# Start the HTTP server\n",
    "start_background_process(\"mcp_server\", \"uv run lablib/mcp/streamable/server.py\")\n",
    "\n",
    "# Start the mcp inspector\n",
    "start_background_process(\"mcp_inspector\", \"npx @modelcontextprotocol/inspector\")\n",
    "\n",
    "print(\"Starting MCP inspector and server... If inspector fails to start, try waiting for server setup or running the command manually.\") \n",
    "print(\"MCP inspector UI available at http://localhost:6274/\")\n",
    "print(\"MCP Streamable HTTP demo server available at http://localhost:8000/math/mcp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfeaba8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Attempting to terminate 'mcp_server' (PID: 39509)...\n",
      "BackgroundProcessManager: Process group for 'mcp_server' (PGID: 39509) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'mcp_server'.\n",
      "BackgroundProcessManager: Attempting to terminate 'mcp_inspector' (PID: 39511)...\n",
      "BackgroundProcessManager: Process group for 'mcp_inspector' (PGID: 39511) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'mcp_inspector'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cleanup streamable-http processes\"\"\"\n",
    "kill_background_process(\"mcp_server\")\n",
    "kill_background_process(\"mcp_inspector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a075311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Manually killing all registered processes...\n",
      "BackgroundProcessManager: Manual cleanup attempt complete.\n"
     ]
    }
   ],
   "source": [
    "# To kill ALL processes managed by lablib.util (if you started others):\n",
    "kill_all_background_processes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
