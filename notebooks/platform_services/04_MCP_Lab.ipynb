{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1051b6",
   "metadata": {},
   "source": [
    "# Lab: Model Context Protocol (MCP)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Model Context Protocol (MCP) provides a standardized approach for communication between AI-powered applications, particularly Large Language Models (LLMs). This is to facilitate easier integrations between various frameworks, applications, and model services as they continue to grow in scale/complexity. \n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "\n",
    "MCP is designed to define a clear structure for managing LLM **prompts**, contextual information (which can include references to **tools** and **resources**), and data exchange when an application (a client) communicates with a model service (a server). This is not the complete specification of the protocol, but a simplified overview to illustrate the key concepts.\n",
    "\n",
    "Key objectives and potential benefits of using a protocol like MCP include:\n",
    "\n",
    "* **Improved Interoperability**: Facilitating connections between applications and a variety of model backends.\n",
    "* **Standardized Patterns**: Promoting common communication methods for leveraging **prompts**, requesting **tool** executions, and receiving model-generated content.\n",
    "* **Rich Context & Instruction Handling**: Providing defined ways to pass not just conversational history, but also detailed **prompts**, and specifications for how models should use available **tools** or access **resources**.\n",
    "* **Extensibility**: Allowing for future enhancements or specialized data exchanges within the protocol's framework.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "MCP interactions use a client-server architecture:\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/mcp-arch.png\" alt=\"MCP Architecture\" width=\"700\">\n",
    "</div>\n",
    "\n",
    "1.  **Client**: The component within a host application (such as an LLM-powered app or IDE) that communicates with the MCP server. The client forms and sends requests (such as tool invocations, resource lookups, or prompt template requests) to the server and processes the responses. The client typically acts on behalf of the application's AI model, enabling it to access external tools and resources via the MCP protocol.\n",
    "2.  **Server**: A service that exposes tools, resources, and prompt templates via MCP. It listens for client requests, manages tool execution or resource access as defined by the protocol, and returns structured responses. The MCP server does not typically host or run AI models itself; instead, it provides external capabilities that can be used by models running in the clientâ€™s host application.\n",
    "3.  **Transport Layer**: The underlying mechanism for message transmission. MCP defines the structure and interaction flow, with flexibility in transport mechanisms. MCP supports:\n",
    "    * Standard Input/Output (STDIO)\n",
    "    * Streamable HTTP (with optional Server-Sent Events)\n",
    "\n",
    "This separation allows flexibility in choosing a transport suitable for the application's environment.\n",
    "\n",
    "**Note:** The protocol itself does not define this but MCP Hosts are discrete components actually implementing the protocol. A MCP Host is the entity that includes server(s) and/or client(s) and can be implemented in any programming language. Servers and client connections can/often-are maintained within the same MCP host.\n",
    "\n",
    "#### Components\n",
    "\n",
    "MCP interactions involve several key elements and types of information:\n",
    "\n",
    "1.  **Messages**: Defined units of information exchanged between client and server using JSON-RPC format. Common patterns include:\n",
    "    * Request messages from client to server\n",
    "    * Response messages from server to client\n",
    "    * Notification messages that don't require responses\n",
    "    * Error messages for communicating issues\n",
    "\n",
    "2.  **Prompts**: The core input that guides the model's behavior. MCP facilitates powerful prompt management.\n",
    "\n",
    "3.  **Tools & Resources**: Mechanisms by which a model can interact with external systems or data:\n",
    "    * **Tools**: Pre-defined functions or capabilities that the model can be instructed to use (e.g., calculators, search engines, database query functions)\n",
    "    * **Resources**: External data sources or knowledge bases that the model might need to access to fulfill a request\n",
    "    * *Note: The specifics of how tools and resources are defined and invoked can vary*\n",
    "\n",
    "#### Protocol (Interaction Flow)\n",
    "\n",
    "A typical interaction sequence in MCP includes:\n",
    "\n",
    "1.  **Connection**: The client connects to the MCP server via the chosen transport mechanism.\n",
    "2.  **Initialization**: Client and server exchange initialization messages to establish capabilities and protocol version.\n",
    "3.  **Request**: The client sends a request message with the **prompt**, relevant context, parameters, and any **tool** or **resource** specifications.\n",
    "4.  **Processing**: The server processes the request, which may involve:\n",
    "    * Interpreting the prompt\n",
    "    * Invoking tools as specified\n",
    "    * Accessing resources as needed\n",
    "    * Generating model responses\n",
    "5.  **Response**: The server returns the response, either as a complete message or (if using Streamable HTTP) potentially as a stream of partial responses.\n",
    "6.  **Error Handling**: If issues arise, appropriate error messages are exchanged.\n",
    "7.  **Disconnection**: The connection may be closed or kept alive for subsequent interactions, depending on the transport and use case.\n",
    "\n",
    "This structured approach helps create more predictable and maintainable integrations with AI models, encompassing complex interactions involving prompts, tools, and resources while maintaining flexibility in implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8976a",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    "- Understand the core principles of the Model Context Protocol (MCP) and why it's valuable for LLM applications\n",
    "- Explore different transport mechanisms for MCP implementation (STDIO, SSE, and HTTP)\n",
    "- Implement basic MCP clients and servers using Python\n",
    "- Learn how to integrate LLMs with MCP\n",
    "- Gain practical experience with standardized protocols for AI communication\n",
    "\n",
    "This lab provides hands-on experience with an emerging standard in the AI ecosystem, giving you the skills to build more interoperable and flexible LLM-powered applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e09592",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "The following bullets must be ran prior to executing notebooks for running this lab:\n",
    "  1. uv installed and available on PATH\n",
    "      - Linux/MacOS:\n",
    "          - `curl -sSL https://get.uv.dev | sh`\n",
    "          - `source ~/.bashrc`\n",
    "          - `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
    "          - `source $HOME/.local/bin/env`\n",
    "\n",
    "      - Windows:\n",
    "          - `powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"`\n",
    "          - Completley close and re-open VSCode, essentailly just restart terminal.\n",
    "              - In my experience doing a terminal restart without closing VSCode does not work.\n",
    "\n",
    "  2. Python 3.12 installed and venv created\n",
    "      - `uv python install 3.12`\n",
    "      - `uv venv -p 3.12`\n",
    "      - Activate the venv:\n",
    "          - Linux/MacOS:\n",
    "              - `source .venv/bin/activate`\n",
    "          - Windows:\n",
    "              - `.\\.venv\\Scripts\\activate`\n",
    "      - `uv pip install ipykernel`\n",
    "\n",
    "  3. (Optional) In order to run the final labe section you need npx installed which requires Node.js (and npm if on linux)\n",
    "      - Windows:\n",
    "          - Simply use installer and include extra tools checkbox when installing Node.js https://nodejs.org/en/download.\n",
    "      - Linux/MacOS:\n",
    "          - Google how to install Node.js on your OS as there are many different ways to do it.\n",
    "\n",
    "  4. Select the venv as the notebook kernel\n",
    "  <div align=\"left\">\n",
    "    <img src=\"pictures/kernel.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "  </div>\n",
    "\n",
    "  \n",
    "\n",
    "**MUST restart Juypter kernel if automated install dependencies cell is ran**\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/restart.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eafda488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Kernel will shut down to apply new pip installations, manual restart required.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"Install platform chat & mcp dependencies.\"\"\"\n",
    "!cd ../.. && uv pip install --quiet -e .[chat] mcp\n",
    "# Shut down the kernel so user must restart it to apply new pip installations.\n",
    "# This is a workaround for the fact that Jupyter does not automatically\n",
    "# pick up new installations in the current kernel.\n",
    "!echo \"Kernel will shut down to apply new pip installations, manual restart required.\"\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda5fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: atexit handler registered for automated cleanup on kernel exit.\n",
      "lablib.util process management functions are ready.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Setup background process manager for sse/http server demos.\"\"\"\n",
    "from notebooks.platform_services.lablib.bg_util import clear_port, start_background_process, kill_background_process, kill_all_background_processes\n",
    "from time import sleep\n",
    "import subprocess as sp\n",
    "print(\"lablib.util process management functions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6018be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env has been overwritten.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set up the lab chat model.\"\"\"\n",
    "from notebooks.platform_services.lablib.env_util import set_services_env\n",
    "\n",
    "_, _, _ = set_services_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdd7bf",
   "metadata": {},
   "source": [
    "## STDIO Transport\n",
    "\n",
    "The STDIO (Standard Input/Output) transport is the primary MCP implementation that clients should support. It uses standard input and output streams to exchange MCP messages between client and server processes.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Subprocess Communication**: The client launches the MCP server as a subprocess\n",
    "- **Newline-Delimited Messages**: JSON-RPC messages are separated by newlines\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Process Launch**: The client launches the MCP server as a subprocess\n",
    "2. **Message Exchange**:\n",
    "   - The server reads JSON-RPC messages from its standard input (stdin)\n",
    "   - The server sends messages to its standard output (stdout)\n",
    "   - Messages are delimited by newlines and MUST NOT contain embedded newlines\n",
    "3. **Message Types**: Messages may be JSON-RPC requests, notifications, responses, or batched messages containing multiple requests/notifications\n",
    "4. **Logging**: The server MAY write UTF-8 strings to its standard error (stderr) for logging purposes, which clients MAY capture, forward, or ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef00f934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 31ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      "2025-05-12 18:06:56,243 - ATHON - DEBUG - Selected Langchain ChatOpenAI\n",
      "==================================================\n",
      "MCP Client-Server Demonstration\n",
      "==================================================\n",
      "\n",
      "==================== STEP: Initialization ====================\n",
      "Description: Establishing connection with MCP server\n",
      "\n",
      "âœ… Successfully connected to MCP server\n",
      "\n",
      "==================== STEP: Discovery ====================\n",
      "Description: Finding what the server offers\n",
      "\n",
      "\u001b[2;36m[05/12/25 18:06:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=820435;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=530867;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListToolsRequest                      \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=43137;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=542318;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListResourcesRequest                  \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=21423;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=804420;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListPromptsRequest                    \u001b[2m             \u001b[0m\n",
      "ðŸ”§ Found 2 tools: ['add', 'get_weather']\n",
      "ðŸ“„ Found 1 resources: [AnyUrl('config://settings')]\n",
      "ðŸ’¬ Found 1 prompts: ['system_prompt']\n",
      "\n",
      "==================== STEP: Tool Usage ====================\n",
      "Description: Calling different MCP tools to show functionality\n",
      "\n",
      "\n",
      "ðŸ”§ Using tool: add\n",
      "   Description: Add two numbers\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=921501;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=540704;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "\n",
      "--- Add Tool Response ---\n",
      "{'content': [{'annotations': None, 'text': '8.5', 'type': 'text'}],\n",
      " 'isError': False,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Basic addition using MCP tool', 'operation': '5 + 3.5'}\n",
      "--- End of Add Tool ---\n",
      "\n",
      "ðŸ”§ Using tool: get_weather\n",
      "   Description: Get current weather for a city\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=373005;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=115936;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "\n",
      "--- Weather Tool Response ---\n",
      "{'content': [{'annotations': None,\n",
      "              'text': 'Error executing tool get_weather: 1 validation error '\n",
      "                      'for get_weatherArguments\\n'\n",
      "                      'city\\n'\n",
      "                      '  Field required [type=missing, '\n",
      "                      \"input_value={'location': 'San Francisco'}, \"\n",
      "                      'input_type=dict]\\n'\n",
      "                      '    For further information visit '\n",
      "                      'https://errors.pydantic.dev/2.10/v/missing',\n",
      "              'type': 'text'}],\n",
      " 'isError': True,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Getting weather information', 'location': 'San Francisco'}\n",
      "--- End of Weather Tool ---\n",
      "\n",
      "==================== STEP: Resource Access ====================\n",
      "Description: Reading data from MCP resources\n",
      "\n",
      "\n",
      "ðŸ“„ Accessing resource: config://settings\n",
      "   Name: config://settings\n",
      "   Description: None\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=402781;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=636604;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ReadResourceRequest                   \u001b[2m             \u001b[0m\n",
      "\n",
      "--- Config Resource Response ---\n",
      "{'contents': [{'mimeType': 'text/plain',\n",
      "               'text': '{\\n'\n",
      "                       '  \"type\": \"text\",\\n'\n",
      "                       '  \"text\": \"{\\\\\"version\\\\\": \\\\\"1.0\\\\\", \\\\\"status\\\\\": '\n",
      "                       '\\\\\"active\\\\\"}\",\\n'\n",
      "                       '  \"annotations\": null\\n'\n",
      "                       '}',\n",
      "               'uri': AnyUrl('config://settings')}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Configuration data resource'}\n",
      "--- End of Config Resource ---\n",
      "\n",
      "==================== STEP: Prompt Usage ====================\n",
      "Description: Getting and using MCP prompts\n",
      "\n",
      "\n",
      "ðŸ’¬ Using prompt: system_prompt\n",
      "   Description: System prompt template for chat responses for one of two modes.\n",
      "\n",
      "        Mode 1: Translate user query to pirate language.\n",
      "        Mode 2: Answer the user query in pirate language.\n",
      "\n",
      "        Args:\n",
      "            user_query (str): The user's query.\n",
      "            mode (str): The mode of operation. Can be 'translate' or 'answer'.\n",
      "\n",
      "        Returns:\n",
      "            A list containing system instruction string and message objects.\n",
      "        \n",
      "\n",
      "   Trying mode: translate\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=223054;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=658378;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         GetPromptRequest                      \u001b[2m             \u001b[0m\n",
      "\n",
      "--- System Prompt (mode=translate) Response ---\n",
      "{'description': None,\n",
      " 'messages': [{'content': {'annotations': None,\n",
      "                           'text': 'You are a helpful assistant who is fluent '\n",
      "                                   \"in pirate language. Please 'translate' the \"\n",
      "                                   'following text.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'Hello, how are you today?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'Ahoy there, how be ye this fine day, '\n",
      "                                   'matey?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'I need to find a restaurant nearby.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': \"Arr, I be needin' to find a place to fill \"\n",
      "                                   'me belly nearby, savvy?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': \"What's the weather like today?\",\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Prompt in translate mode',\n",
      " 'message_count': 6,\n",
      " 'parameters': {'mode': 'translate',\n",
      "                'user_query': \"What's the weather like today?\"}}\n",
      "--- End of System Prompt (mode=translate) ---\n",
      "\n",
      "==================== STEP: MCP Prompt + LLM Integration ====================\n",
      "Description: Using MCP prompts directly with Language Model\n",
      "\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=229939;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=906579;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         GetPromptRequest                      \u001b[2m             \u001b[0m\n",
      "âœ… Retrieved prompt from MCP server\n",
      "   Prompt has 6 messages\n",
      "\n",
      "ðŸ“¤ Sending messages to LLM...\n",
      "   Converting 6 MCP messages to LangChain format\n",
      "2025-05-12 18:06:57,695 - ATHON - DEBUG - Prompt generated Arrr, I be needin' to secure a table fer supper, savvy?\n",
      "\n",
      "--- LLM Response Using MCP Prompt Response ---\n",
      "{'explanation': 'LLM processed the MCP prompt and generated a pirate '\n",
      "                'translation',\n",
      " 'llm_response': \"Arrr, I be needin' to secure a table fer supper, savvy?\",\n",
      " 'mcp_prompt': {'last_user_message': 'I need to book a dinner reservation',\n",
      "                'message_count': 6,\n",
      "                'name': 'system_prompt'}}\n",
      "--- End of LLM Response Using MCP Prompt ---\n",
      "\n",
      "==================== STEP: Tools + Prompts + LLM ====================\n",
      "Description: Complete workflow: MCP tools -> MCP prompts -> LLM\n",
      "\n",
      "1ï¸âƒ£ Getting data from MCP tool...\n",
      "\u001b[2;36m[05/12/25 18:06:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=865541;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=241147;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "   Tool result: 50.0\n",
      "\n",
      "2ï¸âƒ£ Getting translation prompt from MCP server...\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=534917;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=568093;file:///home/kinsy/llmesh/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py#545\u001b\\\u001b[2m545\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         GetPromptRequest                      \u001b[2m             \u001b[0m\n",
      "\n",
      "3ï¸âƒ£ Using MCP prompt with LLM...\n",
      "2025-05-12 18:06:58,289 - ATHON - DEBUG - Prompt generated The reckonin' be fifty, me hearty!\n",
      "\n",
      "--- Complete MCP Workflow Result Response ---\n",
      "{'explanation': 'Full workflow: MCP tool -> MCP prompt -> LLM processing',\n",
      " 'step_1_tool_result': '50.0',\n",
      " 'step_2_prompt_messages': 6,\n",
      " 'step_3_llm_result': \"The reckonin' be fifty, me hearty!\"}\n",
      "--- End of Complete MCP Workflow Result ---\n",
      "\n",
      "==================================================\n",
      "âœ… MCP Demo Complete!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the STDIO demo\"\"\"\n",
    "# Warning on windows the STDIO demo works but a exception that can be ignored is printed after the demo output\n",
    "!uv run lablib/mcp/stdio/client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdfe8d",
   "metadata": {},
   "source": [
    "## SSE Transport (Deprecated)\n",
    "\n",
    "Server-Sent Events (SSE) was originally used in MCP as part of the \"HTTP+SSE transport\" protocol, which has since been deprecated and replaced by the Streamable HTTP transport.\n",
    "\n",
    "### Important Note\n",
    "\n",
    "- The standalone \"HTTP+SSE transport\" from protocol version 2024-11-05 has been **deprecated**\n",
    "- SSE functionality has been **absorbed into** the new Streamable HTTP transport (as of protocol version 2025-03-26)\n",
    "- The Python SDK currently only supports SSE transport\n",
    "- This section covers the SSE implementation as used in current Python MCP implementations\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Server-to-Client Streaming**: Enables servers to push updates to clients over HTTP\n",
    "- **Event-Based Communication**: Uses SSE event format with `event`, `data`, and `id` fields\n",
    "- **HTTP-Compatible**: Works over standard HTTP connections\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Client establishes an HTTP connection requesting SSE stream\n",
    "2. Server sends initial `endpoint` event with connection details\n",
    "3. Server streams JSON-RPC messages as SSE events\n",
    "4. Client processes messages as they arrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2029b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Clearing port 8000...\n",
      "BackgroundProcessManager: No processes found using port 8000\n",
      "BackgroundProcessManager: Port 8000 was already clear.\n",
      "Starting SSE server: uv run lablib/mcp/sse/server.py\n",
      "BackgroundProcessManager: Started 'sse_server' (PID: 39275, PGID: 39275). Command: uv run lablib/mcp/sse/server.py\n",
      "BackgroundProcessManager: Process 'sse_server' started successfully. PID: 39275\n",
      "2025-05-12 18:07:09,603 - ATHON - DEBUG - Selected Langchain ChatOpenAI\n",
      "Connecting to SSE server at http://localhost:8000/math/sse...\n",
      "==================================================\n",
      "MCP Client-Server Demonstration\n",
      "==================================================\n",
      "\n",
      "==================== STEP: Initialization ====================\n",
      "Description: Establishing connection with MCP server\n",
      "\n",
      "âœ… Successfully connected to MCP server\n",
      "\n",
      "==================== STEP: Discovery ====================\n",
      "Description: Finding what the server offers\n",
      "\n",
      "ðŸ”§ Found 3 tools: ['add', 'get_weather', 'add_two']\n",
      "ðŸ“„ Found 1 resources: [AnyUrl('config://settings')]\n",
      "ðŸ’¬ Found 1 prompts: ['system_prompt']\n",
      "\n",
      "==================== STEP: Tool Usage ====================\n",
      "Description: Calling different MCP tools to show functionality\n",
      "\n",
      "\n",
      "ðŸ”§ Using tool: add\n",
      "   Description: Add two numbers\n",
      "\n",
      "--- Add Tool Response ---\n",
      "{'content': [{'annotations': None, 'text': '8.5', 'type': 'text'}],\n",
      " 'isError': False,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Basic addition using MCP tool', 'operation': '5 + 3.5'}\n",
      "--- End of Add Tool ---\n",
      "\n",
      "ðŸ”§ Using tool: get_weather\n",
      "   Description: Get current weather for a city\n",
      "\n",
      "--- Weather Tool Response ---\n",
      "{'content': [{'annotations': None,\n",
      "              'text': 'Error executing tool get_weather: 1 validation error '\n",
      "                      'for get_weatherArguments\\n'\n",
      "                      'city\\n'\n",
      "                      '  Field required [type=missing, '\n",
      "                      \"input_value={'location': 'San Francisco'}, \"\n",
      "                      'input_type=dict]\\n'\n",
      "                      '    For further information visit '\n",
      "                      'https://errors.pydantic.dev/2.10/v/missing',\n",
      "              'type': 'text'}],\n",
      " 'isError': True,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Getting weather information', 'location': 'San Francisco'}\n",
      "--- End of Weather Tool ---\n",
      "\n",
      "ðŸ”§ Using tool: add_two\n",
      "   Description: A simple add tool\n",
      "\n",
      "--- Add Two Tool Response ---\n",
      "{'content': [{'annotations': None, 'text': '2 + 2 = 4', 'type': 'text'}],\n",
      " 'isError': False,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Another addition tool variant', 'operation': '10 + 7'}\n",
      "--- End of Add Two Tool ---\n",
      "\n",
      "==================== STEP: Resource Access ====================\n",
      "Description: Reading data from MCP resources\n",
      "\n",
      "\n",
      "ðŸ“„ Accessing resource: config://settings\n",
      "   Name: config://settings\n",
      "   Description: None\n",
      "\n",
      "--- Config Resource Response ---\n",
      "{'contents': [{'mimeType': 'text/plain',\n",
      "               'text': '{\\n'\n",
      "                       '  \"type\": \"text\",\\n'\n",
      "                       '  \"text\": \"{\\\\\"version\\\\\": \\\\\"1.0\\\\\", \\\\\"status\\\\\": '\n",
      "                       '\\\\\"active\\\\\"}\",\\n'\n",
      "                       '  \"annotations\": null\\n'\n",
      "                       '}',\n",
      "               'uri': AnyUrl('config://settings')}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Configuration data resource'}\n",
      "--- End of Config Resource ---\n",
      "\n",
      "==================== STEP: Prompt Usage ====================\n",
      "Description: Getting and using MCP prompts\n",
      "\n",
      "\n",
      "ðŸ’¬ Using prompt: system_prompt\n",
      "   Description: System prompt template for chat responses for one of two modes.\n",
      "\n",
      "        Mode 1: Translate user query to pirate language.\n",
      "        Mode 2: Answer the user query in pirate language.\n",
      "\n",
      "        Args:\n",
      "            user_query (str): The user's query.\n",
      "            mode (str): The mode of operation. Can be 'translate' or 'answer'.\n",
      "\n",
      "        Returns:\n",
      "            A list containing system instruction string and message objects.\n",
      "        \n",
      "\n",
      "   Trying mode: translate\n",
      "\n",
      "--- System Prompt (mode=translate) Response ---\n",
      "{'description': None,\n",
      " 'messages': [{'content': {'annotations': None,\n",
      "                           'text': 'You are a helpful assistant who is fluent '\n",
      "                                   \"in pirate language. Please 'translate' the \"\n",
      "                                   'following text.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'Hello, how are you today?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'Ahoy there, how be ye this fine day, '\n",
      "                                   'matey?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'I need to find a restaurant nearby.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': \"Arr, I be needin' to find a place to fill \"\n",
      "                                   'me belly nearby, savvy?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': \"What's the weather like today?\",\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Prompt in translate mode',\n",
      " 'message_count': 6,\n",
      " 'parameters': {'mode': 'translate',\n",
      "                'user_query': \"What's the weather like today?\"}}\n",
      "--- End of System Prompt (mode=translate) ---\n",
      "\n",
      "==================== STEP: MCP Prompt + LLM Integration ====================\n",
      "Description: Using MCP prompts directly with Language Model\n",
      "\n",
      "âœ… Retrieved prompt from MCP server\n",
      "   Prompt has 6 messages\n",
      "\n",
      "ðŸ“¤ Sending messages to LLM...\n",
      "   Converting 6 MCP messages to LangChain format\n",
      "2025-05-12 18:07:10,712 - ATHON - DEBUG - Prompt generated Arrr, I be needin' to secure a fine table fer dinner, if ye please!\n",
      "\n",
      "--- LLM Response Using MCP Prompt Response ---\n",
      "{'explanation': 'LLM processed the MCP prompt and generated a pirate '\n",
      "                'translation',\n",
      " 'llm_response': \"Arrr, I be needin' to secure a fine table fer dinner, if ye \"\n",
      "                 'please!',\n",
      " 'mcp_prompt': {'last_user_message': 'I need to book a dinner reservation',\n",
      "                'message_count': 6,\n",
      "                'name': 'system_prompt'}}\n",
      "--- End of LLM Response Using MCP Prompt ---\n",
      "\n",
      "==================== STEP: Tools + Prompts + LLM ====================\n",
      "Description: Complete workflow: MCP tools -> MCP prompts -> LLM\n",
      "\n",
      "1ï¸âƒ£ Getting data from MCP tool...\n",
      "   Tool result: 50.0\n",
      "\n",
      "2ï¸âƒ£ Getting translation prompt from MCP server...\n",
      "\n",
      "3ï¸âƒ£ Using MCP prompt with LLM...\n",
      "2025-05-12 18:07:11,345 - ATHON - DEBUG - Prompt generated The reckonin' be 50.0, matey!\n",
      "\n",
      "--- Complete MCP Workflow Result Response ---\n",
      "{'explanation': 'Full workflow: MCP tool -> MCP prompt -> LLM processing',\n",
      " 'step_1_tool_result': '50.0',\n",
      " 'step_2_prompt_messages': 6,\n",
      " 'step_3_llm_result': \"The reckonin' be 50.0, matey!\"}\n",
      "--- End of Complete MCP Workflow Result ---\n",
      "\n",
      "==================================================\n",
      "âœ… MCP Demo Complete!\n",
      "==================================================\n",
      "BackgroundProcessManager: Attempting to terminate 'sse_server' (PID: 39275)...\n",
      "BackgroundProcessManager: Process group for 'sse_server' (PGID: 39275) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'sse_server'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the SSE demo\"\"\"\n",
    "# Clear port 8000 before starting the server\n",
    "clear_port(8000)\n",
    "\n",
    "sse_server_command = \"uv run lablib/mcp/sse/server.py\"\n",
    "sse_client_command = \"uv run lablib/mcp/sse/client.py\"\n",
    "\n",
    "print(f\"Starting SSE server: {sse_server_command}\")\n",
    "server_proc = start_background_process(\"sse_server\", sse_server_command)\n",
    "\n",
    "if not server_proc and server_proc.poll() is not None:\n",
    "    print(\"SSE server failed to start or exited prematurely. Client will not run.\")\n",
    "\n",
    "!uv run lablib/mcp/sse/client.py\n",
    "\n",
    "kill_background_process(\"sse_server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d436b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SSE server: uv run --active lablib/mcp/sse/server.py\n",
      "BackgroundProcessManager: Started 'sse_server' (PID: 39316, PGID: 39316). Command: uv run --active lablib/mcp/sse/server.py\n",
      "BackgroundProcessManager: Process 'sse_server' started successfully. PID: 39316\n",
      "Add the following to your llmesh/.vscode/mcp.json file:\n",
      "\n",
      "        \"my-test-mcp\": {\n",
      "            \"url\": \"http://localhost:8000/math/sse\"\n",
      "        }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the SSE Copilot demo\"\"\"\n",
    "sse_server_command = \"uv run --active lablib/mcp/sse/server.py\"\n",
    "\n",
    "print(f\"Starting SSE server: {sse_server_command}\")\n",
    "server_proc = start_background_process(\"sse_server\", sse_server_command)\n",
    "\n",
    "output = \"\"\"\n",
    "        \"my-test-mcp\": {\n",
    "            \"url\": \"http://localhost:8000/math/sse\"\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Add the following to your llmesh/.vscode/mcp.json file:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaf418fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Attempting to terminate 'sse_server' (PID: 39316)...\n",
      "BackgroundProcessManager: Process group for 'sse_server' (PGID: 39316) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'sse_server'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cleanup sse process\"\"\"\n",
    "kill_background_process(\"sse_server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb4c98",
   "metadata": {},
   "source": [
    "## Streamable HTTP Transport\n",
    "\n",
    "The Streamable HTTP transport is the current standard MCP transport mechanism, replacing the deprecated HTTP+SSE transport.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Current Standard**: The recommended transport mechanism for MCP\n",
    "- **HTTP-Based**: Uses standard HTTP POST and GET requests\n",
    "- **Optional Streaming**: Server can optionally use Server-Sent Events (SSE) for streaming responses\n",
    "- **Stateless or Stateful**: Supports both basic stateless servers and more feature-rich servers with sessions\n",
    "- **Multiple Connections**: Clients can maintain multiple concurrent connections\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Single Endpoint**: Server provides one HTTP endpoint that supports both POST and GET methods\n",
    "2. **Sending Messages**: \n",
    "   - Client sends JSON-RPC messages via HTTP POST to the MCP endpoint\n",
    "   - Client must include `Accept` header with both `application/json` and `text/event-stream`\n",
    "3. **Server Response Options**:\n",
    "   - **Simple Response**: Return `Content-Type: application/json` with a single JSON object\n",
    "   - **Streaming Response**: Return `Content-Type: text/event-stream` to initiate SSE streaming\n",
    "4. **Receiving Messages**: \n",
    "   - Client can issue HTTP GET to open SSE streams for server-initiated messages\n",
    "   - Server may send JSON-RPC requests and notifications via SSE\n",
    "5. **Session Management**: Server may optionally assign session IDs for stateful interactions\n",
    "\n",
    "**Note**: The MCP Inspector (shown below) provides a TypeScript client for testing Streamable HTTP connections.\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/inspector.png\" alt=\"Inspector screen shot\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rk85c7lmd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Clearing port 8000...\n",
      "BackgroundProcessManager: No processes found using port 8000\n",
      "BackgroundProcessManager: Port 8000 was already clear.\n",
      "Starting Streamable HTTP server: uv run lablib/mcp/streamable/server.py\n",
      "BackgroundProcessManager: Started 'http_server' (PID: 4246, PGID: 4246). Command: uv run lablib/mcp/streamable/server.py\n",
      "BackgroundProcessManager: Process 'http_server' started successfully. PID: 4246\n",
      "\n",
      "Running Streamable HTTP client...\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/kinsy/llmesh/.venv3119` does not match the project environment path `/home/kinsy/llmesh/.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "2025-07-14 19:42:40,071 - ATHON - DEBUG - Selected Langchain ChatOpenAI\n",
      "Connecting to Streamable HTTP server at http://localhost:8000/math/mcp...\n",
      "==================================================\n",
      "MCP Client-Server Demonstration\n",
      "==================================================\n",
      "\n",
      "==================== STEP: Initialization ====================\n",
      "Description: Establishing connection with MCP server\n",
      "\n",
      "âœ… Successfully connected to MCP server\n",
      "\n",
      "==================== STEP: Discovery ====================\n",
      "Description: Finding what the server offers\n",
      "\n",
      "ðŸ”§ Found 3 tools: ['add_two', 'add', 'get_weather']\n",
      "ðŸ“„ Found 1 resources: [AnyUrl('config://settings')]\n",
      "ðŸ’¬ Found 1 prompts: ['system_prompt']\n",
      "\n",
      "==================== STEP: Tool Usage ====================\n",
      "Description: Calling different MCP tools to show functionality\n",
      "\n",
      "\n",
      "ðŸ”§ Using tool: add_two\n",
      "   Description: A simple add tool\n",
      "\n",
      "--- Add Two Tool Response ---\n",
      "{'content': [{'annotations': None,\n",
      "              'meta': None,\n",
      "              'text': '2 + 2 = 4',\n",
      "              'type': 'text'}],\n",
      " 'isError': False,\n",
      " 'meta': None,\n",
      " 'structuredContent': {'result': '2 + 2 = 4'}}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Another addition tool variant', 'operation': '10 + 7'}\n",
      "--- End of Add Two Tool ---\n",
      "\n",
      "ðŸ”§ Using tool: add\n",
      "   Description: Add two numbers\n",
      "\n",
      "--- Add Tool Response ---\n",
      "{'content': [{'annotations': None,\n",
      "              'meta': None,\n",
      "              'text': '8.5',\n",
      "              'type': 'text'}],\n",
      " 'isError': False,\n",
      " 'meta': None,\n",
      " 'structuredContent': {'result': 8.5}}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Basic addition using MCP tool', 'operation': '5 + 3.5'}\n",
      "--- End of Add Tool ---\n",
      "\n",
      "ðŸ”§ Using tool: get_weather\n",
      "   Description: Get current weather for a city\n",
      "\n",
      "--- Weather Tool Response ---\n",
      "{'content': [{'annotations': None,\n",
      "              'meta': None,\n",
      "              'text': 'Error executing tool get_weather: 1 validation error '\n",
      "                      'for get_weatherArguments\\n'\n",
      "                      'city\\n'\n",
      "                      '  Field required [type=missing, '\n",
      "                      \"input_value={'location': 'San Francisco'}, \"\n",
      "                      'input_type=dict]\\n'\n",
      "                      '    For further information visit '\n",
      "                      'https://errors.pydantic.dev/2.11/v/missing',\n",
      "              'type': 'text'}],\n",
      " 'isError': True,\n",
      " 'meta': None,\n",
      " 'structuredContent': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Getting weather information', 'location': 'San Francisco'}\n",
      "--- End of Weather Tool ---\n",
      "\n",
      "==================== STEP: Resource Access ====================\n",
      "Description: Reading data from MCP resources\n",
      "\n",
      "\n",
      "ðŸ“„ Accessing resource: config://settings\n",
      "   Name: config_resource\n",
      "   Description: Server configuration\n",
      "\n",
      "--- Config Resource Response ---\n",
      "{'contents': [{'meta': None,\n",
      "               'mimeType': 'text/plain',\n",
      "               'text': '{\\n'\n",
      "                       '  \"type\": \"text\",\\n'\n",
      "                       '  \"text\": \"{\\\\\"version\\\\\": \\\\\"1.0\\\\\", \\\\\"status\\\\\": '\n",
      "                       '\\\\\"active\\\\\"}\",\\n'\n",
      "                       '  \"annotations\": null,\\n'\n",
      "                       '  \"_meta\": null\\n'\n",
      "                       '}',\n",
      "               'uri': AnyUrl('config://settings')}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Configuration data resource'}\n",
      "--- End of Config Resource ---\n",
      "\n",
      "==================== STEP: Prompt Usage ====================\n",
      "Description: Getting and using MCP prompts\n",
      "\n",
      "\n",
      "ðŸ’¬ Using prompt: system_prompt\n",
      "   Description: System prompt template for chat responses for one of two modes.\n",
      "\n",
      "        Mode 1: Translate user query to pirate language.\n",
      "        Mode 2: Answer the user query in pirate language.\n",
      "\n",
      "        Args:\n",
      "            user_query (str): The user's query.\n",
      "            mode (str): The mode of operation. Can be 'translate' or 'answer'.\n",
      "\n",
      "        Returns:\n",
      "            A list containing system instruction string and message objects.\n",
      "        \n",
      "\n",
      "   Trying mode: translate\n",
      "\n",
      "--- System Prompt (mode=translate) Response ---\n",
      "{'description': 'System prompt template for chat responses for one of two '\n",
      "                'modes.\\n'\n",
      "                '\\n'\n",
      "                '        Mode 1: Translate user query to pirate language.\\n'\n",
      "                '        Mode 2: Answer the user query in pirate language.\\n'\n",
      "                '\\n'\n",
      "                '        Args:\\n'\n",
      "                \"            user_query (str): The user's query.\\n\"\n",
      "                '            mode (str): The mode of operation. Can be '\n",
      "                \"'translate' or 'answer'.\\n\"\n",
      "                '\\n'\n",
      "                '        Returns:\\n'\n",
      "                '            A list containing system instruction string and '\n",
      "                'message objects.\\n'\n",
      "                '        ',\n",
      " 'messages': [{'content': {'annotations': None,\n",
      "                           'meta': None,\n",
      "                           'text': 'You are a helpful assistant who is fluent '\n",
      "                                   \"in pirate language. Please 'translate' the \"\n",
      "                                   'following text.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'meta': None,\n",
      "                           'text': 'Hello, how are you today?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'meta': None,\n",
      "                           'text': 'Ahoy there, how be ye this fine day, '\n",
      "                                   'matey?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'meta': None,\n",
      "                           'text': 'I need to find a restaurant nearby.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'meta': None,\n",
      "                           'text': \"Arr, I be needin' to find a place to fill \"\n",
      "                                   'me belly nearby, savvy?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'meta': None,\n",
      "                           'text': \"What's the weather like today?\",\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Prompt in translate mode',\n",
      " 'message_count': 6,\n",
      " 'parameters': {'mode': 'translate',\n",
      "                'user_query': \"What's the weather like today?\"}}\n",
      "--- End of System Prompt (mode=translate) ---\n",
      "\n",
      "==================== STEP: MCP Prompt + LLM Integration ====================\n",
      "Description: Using MCP prompts directly with Language Model\n",
      "\n",
      "âœ… Retrieved prompt from MCP server\n",
      "   Prompt has 6 messages\n",
      "\n",
      "ðŸ“¤ Sending messages to LLM...\n",
      "   Converting 6 MCP messages to LangChain format\n",
      "2025-07-14 19:42:41,872 - ATHON - DEBUG - Prompt generated I be needin' to secure a table fer dinner, if ye please!\n",
      "\n",
      "--- LLM Response Using MCP Prompt Response ---\n",
      "{'explanation': 'LLM processed the MCP prompt and generated a pirate '\n",
      "                'translation',\n",
      " 'llm_response': \"I be needin' to secure a table fer dinner, if ye please!\",\n",
      " 'mcp_prompt': {'last_user_message': 'I need to book a dinner reservation',\n",
      "                'message_count': 6,\n",
      "                'name': 'system_prompt'}}\n",
      "--- End of LLM Response Using MCP Prompt ---\n",
      "\n",
      "==================== STEP: Tools + Prompts + LLM ====================\n",
      "Description: Complete workflow: MCP tools -> MCP prompts -> LLM\n",
      "\n",
      "1ï¸âƒ£ Getting data from MCP tool...\n",
      "   Tool result: 50.0\n",
      "\n",
      "2ï¸âƒ£ Getting translation prompt from MCP server...\n",
      "\n",
      "3ï¸âƒ£ Using MCP prompt with LLM...\n",
      "2025-07-14 19:42:42,871 - ATHON - DEBUG - Prompt generated The tally be 50.0, matey!\n",
      "\n",
      "--- Complete MCP Workflow Result Response ---\n",
      "{'explanation': 'Full workflow: MCP tool -> MCP prompt -> LLM processing',\n",
      " 'step_1_tool_result': '50.0',\n",
      " 'step_2_prompt_messages': 6,\n",
      " 'step_3_llm_result': 'The tally be 50.0, matey!'}\n",
      "--- End of Complete MCP Workflow Result ---\n",
      "\n",
      "==================================================\n",
      "âœ… MCP Demo Complete!\n",
      "==================================================\n",
      "BackgroundProcessManager: Attempting to terminate 'http_server' (PID: 4246)...\n",
      "BackgroundProcessManager: Process group for 'http_server' (PGID: 4246) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'http_server'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the Streamable HTTP client demo using Python MCP SDK\"\"\"\n",
    "# Clear port 8000 before starting the server\n",
    "clear_port(8000)\n",
    "\n",
    "http_server_command = \"uv run lablib/mcp/streamable/server.py\"\n",
    "http_client_command = \"uv run lablib/mcp/streamable/client.py\"\n",
    "\n",
    "print(f\"Starting Streamable HTTP server: {http_server_command}\")\n",
    "server_proc = start_background_process(\"http_server\", http_server_command)\n",
    "\n",
    "if not server_proc or server_proc.poll() is not None:\n",
    "    print(\"HTTP server failed to start or exited prematurely. Client will not run.\")\n",
    "else:\n",
    "    # Give server time to start\n",
    "    sleep(2)\n",
    "    print(\"\\nRunning Streamable HTTP client...\")\n",
    "    !uv run lablib/mcp/streamable/client.py\n",
    "\n",
    "kill_background_process(\"http_server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10bf05e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Manually killing all registered processes...\n",
      "BackgroundProcessManager: Manual cleanup attempt complete.\n",
      "BackgroundProcessManager: Clearing port 8000...\n",
      "BackgroundProcessManager: No processes found using port 8000\n",
      "BackgroundProcessManager: Port 8000 was already clear.\n",
      "BackgroundProcessManager: Clearing port 6274...\n",
      "BackgroundProcessManager: No processes found using port 6274\n",
      "BackgroundProcessManager: Port 6274 was already clear.\n",
      "BackgroundProcessManager: Started 'mcp_server' (PID: 39509, PGID: 39509). Command: uv run lablib/mcp/streamable/server.py\n",
      "BackgroundProcessManager: Process 'mcp_server' started successfully. PID: 39509\n",
      "BackgroundProcessManager: Started 'mcp_inspector' (PID: 39511, PGID: 39511). Command: npx @modelcontextprotocol/inspector\n",
      "BackgroundProcessManager: Process 'mcp_inspector' started successfully. PID: 39511\n",
      "Starting MCP inspector and server... If inspector fails to start, try waiting for server setup or running the command manually.\n",
      "MCP inspector UI available at http://localhost:6274/\n",
      "MCP Streamable HTTP demo server available at http://localhost:8000/math/mcp\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the HTTP demo\"\"\"\n",
    "\n",
    "# Ensure background processes are killed before starting new ones\n",
    "kill_all_background_processes()\n",
    "\n",
    "# Ensure ports are clear before starting the server\n",
    "clear_port(8000)\n",
    "clear_port(6274)\n",
    "\n",
    "# Start the HTTP server\n",
    "start_background_process(\"mcp_server\", \"uv run lablib/mcp/streamable/server.py\")\n",
    "\n",
    "# Start the mcp inspector\n",
    "start_background_process(\"mcp_inspector\", \"npx @modelcontextprotocol/inspector\")\n",
    "\n",
    "print(\"Starting MCP inspector and server... If inspector fails to start, try waiting for server setup or running the command manually.\") \n",
    "print(\"MCP inspector UI available at http://localhost:6274/\")\n",
    "print(\"MCP Streamable HTTP demo server available at http://localhost:8000/math/mcp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfeaba8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Attempting to terminate 'mcp_server' (PID: 39509)...\n",
      "BackgroundProcessManager: Process group for 'mcp_server' (PGID: 39509) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'mcp_server'.\n",
      "BackgroundProcessManager: Attempting to terminate 'mcp_inspector' (PID: 39511)...\n",
      "BackgroundProcessManager: Process group for 'mcp_inspector' (PGID: 39511) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'mcp_inspector'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cleanup streamable-http processes\"\"\"\n",
    "kill_background_process(\"mcp_server\")\n",
    "kill_background_process(\"mcp_inspector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a075311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Manually killing all registered processes...\n",
      "BackgroundProcessManager: Manual cleanup attempt complete.\n"
     ]
    }
   ],
   "source": [
    "# To kill ALL processes managed by lablib.util (if you started others):\n",
    "kill_all_background_processes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z99awufwln",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you've learned about the Model Context Protocol (MCP) and its various transport mechanisms:\n",
    "\n",
    "1. **STDIO Transport**: Direct subprocess communication for local MCP servers\n",
    "2. **SSE Transport** (Deprecated): Server-Sent Events for streaming responses\n",
    "3. **Streamable HTTP Transport**: The current standard with optional SSE streaming\n",
    "4. **OAuth Authentication**: Securing MCP servers with Auth0 and OAuth 2.0\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- MCP provides a standardized way for AI applications to communicate\n",
    "- The Python SDK now includes built-in support for streamable HTTP clients\n",
    "- OAuth authentication can be easily integrated for production deployments\n",
    "- The protocol supports tools, resources, and prompts as core primitives\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore creating your own MCP tools and resources\n",
    "- Implement real Auth0 authentication for production use\n",
    "- Build MCP clients that integrate with your AI applications\n",
    "- Contribute to the MCP ecosystem with new server implementations\n",
    "\n",
    "For more information, visit the [MCP Python SDK documentation](https://github.com/modelcontextprotocol/python-sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zywpgrmp48h",
   "metadata": {},
   "source": [
    "## OAuth Authentication with Auth0\n",
    "\n",
    "The Python MCP SDK supports OAuth 2.0 authentication, allowing you to secure your MCP servers with industry-standard authentication. This section demonstrates how to implement Auth0 authentication for MCP servers.\n",
    "\n",
    "### âš ï¸ Requirements\n",
    "\n",
    "This lab section requires additional dependencies:\n",
    "- **PyJWT**: For JWT token verification (`pip install PyJWT`)\n",
    "- **aiohttp**: For the callback server (`pip install aiohttp`)\n",
    "\n",
    "**Important**: The authentication implementation in this lab is now fully functional with real JWT verification, callback server, and token refresh capabilities.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **OAuth 2.0 with PKCE**: Secure authorization code flow with Proof Key for Code Exchange\n",
    "- **Real JWT Verification**: Server-side validation of JWT tokens using PyJWT and Auth0's JWKS\n",
    "- **Automatic Callback Handling**: Built-in callback server for authorization code exchange\n",
    "- **Token Refresh**: Automatic token refresh when tokens approach expiration\n",
    "- **Scoped Access**: Fine-grained permissions using OAuth scopes\n",
    "- **RFC 9728 Compliance**: Implements OAuth discovery endpoints for standardized authentication\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Server Configuration**: The MCP server is configured with an Auth0 token verifier that:\n",
    "   - Fetches JWKS (JSON Web Key Set) from Auth0\n",
    "   - Validates JWT signatures using RS256 algorithm\n",
    "   - Verifies token claims (audience, issuer, expiration)\n",
    "   - Extracts scopes and user information\n",
    "\n",
    "2. **Client Authentication**: \n",
    "   - Client starts a local callback server (auto-finds available port)\n",
    "   - Opens Auth0 login in browser with PKCE parameters\n",
    "   - User authenticates and authorizes the application\n",
    "   - Auth0 redirects to local callback server\n",
    "   - Client automatically exchanges authorization code for tokens\n",
    "\n",
    "3. **Token Management**:\n",
    "   - Tokens are stored in memory (production should use secure storage)\n",
    "   - Client monitors token expiration\n",
    "   - Automatically refreshes tokens before expiration\n",
    "   - Handles refresh token rotation if configured in Auth0\n",
    "\n",
    "4. **Authorized Access**: Client can now call protected MCP tools with valid tokens\n",
    "\n",
    "### Setting Up Auth0\n",
    "\n",
    "To use Auth0 authentication in production:\n",
    "\n",
    "1. Create an Auth0 account at https://auth0.com\n",
    "2. Create a new Application (type: \"Single Page Application\")\n",
    "3. Configure allowed callback URLs: `http://localhost:8080/callback` (and other ports if needed)\n",
    "4. Set up an API in Auth0 with appropriate scopes\n",
    "5. Configure environment variables:\n",
    "   - `AUTH0_DOMAIN`: Your Auth0 domain (e.g., `dev-example.auth0.com`)\n",
    "   - `AUTH0_AUDIENCE`: API identifier from Auth0\n",
    "   - `AUTH0_CLIENT_ID`: Your Auth0 application's Client ID\n",
    "\n",
    "**Note**: This demo uses example Auth0 credentials. For production use, you must:\n",
    "- Replace with your own Auth0 configuration\n",
    "- Ensure your Auth0 application is properly configured\n",
    "- Add appropriate scopes and permissions\n",
    "- Implement secure token storage (not in-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fta354bhqwt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Installing OAuth authentication dependencies...\n",
      "This may take a moment...\n",
      "\n",
      "âœ… Dependencies installed successfully!\n",
      "   - PyJWT: For JWT token verification\n",
      "   - aiohttp: For OAuth callback server\n",
      "\n",
      "ðŸ’¡ You're now ready to run the Auth0 authentication demo.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Install required dependencies for OAuth authentication\"\"\"\n",
    "print(\"ðŸ“¦ Installing OAuth authentication dependencies...\")\n",
    "print(\"This may take a moment...\")\n",
    "\n",
    "# Install PyJWT for JWT token verification\n",
    "!uv pip install --quiet PyJWT\n",
    "\n",
    "# Install aiohttp for callback server\n",
    "!uv pip install --quiet aiohttp\n",
    "\n",
    "print(\"\\nâœ… Dependencies installed successfully!\")\n",
    "print(\"   - PyJWT: For JWT token verification\")\n",
    "print(\"   - aiohttp: For OAuth callback server\")\n",
    "print(\"\\nðŸ’¡ You're now ready to run the Auth0 authentication demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "lwd2tfxn5v9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Manually killing all registered processes...\n",
      "BackgroundProcessManager: Manual cleanup attempt complete.\n",
      "BackgroundProcessManager: Clearing port 8003...\n",
      "BackgroundProcessManager: No processes found using port 8003\n",
      "BackgroundProcessManager: Port 8003 was already clear.\n",
      "ðŸ” Starting Auth0-enabled MCP server...\n",
      "Command: uv run lablib/mcp/auth/server.py\n",
      "BackgroundProcessManager: Started 'auth_server' (PID: 77518, PGID: 77518). Command: uv run lablib/mcp/auth/server.py\n",
      "BackgroundProcessManager: Process 'auth_server' started successfully. PID: 77518\n",
      "âœ… Auth0-enabled server started successfully!\n",
      "\n",
      "ðŸ“ Server endpoints:\n",
      "   - MCP endpoint: http://localhost:8003/secure/mcp\n",
      "   - Discovery: http://localhost:8003/.well-known/oauth-protected-resource\n",
      "   - Auth config: http://localhost:8003/auth/config\n",
      "\n",
      "âš ï¸  Note: This server uses real JWT verification.\n",
      "For production use, configure your own Auth0 credentials.\n",
      "\n",
      "ðŸ’¡ To test the authenticated client, run the next cell.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the Auth0 authentication demo\"\"\"\n",
    "# Clean up any existing processes and clear ports\n",
    "kill_all_background_processes()\n",
    "clear_port(8003)\n",
    "\n",
    "auth_server_command = \"uv run lablib/mcp/auth/server.py\"\n",
    "\n",
    "print(\"ðŸ” Starting Auth0-enabled MCP server...\")\n",
    "print(f\"Command: {auth_server_command}\")\n",
    "server_proc = start_background_process(\"auth_server\", auth_server_command)\n",
    "\n",
    "if not server_proc or server_proc.poll() is not None:\n",
    "    print(\"âŒ Auth0 server failed to start. Check the error messages above.\")\n",
    "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
    "    print(\"   1. Ensure PyJWT is installed: pip install PyJWT\")\n",
    "    print(\"   2. Check for port conflicts on 8003\")\n",
    "    print(\"   3. Review error messages above\")\n",
    "else:\n",
    "    print(\"âœ… Auth0-enabled server started successfully!\")\n",
    "    print(\"\\nðŸ“ Server endpoints:\")\n",
    "    print(\"   - MCP endpoint: http://localhost:8003/secure/mcp\")\n",
    "    print(\"   - Discovery: http://localhost:8003/.well-known/oauth-protected-resource\")\n",
    "    print(\"   - Auth config: http://localhost:8003/auth/config\")\n",
    "    print(\"\\nâš ï¸  Note: This server uses real JWT verification.\")\n",
    "    print(\"For production use, configure your own Auth0 credentials.\")\n",
    "    print(\"\\nðŸ’¡ To test the authenticated client, run the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aiomq8rnzj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/kinsy/llmesh/.venv3119` does not match the project environment path `/home/kinsy/llmesh/.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "2025-07-17 20:20:37,204 - ATHON - DEBUG - Selected Langchain ChatOpenAI\n",
      "ðŸ” Auth0 MCP Client Demo\n",
      "==================================================\n",
      "Fetching Auth0 configuration from http://localhost:8003/auth/config...\n",
      "âœ… Auth0 Domain: dev-zeytropyq5oacx6v.us.auth0.com\n",
      "âœ… Client ID: ZBQHZqOAzikMghrkozgObJ3424wx4USR\n",
      "âœ… Audience: https://llmeshmcpfastapiexample.com\n",
      "âœ… Callback URL: http://localhost:8080/callback\n",
      "\n",
      "ðŸ“¡ Connecting to authenticated MCP server at http://localhost:8003/secure/mcp...\n",
      "âœ… Successfully authenticated and connected!\n",
      "==================================================\n",
      "MCP Client-Server Demonstration\n",
      "==================================================\n",
      "\n",
      "==================== STEP: Initialization ====================\n",
      "Description: Establishing connection with MCP server\n",
      "\n",
      "ðŸ“¡ Started callback server on http://localhost:8080/callback\n",
      "\n",
      "ðŸ” Opening browser for authentication...\n",
      "If browser doesn't open, please visit:\n",
      "http://localhost:8003/authorize?response_type=code&client_id=ZBQHZqOAzikMghrkozgObJ3424wx4USR&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2Fcallback&state=mMufCgBoJHe4nJZ81HcKfcl4eHVb9Jpg0YI86JHKA8g&code_challenge=jSrFLjtLkR8Jq_EDEgg1HIVF1eJMEMsvHrfhs8dp97A&code_challenge_method=S256&resource=http%3A%2F%2Flocalhost%3A8003&scope=openid+profile+email+mcp%3Aread+mcp%3Awrite+offline_access\n",
      "\n",
      "â³ Waiting for authentication callback...\n",
      "tcgetpgrp failed: Not a tty\n",
      "âœ… Authorization code received\n",
      "âœ… Successfully connected to MCP server\n",
      "\n",
      "==================== STEP: Discovery ====================\n",
      "Description: Finding what the server offers\n",
      "\n",
      "ðŸ”§ Found 3 tools: ['secure_multiply', 'add', 'get_weather']\n",
      "ðŸ“„ Found 1 resources: [AnyUrl('config://settings')]\n",
      "ðŸ’¬ Found 1 prompts: ['system_prompt']\n",
      "\n",
      "==================== STEP: Tool Usage ====================\n",
      "Description: Calling different MCP tools to show functionality\n",
      "\n",
      "\n",
      "ðŸ”§ Using tool: secure_multiply\n",
      "   Description: A secure calculation tool\n",
      "\n",
      "ðŸ”§ Using tool: add\n",
      "   Description: Add two numbers\n",
      "\n",
      "--- Add Tool Response ---\n",
      "{'content': [{'annotations': None,\n",
      "              'meta': None,\n",
      "              'text': '8.5',\n",
      "              'type': 'text'}],\n",
      " 'isError': False,\n",
      " 'meta': None,\n",
      " 'structuredContent': {'result': 8.5}}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Basic addition using MCP tool', 'operation': '5 + 3.5'}\n",
      "--- End of Add Tool ---\n",
      "\n",
      "ðŸ”§ Using tool: get_weather\n",
      "   Description: Get current weather for a city\n",
      "\n",
      "--- Weather Tool Response ---\n",
      "{'content': [{'annotations': None,\n",
      "              'meta': None,\n",
      "              'text': 'Error executing tool get_weather: 1 validation error '\n",
      "                      'for get_weatherArguments\\n'\n",
      "                      'city\\n'\n",
      "                      '  Field required [type=missing, '\n",
      "                      \"input_value={'location': 'San Francisco'}, \"\n",
      "                      'input_type=dict]\\n'\n",
      "                      '    For further information visit '\n",
      "                      'https://errors.pydantic.dev/2.11/v/missing',\n",
      "              'type': 'text'}],\n",
      " 'isError': True,\n",
      " 'meta': None,\n",
      " 'structuredContent': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Getting weather information', 'location': 'San Francisco'}\n",
      "--- End of Weather Tool ---\n",
      "\n",
      "==================== STEP: Resource Access ====================\n",
      "Description: Reading data from MCP resources\n",
      "\n",
      "\n",
      "ðŸ“„ Accessing resource: config://settings\n",
      "   Name: config_resource\n",
      "   Description: Server configuration\n",
      "\n",
      "--- Config Resource Response ---\n",
      "{'contents': [{'meta': None,\n",
      "               'mimeType': 'text/plain',\n",
      "               'text': '{\\n'\n",
      "                       '  \"type\": \"text\",\\n'\n",
      "                       '  \"text\": \"{\\\\\"version\\\\\": \\\\\"1.0\\\\\", \\\\\"status\\\\\": '\n",
      "                       '\\\\\"active\\\\\"}\",\\n'\n",
      "                       '  \"annotations\": null,\\n'\n",
      "                       '  \"_meta\": null\\n'\n",
      "                       '}',\n",
      "               'uri': AnyUrl('config://settings')}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Configuration data resource'}\n",
      "--- End of Config Resource ---\n",
      "\n",
      "==================== STEP: Prompt Usage ====================\n",
      "Description: Getting and using MCP prompts\n",
      "\n",
      "\n",
      "ðŸ’¬ Using prompt: system_prompt\n",
      "   Description: System prompt template for chat responses for one of two modes.\n",
      "\n",
      "        Mode 1: Translate user query to pirate language.\n",
      "        Mode 2: Answer the user query in pirate language.\n",
      "\n",
      "        Args:\n",
      "            user_query (str): The user's query.\n",
      "            mode (str): The mode of operation. Can be 'translate' or 'answer'.\n",
      "\n",
      "        Returns:\n",
      "            A list containing system instruction string and message objects.\n",
      "        \n",
      "\n",
      "   Trying mode: translate\n",
      "\n",
      "--- System Prompt (mode=translate) Response ---\n",
      "{'description': 'System prompt template for chat responses for one of two '\n",
      "                'modes.\\n'\n",
      "                '\\n'\n",
      "                '        Mode 1: Translate user query to pirate language.\\n'\n",
      "                '        Mode 2: Answer the user query in pirate language.\\n'\n",
      "                '\\n'\n",
      "                '        Args:\\n'\n",
      "                \"            user_query (str): The user's query.\\n\"\n",
      "                '            mode (str): The mode of operation. Can be '\n",
      "                \"'translate' or 'answer'.\\n\"\n",
      "                '\\n'\n",
      "                '        Returns:\\n'\n",
      "                '            A list containing system instruction string and '\n",
      "                'message objects.\\n'\n",
      "                '        ',\n",
      " 'messages': [{'content': {'annotations': None,\n",
      "                           'meta': None,\n",
      "                           'text': 'You are a helpful assistant who is fluent '\n",
      "                                   \"in pirate language. Please 'translate' the \"\n",
      "                                   'following text.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'meta': None,\n",
      "                           'text': 'Hello, how are you today?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'meta': None,\n",
      "                           'text': 'Ahoy there, how be ye this fine day, '\n",
      "                                   'matey?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'meta': None,\n",
      "                           'text': 'I need to find a restaurant nearby.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'meta': None,\n",
      "                           'text': \"Arr, I be needin' to find a place to fill \"\n",
      "                                   'me belly nearby, savvy?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'meta': None,\n",
      "                           'text': \"What's the weather like today?\",\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Prompt in translate mode',\n",
      " 'message_count': 6,\n",
      " 'parameters': {'mode': 'translate',\n",
      "                'user_query': \"What's the weather like today?\"}}\n",
      "--- End of System Prompt (mode=translate) ---\n",
      "\n",
      "==================== STEP: MCP Prompt + LLM Integration ====================\n",
      "Description: Using MCP prompts directly with Language Model\n",
      "\n",
      "âœ… Retrieved prompt from MCP server\n",
      "   Prompt has 6 messages\n",
      "\n",
      "ðŸ“¤ Sending messages to LLM...\n",
      "   Converting 6 MCP messages to LangChain format\n",
      "2025-07-17 20:20:43,803 - ATHON - DEBUG - Prompt generated I be needin' to secure a table fer dinner, if ye please!\n",
      "\n",
      "--- LLM Response Using MCP Prompt Response ---\n",
      "{'explanation': 'LLM processed the MCP prompt and generated a pirate '\n",
      "                'translation',\n",
      " 'llm_response': \"I be needin' to secure a table fer dinner, if ye please!\",\n",
      " 'mcp_prompt': {'last_user_message': 'I need to book a dinner reservation',\n",
      "                'message_count': 6,\n",
      "                'name': 'system_prompt'}}\n",
      "--- End of LLM Response Using MCP Prompt ---\n",
      "\n",
      "==================== STEP: Tools + Prompts + LLM ====================\n",
      "Description: Complete workflow: MCP tools -> MCP prompts -> LLM\n",
      "\n",
      "1ï¸âƒ£ Getting data from MCP tool...\n",
      "   Tool result: 50.0\n",
      "\n",
      "2ï¸âƒ£ Getting translation prompt from MCP server...\n",
      "\n",
      "3ï¸âƒ£ Using MCP prompt with LLM...\n",
      "2025-07-17 20:20:44,284 - ATHON - DEBUG - Prompt generated The reckonin' be 50.0, matey!\n",
      "\n",
      "--- Complete MCP Workflow Result Response ---\n",
      "{'explanation': 'Full workflow: MCP tool -> MCP prompt -> LLM processing',\n",
      " 'step_1_tool_result': '50.0',\n",
      " 'step_2_prompt_messages': 6,\n",
      " 'step_3_llm_result': \"The reckonin' be 50.0, matey!\"}\n",
      "--- End of Complete MCP Workflow Result ---\n",
      "\n",
      "==================================================\n",
      "âœ… MCP Demo Complete!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the Auth0 client demo\"\"\"\n",
    "\n",
    "# Run the Auth0 client\n",
    "!uv run lablib/mcp/auth/client.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "l7aoj2o3vs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Attempting to terminate 'auth_server' (PID: 77518)...\n",
      "BackgroundProcessManager: Process group for 'auth_server' (PGID: 77518) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'auth_server'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cleanup Auth0 demo processes\"\"\"\n",
    "kill_background_process(\"auth_server\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
