{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Building a Chat Application with LLM Agentic Tool Mesh\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll explore how to create a robust chat application using Large Language Models (LLMs) with the help of LLM Agentic Tool Mesh's chat services. LLM Agentic Tool Mesh provides all the necessary tools to build a powerful chat system by handling:\n",
    "\n",
    "- **Prompt Rendering**\n",
    "- **Model Management**\n",
    "- **Message Processing**\n",
    "- **Memory Management**\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"pictures/chat.png\" alt=\"LLM Agentic Tool Mesh Chat\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "These services are implemented following the **Factory Design Pattern**, which ensures consistency, scalability, and ease of maintenance. Configuration settings and details of the general services are defined in abstract base classes, while instance-specific settings and behaviors are documented within each concrete implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    "- Understand how LLM Agentic Tool Mesh simplifies the creation of chat applications using LLMs.\n",
    "- Learn how to manage models and memories within the chat service.\n",
    "- Implement prompt rendering and message processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "The following bullets must be ran prior to executing notebooks for running this lab:\n",
    "  1. uv installed and available on PATH with python 3.12 venv\n",
    "\n",
    "      - Linux/MacOS:\n",
    "          - `curl -sSL https://get.uv.dev | sh`\n",
    "          - `source ~/.bashrc`\n",
    "          - `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
    "          - `source $HOME/.local/bin/env`\n",
    "          - `uv python install 3.12`\n",
    "          - `uv venv -p 3.12`\n",
    "          - `source .venv/bin/activate`\n",
    "          - `uv pip install ipykernel`\n",
    "\n",
    "      - Windows:\n",
    "          - TODO\n",
    "  - Select the venv as the notebook kernel\n",
    "  <div align=\"left\">\n",
    "    <img src=\"pictures/kernel.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "  </div>\n",
    "\n",
    "**MUST restart Juypter kernel if automated install dependencies cell is ran**\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/restart.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel will shut down to apply new pip installations, manual restart required.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"Install platform chat dependencies.\"\"\"\n",
    "!cd ../../.. && uv pip install --quiet 'llmesh[chat]'\n",
    "# Shut down the kernel so user must restart it to apply new pip installations.\n",
    "# This is a workaround for the fact that Jupyter does not automatically\n",
    "# pick up new installations in the current kernel.\n",
    "!echo \"Kernel will shut down to apply new pip installations, manual restart required.\"\n",
    "import os\n",
    "os._exit(00)\n",
    "print(\"Environment variables loaded from .env\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Rendering\n",
    "\n",
    "As we learned in the previous notebook, Large Language Models (LLMs) rely heavily on **prompts** to generate meaningful responses. A prompt is essentially the input text that the model uses to understand the context and generate an appropriate output. The effectiveness of an LLM is largely determined by how well these prompts are crafted, making **prompt engineering** a critical aspect of working with LLMs.\n",
    "\n",
    "The **Prompt Rendering** service in LLM Agentic Tool Mesh is designed to simplify and automate the creation and management of prompts, which are essential for interacting with LLMs effectively. This service leverages the concepts we explored earlier to ensure that the prompts fed to the model are well-structured, contextually appropriate, and optimized for desired outputs.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Dynamic Prompt Creation**: \n",
    "   - Allows for generating prompts from string templates and files.\n",
    "   - Supports placeholders for dynamic content, enabling the insertion of variables (e.g., user input or context) into the prompt structure.\n",
    "  \n",
    "2. **Template-Based Rendering**:\n",
    "   - Uses pre-defined templates to maintain consistent prompt formats, ensuring clarity and coherence.\n",
    "   - Templates can be customized based on the specific use case, enhancing the effectiveness of the prompt.\n",
    "\n",
    "3. **File Management**:\n",
    "   - Enables saving customized prompts back to the file system for reuse.\n",
    "   - Supports versioning and management of different prompt styles, allowing teams to experiment and find the most effective formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athon.chat import PromptRender\n",
    "\n",
    "# Example configuration for the Prompt Render\n",
    "PROMPT_CONFIG = {\n",
    "    'type': 'JinjaTemplate',\n",
    "    'environment': 'documents/',\n",
    "    'templates': {\n",
    "        'welcome': 'welcome_template.txt',\n",
    "        'goodbye': 'goodbye_template.txt'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the Prompt Render with the provided configuration\n",
    "prompt_render = PromptRender.create(PROMPT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your name\n",
    "user_name = \"John Doe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 15:44:32,759 - ATHON - DEBUG - Prompt generated from string with params {'name': 'John Doe'}\n",
      "RENDERED CONTENT:\n",
      "Hello, John Doe! Welcome to our service.\n"
     ]
    }
   ],
   "source": [
    "# Render a prompt from a string template\n",
    "template_string = \"Hello, {{ name }}! Welcome to our service.\"\n",
    "render_result = prompt_render.render(template_string, name=user_name)\n",
    "\n",
    "if render_result.status == \"success\":\n",
    "    print(f\"RENDERED CONTENT:\\n{render_result.content}\")\n",
    "else:\n",
    "    print(f\"ERROR:\\n{render_result.error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 15:44:34,949 - ATHON - DEBUG - Prompt generated from documents//welcome_template.txt with params {'name': 'John Doe'}\n",
      "LOADED CONTENT:\n",
      "Welcome to our chat service!\n",
      "\n",
      "Hello John Doe,\n",
      "\n",
      "Iâ€™m Athon, your AI assistant here to help you with any questions or tasks you may have.\n",
      "\n",
      "Here are a few things I can assist you with:\n",
      "- Answering questions on a variety of topics.\n",
      "- Providing summaries and explanations.\n",
      "- Assisting with translations or information retrieval.\n",
      "\n",
      "Feel free to ask anything you need!\n",
      "\n",
      "How can I assist you today?\n",
      "\n",
      "Best regards,\n",
      "Athon\n"
     ]
    }
   ],
   "source": [
    "# Load a prompt from a file\n",
    "load_result = prompt_render.load('welcome', name=user_name)\n",
    "\n",
    "if load_result.status == \"success\":\n",
    "    print(f\"LOADED CONTENT:\\n{load_result.content}\")\n",
    "else:\n",
    "    print(f\"ERROR:\\n{load_result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model Handling\n",
    "\n",
    "The **Chat Model** service in LLM Agentic Tool Mesh is designed to handle the instantiation and utilization of various Large Language Models (LLMs) based on user-defined configurations. This service is essential for managing different models and ensuring that the appropriate LLM is used for each specific use case.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Model Instantiation**:\n",
    "   - Dynamically creates instances of different LLMs (e.g., OpenAI's GPT, Azure GPT) based on configuration parameters.\n",
    "   - Ensures that the selected model is initialized with the correct settings, such as API keys, temperature, maximum tokens, and other hyperparameters.\n",
    "\n",
    "2. **Model Utilization**:\n",
    "   - Provides a standardized interface for interacting with the instantiated LLMs.\n",
    "   - Manages the execution of prompts and the retrieval of responses from the models.\n",
    "   - Supports seamless switching between models, allowing for flexibility and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: atexit handler registered for automated cleanup on kernel exit.\n",
      "Keeping existing .env file.\n",
      "2025-05-11 15:44:43,245 - ATHON - DEBUG - Selected Langchain ChatOpenAI\n"
     ]
    }
   ],
   "source": [
    "from athon.chat import ChatModel\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "from src.notebooks.platform_services.lablib.env_util import set_services_env\n",
    "\n",
    "llm_api_key, llm_model_name, _ = set_services_env()\n",
    "\n",
    "# Example configuration for the Chat Model\n",
    "LLM_CONFIG = {\n",
    "    'type': 'LangChainChatOpenAI',\n",
    "    'api_key': llm_api_key,\n",
    "    'model_name': llm_model_name,\n",
    "    'temperature': 0.7,\n",
    "}\n",
    "\n",
    "# Initialize the Chat Model with the provided configuration\n",
    "chat = ChatModel.create(LLM_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompts\n",
    "prompts = [\n",
    "    SystemMessage(content=\"Convert the message to pirate language\"),\n",
    "    HumanMessage(content=\"Today is a sunny day and the sky is blue\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 15:44:56,706 - ATHON - DEBUG - Prompt generated Arrr, today be a fine sunny day and the sky be a brilliant blue, matey!\n",
      "COMPLETION:\n",
      "Arrr, today be a fine sunny day and the sky be a brilliant blue, matey!\n"
     ]
    }
   ],
   "source": [
    "# Invoke the model with the prompts\n",
    "result = chat.invoke(prompts)\n",
    "\n",
    "# Handle the response\n",
    "if result.status == \"success\":\n",
    "    print(f\"COMPLETION:\\n{result.content}\")\n",
    "else:\n",
    "    print(f\"ERROR:\\n{result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Management\n",
    "\n",
    "The **Message Management** service in LLM Agentic Tool Mesh handles the serialization and deserialization of messages, ensuring smooth communication between different components of the chat system and compatibility across various data formats. This service is critical for maintaining consistency and enabling effective interactions within and outside the LLM Agentic Tool Mesh environment.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Message Serialization**:\n",
    "   - Converts message objects into a format suitable for storage or transmission (e.g., JSON, XML).\n",
    "   - Facilitates the exchange of data between different components of the chat system, such as models, memory services, and prompt renderers.\n",
    "   - Ensures messages can be sent to external applications or integrated into various web applications, enhancing interoperability.\n",
    "\n",
    "2. **Message Deserialization**:\n",
    "   - Transforms serialized data back into message objects that can be processed by the chat system.\n",
    "   - Maintains compatibility with multiple data formats, allowing the chat system to receive and handle messages from various sources.\n",
    "   - Supports error handling and validation to ensure that deserialized messages are complete and accurate.\n",
    "\n",
    "3. **Compatibility Across Platforms**:\n",
    "   - Enables messages to be exchanged between LLM Agentic Tool Mesh and different web applications, ensuring seamless communication.\n",
    "   - Provides support for multiple serialization formats, making it easy to integrate the chat system with external APIs and third-party tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Memory\n",
    "\n",
    "The **Chat Memory** service in LLM Agentic Tool Mesh manages the storage and retrieval of conversation history, which is crucial for maintaining context in chat interactions. This service ensures that the chat application can remember past interactions, enabling it to provide coherent and context-aware responses across multiple turns in a conversation.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Storage of Conversation History**:\n",
    "   - Records user inputs and system responses, creating a structured history of the chat session.\n",
    "   - Supports different memory types (e.g., short-term, long-term) to manage context based on the application's needs.\n",
    "   - Utilizes efficient data storage techniques to handle large amounts of conversation data without compromising performance.\n",
    "\n",
    "2. **Retrieval of Context**:\n",
    "   - Provides quick access to relevant parts of the conversation history, ensuring that the LLM has the necessary context to generate appropriate responses.\n",
    "   - Supports customizable retrieval strategies, such as retrieving the last few messages or searching for specific keywords in the conversation history.\n",
    "   - Allows developers to define context windows, determining how much history is retained and referenced in subsequent interactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
