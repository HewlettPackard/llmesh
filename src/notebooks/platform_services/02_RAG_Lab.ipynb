{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll explore the concept of **Retrieval-Augmented Generation (RAG)** within the LLM Agentic Tool Mesh platform. RAG is a crucial process that enhances the capabilities of Large Language Models (LLMs) by providing them with access to external knowledge. Unlike traditional generation techniques, which rely solely on the model's pre-trained knowledge, RAG allows the model to retrieve relevant data from external sources, improving both the relevance and accuracy of the responses. LLM Agentic Tool Mesh provides all the necessary tools to build a powerful RAG system by handling:\n",
    "\n",
    "- **Data Extraction**\n",
    "- **Data Transformation**\n",
    "- **Data Storage**\n",
    "- **Data Load**\n",
    "- **Data Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    "- Understand the concept of **RAG** and its importance in improving LLM performance.\n",
    "- Learn how LLM Agentic Tool Mesh manages the different stages of the RAG process, from data extraction to retrieval.\n",
    "- Build a simple RAG application using LLM Agentic Tool Mesh, integrating data retrieval into the generation process.\n",
    "- Implement best practices for organizing and retrieving data to ensure relevant and accurate generation of content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.11 installed on your system.\n",
    "- LLM Agentic Tool Mesh library installed. If not, install it using: `pip install 'llmesh[rag]'`\n",
    "- Install kernel for Jupiter notebook `pip install ipykernel`\n",
    "- Create `.env` file inside `data` folder with the value of `LLM_API_KEY` and `LLM_MODEL_NAME` environment variables\n",
    "- API keys for the LLM services you plan to use (e.g., OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r'Field \"model_name\" in Config has conflict with protected namespace \"model_\".*',\n",
    "    category=UserWarning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline for RAG\n",
    "\n",
    "In LLM Agentic Tool Mesh, the RAG process is divided into two main stages: **Data Injection** (where data is injected into the LLM's reasoning) and **Data Retrieval** (where relevant information is retrieved to support generation). \n",
    "\n",
    "### Injection Process\n",
    "\n",
    "The **Injection Process** is a critical step in **RAG**, where data is prepared and integrated into a storage system for efficient retrieval during the generation process. This process ensures that the relevant data is available in a format that the LLM can easily access and use to enhance the quality and relevance of its outputs. In LLM Agentic Tool Mesh, the Injection Process is abstracted into several key steps, including data extraction, transformation, and storage injection.\n",
    "\n",
    "#### 1. Extract\n",
    "\n",
    "- **Data Gathering**: The first step in the injection process involves collecting raw data from various sources. This data can come in different formats such as DOCX, PDF, Excel, or even API responses. The goal is to bring together all the information that may be relevant for the tasks the LLM will perform.\n",
    "  \n",
    "- **Conversion**: Once the data is gathered, it needs to be converted into a common format—typically JSON—to ensure consistency across different data types and sources. This conversion process makes it easier to standardize the subsequent transformation and storage processes.\n",
    "\n",
    "#### 2. Transform\n",
    "\n",
    "- **Clean**: In this stage, irrelevant or redundant information is removed from the data. This might involve eliminating duplicates, irrelevant sections, or noisy data that doesn't contribute to the task at hand. The goal is to focus on the core, useful content.\n",
    "\n",
    "- **Enrich Metadata**: Adding metadata to the data helps improve its searchability and contextual relevance during the retrieval process. Metadata can include information such as the source, keywords, timestamps, and other contextual markers that make it easier to retrieve and use the data efficiently.\n",
    "\n",
    "- **Transform with LLM**: This is where the power of the LLM comes into play. The cleaned data can be transformed into useful formats such as summaries, question-and-answer pairs, or structured outputs. This transformation makes it easier for the LLM to access and use the data in specific ways during the generation process, ensuring that the most relevant information is readily available.\n",
    "\n",
    "#### 3. Load\n",
    "\n",
    "- **Storage Injection**: Once the data is transformed, it is injected into the chosen storage solution. This is typically a **vector database**, which allows for fast and efficient retrieval based on the content's semantic meaning. The vector database stores the transformed data in a format that the LLM can easily access during generation.\n",
    "\n",
    "- **Adaptation**: To optimize data retrieval, the stored data may be further adapted by **chunking** it into smaller, manageable pieces. This process ensures that data is stored in a way that allows the LLM to retrieve only the relevant portions when needed, improving both speed and accuracy during the generation process.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"pictures/ingestion.png\" alt=\"RAG Ingestion\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "\n",
    "This step involves gathering information from a variety of sources and document types, preparing it for further processing in the RAG pipeline. The goal of data extraction is to retrieve relevant information from diverse formats—such as PDFs, DOCX files, or web data—and convert it into a standardized format that can be used efficiently by the system in subsequent stages like transformation, storage, and retrieval.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Multi-Format Support**:\n",
    "   - The Data Extraction service is designed to handle a wide variety of document types, such as **PDF**, **DOCX**, **HTML**, and **XLS** files. This flexibility allows LLM Agentic Tool Mesh to process data from different sources and industries.\n",
    "   - Supports extracting both text and metadata from these files, ensuring that all relevant information is captured for downstream tasks.\n",
    "\n",
    "2. **Standardization**:\n",
    "   - Once the data is extracted from its original format, it is converted into a **standardized format**, typically **JSON**. This uniform format ensures that the data can be seamlessly integrated into the rest of the RAG pipeline, enabling consistency in the way the data is transformed, stored, and retrieved.\n",
    "   - Extracted data can include both text and structural elements like tables, lists, and headers, which are preserved in the standardized format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniofin/Documents/Code/llmesh/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.lib.package.athon.rag import DataExtractor\n",
    "\n",
    "# Example configuration for the Data Extractor\n",
    "EXTRACTOR_CONFIG = {\n",
    "    'type': 'UnstructuredForSections',\n",
    "    'document_type': 'Docx',\n",
    "    'extraction_type': 'SectionWithHeader'\n",
    "}\n",
    "\n",
    "# Initialize the Data Extractor with the provided configuration\n",
    "data_extractor = DataExtractor.create(EXTRACTOR_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 14:06:08,389 - ATHON - DEBUG - Parsing document.\n",
      "2024-12-17 14:06:08,389 - ATHON - DEBUG - Partitioning Docx document.\n",
      "2024-12-17 14:06:09,116 - ATHON - WARNING - Image extraction is not implemented for Docx files.\n",
      "2024-12-17 14:06:09,116 - ATHON - DEBUG - Distribution of element types:\n",
      "2024-12-17 14:06:09,116 - ATHON - DEBUG - Header: 1\n",
      "2024-12-17 14:06:09,116 - ATHON - DEBUG - Title: 101\n",
      "2024-12-17 14:06:09,116 - ATHON - DEBUG - NarrativeText: 127\n",
      "2024-12-17 14:06:09,117 - ATHON - DEBUG - Table: 3\n",
      "2024-12-17 14:06:09,117 - ATHON - DEBUG - ListItem: 60\n",
      "2024-12-17 14:06:09,117 - ATHON - DEBUG - PageBreak: 15\n",
      "2024-12-17 14:06:09,117 - ATHON - DEBUG - Text: 10\n",
      "2024-12-17 14:06:09,117 - ATHON - DEBUG - Footer: 1\n",
      "2024-12-17 14:06:09,117 - ATHON - INFO - Parsing all elements.\n",
      "2024-12-17 14:06:09,199 - ATHON - DEBUG - Saving elements to cache.\n",
      "2024-12-17 14:06:09,199 - ATHON - WARNING - Data not saved because the cache is disabled.\n",
      "EXTRACTED # ELEMENTS:\n",
      "316\n",
      "FIRST EXTRACTED ELEMENTS:\n",
      "[{'text': '5.2.8\\tSMF Services', 'metadata': {'category_depth': 2, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'languages': ['eng'], 'last_modified': '2024-12-17T13:45:10', 'page_number': 1, 'parent_id': '0671e7eb28cce9ad755e60c425fc9c18', 'type': 'Title', 'id': 'e836de96dc00ff2305a820293e7abb96', 'header': '5.2.8 SMF Services'}}, {'text': '5.2.8.1\\tGeneral', 'metadata': {'category_depth': 3, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'languages': ['eng'], 'last_modified': '2024-12-17T13:45:10', 'page_number': 1, 'parent_id': 'e836de96dc00ff2305a820293e7abb96', 'type': 'Title', 'id': 'f63d2fd060e4a309bdd460d37b20a0b1', 'header': '5.2.8.1 General'}}]\n"
     ]
    }
   ],
   "source": [
    "# Parse a document file\n",
    "file_path = 'documents/23502-i60-smf.docx'\n",
    "result = data_extractor.parse(file_path)\n",
    "\n",
    "# Handle the extraction result\n",
    "if result.status == \"success\":\n",
    "    print(f\"EXTRACTED # ELEMENTS:\\n{len(result.elements)}\")\n",
    "    print(f\"FIRST EXTRACTED ELEMENTS:\\n{result.elements[:2]}\")\n",
    "    extracted_elements = result.elements\n",
    "else:\n",
    "    print(f\"ERROR:\\n{result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "\n",
    "After the data has been extracted from various sources, the transformation phase ensures that the data is cleaned, optimized, and enriched, making it ready for efficient storage and retrieval. This stage prepares the data in such a way that it can be seamlessly integrated into the retrieval process, allowing language models to access, understand, and use the data effectively during generation.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Data Cleaning**:\n",
    "   - **Remove Redundant Information**: The transformation process begins by cleaning the extracted data. This involves removing irrelevant, duplicate, or noisy content that could confuse or slow down the retrieval and generation processes.\n",
    "   - **Focus on Core Content**: By focusing on essential content, the data is made leaner and more relevant, ensuring that only the most useful and accurate information is retained for use by the LLM.\n",
    "\n",
    "2. **Metadata Enrichment**:\n",
    "   - **Add Contextual Metadata**: Additional metadata, such as keywords, categories, timestamps, and author information, is added to the data during the transformation phase. This enrichment makes the data easier to search and retrieve by improving the system’s ability to match queries with relevant content.\n",
    "   - **Enhance Searchability**: Metadata plays a vital role in making the retrieval process more efficient, ensuring that specific queries lead to precise results. The more enriched the metadata, the more accurate the retrieval, helping the LLM generate contextually relevant responses.\n",
    "\n",
    "3. **Transformation**:\n",
    "   - **Generate Summaries and Q&A Pairs**: The data can also be processed with the help of LLMs to create summaries, question-and-answer pairs, or other useful formats that facilitate quicker retrieval and understanding during the generation phase. This transformation allows the LLM to directly interact with summarized or structured data, improving the quality and relevance of the generated responses.\n",
    "   - **Chunking for Optimization**: The data may also be broken into smaller chunks or sections that allow for quicker, more targeted retrieval during the generation process. Chunking ensures that the language model retrieves only the most relevant pieces of data, improving both response time and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.package.athon.rag import DataTransformer\n",
    "\n",
    "# Example configuration for the Data Transformer\n",
    "TRANSFORMER_CONFIG = {\n",
    "    'type': 'CteActionRunner',\n",
    "    'clean': {\n",
    "        'min_section_length': 100\n",
    "    },\n",
    "    'transform': {\n",
    "        'chunk_size': 1000,\n",
    "        'chunk_overlap': 0,\n",
    "        'token_chunk': 256\n",
    "    },\n",
    "    'enrich': {\n",
    "        'metadata': {\n",
    "            'processed_by': 'WKSHP-LLM'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the Data Transformer with the provided configuration\n",
    "data_transformer = DataTransformer.create(TRANSFORMER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 14:06:09,207 - ATHON - DEBUG - Performing action: RemoveMultipleSpaces\n",
      "2024-12-17 14:06:09,207 - ATHON - DEBUG - Performing action: ReplaceTabsWithSpaces\n",
      "2024-12-17 14:06:09,208 - ATHON - DEBUG - Performing action: TransformInSectionByHeader\n",
      "2024-12-17 14:06:09,208 - ATHON - DEBUG - Performing action: RemoveTitleElementsOnly\n",
      "2024-12-17 14:06:09,208 - ATHON - DEBUG - Performing action: EnrichMetadata\n",
      "2024-12-17 14:06:09,208 - ATHON - DEBUG - Performing action: TransformInChunk\n",
      "TRANSFORMED # ELEMENTS:\n",
      "88\n",
      "FIRST TRANSFORMED ELEMENTS:\n",
      "[{'text': '5. 2. 8. 1 general the following table shows the smf services and smf service operations. table 5. 2. 8. 1 - 1 : nf services provided by the smf service name service operations operation semantics example consumer ( s ) nsmf _ pdusession create request / response v - smf / i - smf update request / response v - smf / i - smf, h - smf release request / response v - smf / i - smf createsmcontext request / response amf updatesmcontext request / response amf releasesmcontext request / response amf smcontextstatusnotify subscribe / notify amf statusnotify subscribe / notify v - smf / i - smf contextrequest request / response amf, v - smf / i - smf, smf contextpush request / response smf sendmodata request / response amf transfermodata request / response v - smf / i - smf transfermtdata request / response smf, h - smf nsmf _ eventexposure subscribe subscribe / notify nef, amf, nwdaf unsubs', 'metadata': {'category_depth': 3, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'languages': ['eng'], 'last_modified': '2024-12-17T13:45:10', 'page_number': 1, 'parent_id': 'e836de96dc00ff2305a820293e7abb96', 'type': 'Title', 'id': 'f63d2fd060e4a309bdd460d37b20a0b1', 'header': '5.2.8.1 General', 'processed_by': 'WKSHP-LLM'}}, {'text': '##cribe nef, amf, nwdaf notify nef, amf, nwdaf apprelocationinfo af nsmf _ nidd delivery request / response nef nsmf _ trafficcorrelation notify subscribe / notify nef', 'metadata': {'category_depth': 3, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'languages': ['eng'], 'last_modified': '2024-12-17T13:45:10', 'page_number': 1, 'parent_id': 'e836de96dc00ff2305a820293e7abb96', 'type': 'Title', 'id': 'f63d2fd060e4a309bdd460d37b20a0b1', 'header': '5.2.8.1 General', 'processed_by': 'WKSHP-LLM'}}]\n"
     ]
    }
   ],
   "source": [
    "# Define the actions to be performed\n",
    "actions = [\n",
    "    'RemoveMultipleSpaces',\n",
    "    'ReplaceTabsWithSpaces',\n",
    "    'TransformInSectionByHeader',\n",
    "    'RemoveTitleElementsOnly',\n",
    "    'EnrichMetadata',\n",
    "    'TransformInChunk',\n",
    "]\n",
    "\n",
    "# Process the elements\n",
    "result = data_transformer.process(actions, extracted_elements)\n",
    "\n",
    "# Handle the transformation result\n",
    "if result.status == \"success\":\n",
    "    print(f\"TRANSFORMED # ELEMENTS:\\n{len(result.elements)}\")\n",
    "    print(f\"FIRST TRANSFORMED ELEMENTS:\\n{result.elements[:2]}\")\n",
    "    trasformed_elements = result.elements\n",
    "else:\n",
    "    print(f\"ERROR:\\n{result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage\n",
    "\n",
    "Once data has been extracted and transformed, it must be stored in a way that allows for fast and accurate retrieval during the generation process. Effective data storage ensures that the system can quickly access the relevant data needed by the LLM to generate contextually appropriate and accurate responses. LLM Agentic Tool Mesh supports a variety of storage strategies, including the use of specialized databases like vector stores, which are optimized for handling semantic search and retrieval.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Efficient Storage Solutions**:\n",
    "   - **Vector Stores**: LLM Agentic Tool Mesh utilizes vector databases to store data as numerical vectors. This storage method is ideal for handling semantic searches, where the meaning of the input query is more important than exact keyword matching. Vector stores enable the system to retrieve data that is most semantically relevant to the user’s query, improving both speed and accuracy.\n",
    "\n",
    "2. **Structured Data Organization**:\n",
    "   - **Metadata-Enhanced Storage**: During storage, the data is organized with enriched metadata, ensuring that specific filters and searches can be applied quickly. This structured organization helps LLM Agentic Tool Mesh narrow down large datasets to retrieve exactly what is needed for the query.\n",
    "   - **Chunking and Indexing**: Data is stored in chunks, and these chunks are indexed for fast lookup. This chunking strategy ensures that the system retrieves only the most relevant portions of data, improving both the accuracy and efficiency of the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 14:06:11,588 - ATHON - DEBUG - Attempting to get or create Chroma dB collection 'WkshpLlm'.\n",
      "2024-12-17 14:06:13,399 - ATHON - DEBUG - Creating or retrieving collection with arguments: {'name': 'WkshpLlm', 'metadata': {'hnsw:space': 'cosine'}, 'embedding_function': <chromadb.utils.embedding_functions.SentenceTransformerEmbeddingFunction object at 0x3671d5e50>}\n"
     ]
    }
   ],
   "source": [
    "from src.lib.package.athon.rag import DataStorage\n",
    "\n",
    "# Example configuration for the Data Storage\n",
    "STORAGE_CONFIG = {\n",
    "    'type': 'ChromaCollection',\n",
    "    'path': 'data',\n",
    "    'collection': 'WkshpLlm'\n",
    "}\n",
    "\n",
    "# Initialize the Data Storage with the provided configuration\n",
    "data_storage = DataStorage.create(STORAGE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 14:06:13,413 - ATHON - DEBUG - Successfully retrieved the collection.\n",
      "COLLECTION RETRIEVED:\n",
      "name='WkshpLlm' id=UUID('7b367b70-cdcc-4a5e-b0b3-6fa030b23428') metadata={'hnsw:space': 'cosine'} tenant='default_tenant' database='default_database'\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the data collection\n",
    "result = data_storage.get_collection()\n",
    "\n",
    "# Handle the retrieval result\n",
    "if result.status == \"success\":\n",
    "    print(f\"COLLECTION RETRIEVED:\\n{result.collection}\")\n",
    "    collection = result.collection\n",
    "else:\n",
    "    print(f\"ERROR:\\n{result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "Once data has been extracted, transformed, and prepared for storage, the final step is to load this data into the storage system, making it readily accessible for retrieval and use by LLMs. The Data Loader ensures that all the cleaned and structured data is properly indexed and optimized for retrieval, playing a vital role in the efficiency of the overall RAG pipeline.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Seamless Data Loading**:\n",
    "   - The Data Loader is responsible for moving the processed data into the selected storage solution, such as a vector database or a traditional document store. This ensures that the data is loaded in a format optimized for fast access during the retrieval stage.\n",
    "\n",
    "2. **Data Integrity and Validation**:\n",
    "   - Validation processes ensure that the data is compliant with the expected structure and ready to be indexed for fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.package.athon.rag import DataLoader\n",
    "\n",
    "# Example configuration for the Data Loader\n",
    "LOADER_CONFIG = {\n",
    "    'type': 'ChromaForSentences'\n",
    "}\n",
    "\n",
    "# Initialize the Data Loader with the provided configuration\n",
    "data_loader = DataLoader.create(LOADER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and processing elements: 100%|██████████| 88/88 [00:00<00:00, 320120.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 14:06:14,153 - ATHON - DEBUG - Inserted 88 documents into the collection.\n",
      "2024-12-17 14:06:14,154 - ATHON - DEBUG - Successfully inserted elements into the collection.\n",
      "Data successfully inserted into the collection.\n"
     ]
    }
   ],
   "source": [
    "# Insert the elements into the collection\n",
    "result = data_loader.insert(collection, trasformed_elements)\n",
    "\n",
    "# Handle the insertion result\n",
    "if result.status == \"success\":\n",
    "    print(\"Data successfully inserted into the collection.\")\n",
    "else:\n",
    "    print(f\"ERROR:\\n{result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Process\n",
    "\n",
    "Once the data has been injected and stored, the **Retrieval Process** comes into play, focusing on fetching the most relevant information based on a given input query. The retrieval process is essential for ensuring that the LLM can access the appropriate data to generate accurate and contextually relevant responses. In LLM Agentic Tool Mesh, the retrieval process utilizes advanced search techniques, metadata filtering, and chunk expansion to optimize the data retrieval for better output generation.\n",
    "\n",
    "#### 1. Search Techniques\n",
    "\n",
    "- **Data Retrieval**: The retrieval process begins by searching the stored data using advanced techniques like **dense retrieval** (leveraging semantic vector search). Dense retrieval methods use the semantic meaning of the query and stored data to find the most relevant results.\n",
    "\n",
    "#### 2. Metadata Filtering\n",
    "\n",
    "- **Refinement**: After an initial retrieval, metadata filtering is applied to refine the search results. By utilizing the metadata added during the **Injection Process**, the system can narrow down the retrieved data to ensure it closely aligns with the specific needs of the query. For example, filters can be applied based on the document source, creation date, or topic tags to return only the most relevant sections of data.\n",
    "\n",
    "#### 3. Chunk Expansion\n",
    "\n",
    "- **Data Expansion**: In some cases, the initial retrieved data may only include small sections of text or partial results. To provide more comprehensive context, the system applies **chunk expansion**, which expands the data around the retrieved sections, either by adding surrounding paragraphs, sections, or related sentences. This ensures that the LLM has access to a more complete and contextually rich dataset when generating responses, leading to more accurate and nuanced outputs.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"Pictures/retrieve.png\" alt=\"RAG Retrieval\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retriever\n",
    "\n",
    "The Data Retriever ensures that the LLM has access to the most contextually appropriate and relevant information to generate high-quality outputs. It does this by not only searching for relevant data but also expanding and refining the results to provide a more comprehensive and accurate set of information for the language model to use.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Search for Relevant Data**:\n",
    "   - The Data Retriever uses advanced search techniques, such as **semantic vector search** or traditional keyword-based search, to identify the most relevant data from the storage based on the user’s query.\n",
    "   - **Semantic Search**: This method leverages vector embeddings to find data that is semantically related to the query, focusing on meaning rather than exact keyword matching. This ensures that the most contextually relevant data is retrieved, even if the exact terms differ.\n",
    "\n",
    "2. **Chunk Expansion**:\n",
    "   - Once the most relevant chunks of data are retrieved, the Data Retriever can **expand** the results by pulling in surrounding text or related sections of the data. This ensures that the language model has access to a comprehensive context, which is critical for generating accurate and meaningful responses.\n",
    "   - Expansion techniques help provide a more complete picture by retrieving additional information beyond just the exact matching chunk, giving the LLM more context to work with when generating responses.\n",
    "\n",
    "3. **Refinement of Results**:\n",
    "   - After retrieving the initial set of data, the Data Retriever applies **refinement techniques** to filter and prioritize the information. This can include using metadata filters, such as document type, date, or author, to ensure that the results are highly relevant to the user’s specific needs.\n",
    "   - Refinement ensures that irrelevant or outdated information is excluded, leaving only the most valuable and contextually appropriate data for the LLM to use during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.package.athon.rag import DataRetriever\n",
    "\n",
    "# Example configuration for the Data Retriever\n",
    "RETRIEVER_CONFIG = {\n",
    "    'type': 'ChromaForSentences',\n",
    "    'n_results': 3,\n",
    "}\n",
    "\n",
    "# Initialize the Data Retriever with the provided configuration\n",
    "data_retriever = DataRetriever.create(RETRIEVER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 14:06:14,186 - ATHON - DEBUG - Expanding results by section.\n",
      "2024-12-17 14:06:14,188 - ATHON - DEBUG - Successfully retrieved elements from the collection.\n",
      "TEXT:\n",
      "5. 2. 8. 4. 1 general this service is used for nidd transfer between smf and another nf. see clause 4. 25. 5. 5. 2. 8. 4. 1 general this service is used for nidd transfer between smf and another nf. see clause 4. 25. 5.\n",
      "METADATA:\n",
      "{'category_depth': 4, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'header': '5.2.8.4.1 General', 'id': '88cbf2d0246bb602f5dd9f0d9a3b832c', 'languages': \"['eng']\", 'last_modified': '2024-12-17T13:45:10', 'page_number': 15, 'parent_id': '09358a13f349091f7135b4e62a1766cf', 'processed_by': 'WKSHP-LLM', 'type': 'Title'}\n",
      "\n",
      "TEXT:\n",
      "5. 2. 8. 1 general the following table shows the smf services and smf service operations. table 5. 2. 8. 1 - 1 : nf services provided by the smf service name service operations operation semantics example consumer ( s ) nsmf _ pdusession create request / response v - smf / i - smf update request / response v - smf / i - smf, h - smf release request / response v - smf / i - smf createsmcontext request / response amf updatesmcontext request / response amf releasesmcontext request / response amf smcontextstatusnotify subscribe / notify amf statusnotify subscribe / notify v - smf / i - smf contextrequest request / response amf, v - smf / i - smf, smf contextpush request / response smf sendmodata request / response amf transfermodata request / response v - smf / i - smf transfermtdata request / response smf, h - smf nsmf _ eventexposure subscribe subscribe / notify nef, amf, nwdaf unsubs ##cribe nef, amf, nwdaf notify nef, amf, nwdaf apprelocationinfo af nsmf _ nidd delivery request / response nef nsmf _ trafficcorrelation notify subscribe / notify nef 5. 2. 8. 1 general the following table shows the smf services and smf service operations. table 5. 2. 8. 1 - 1 : nf services provided by the smf service name service operations operation semantics example consumer ( s ) nsmf _ pdusession create request / response v - smf / i - smf update request / response v - smf / i - smf, h - smf release request / response v - smf / i - smf createsmcontext request / response amf updatesmcontext request / response amf releasesmcontext request / response amf smcontextstatusnotify subscribe / notify amf statusnotify subscribe / notify v - smf / i - smf contextrequest request / response amf, v - smf / i - smf, smf contextpush request / response smf sendmodata request / response amf transfermodata request / response v - smf / i - smf transfermtdata request / response smf, h - smf nsmf _ eventexposure subscribe subscribe / notify nef, amf, nwdaf unsubs ##cribe nef, amf, nwdaf notify nef, amf, nwdaf apprelocationinfo af nsmf _ nidd delivery request / response nef nsmf _ trafficcorrelation notify subscribe / notify nef\n",
      "METADATA:\n",
      "{'category_depth': 3, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'header': '5.2.8.1 General', 'id': 'f63d2fd060e4a309bdd460d37b20a0b1', 'languages': \"['eng']\", 'last_modified': '2024-12-17T13:45:10', 'page_number': 1, 'parent_id': 'e836de96dc00ff2305a820293e7abb96', 'processed_by': 'WKSHP-LLM', 'type': 'Title'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example query to search within the collection\n",
    "query = \"What are the services of SMF?\"\n",
    "\n",
    "# Retrieve the relevant data based on the query\n",
    "result = data_retriever.select(collection, query)\n",
    "\n",
    "# Handle the retrieval result\n",
    "if result.status == \"success\":\n",
    "    for element in result.elements:\n",
    "        print(f\"TEXT:\\n{element['text']}\\nMETADATA:\\n{element['metadata']}\\n\")\n",
    "else:\n",
    "    print(f\"ERROR:\\n{result.error_message}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
