{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll explore the concept of **Retrieval-Augmented Generation (RAG)** within the LLM Agentic Tool Mesh platform. RAG is a crucial process that enhances the capabilities of Large Language Models (LLMs) by providing them with access to external knowledge. Unlike traditional generation techniques, which rely solely on the model's pre-trained knowledge, RAG allows the model to retrieve relevant data from external sources, improving both the relevance and accuracy of the responses. LLM Agentic Tool Mesh provides all the necessary tools to build a powerful RAG system by handling:\n",
    "\n",
    "- **Data Extraction**\n",
    "- **Data Transformation**\n",
    "- **Data Storage**\n",
    "- **Data Load**\n",
    "- **Data Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    "- Understand the concept of **RAG** and its importance in improving LLM performance.\n",
    "- Learn how LLM Agentic Tool Mesh manages the different stages of the RAG process, from data extraction to retrieval.\n",
    "- Build a simple RAG application using LLM Agentic Tool Mesh, integrating data retrieval into the generation process.\n",
    "- Implement best practices for organizing and retrieving data to ensure relevant and accurate generation of content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "The following bullets must be ran prior to executing notebooks for running this lab:\n",
    "  1. uv installed and available on PATH with python 3.12 venv\n",
    "\n",
    "      - Linux/MacOS:\n",
    "          - `curl -sSL https://get.uv.dev | sh`\n",
    "          - `source ~/.bashrc`\n",
    "          - `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
    "          - `source $HOME/.local/bin/env`\n",
    "          - `uv python install 3.12`\n",
    "          - `uv venv -p 3.12`\n",
    "          - `source .venv/bin/activate`\n",
    "          - `uv pip install ipykernel`\n",
    "\n",
    "      - Windows:\n",
    "          - TODO\n",
    "\n",
    "  - Select the venv as the notebook kernel\n",
    "  <div align=\"left\">\n",
    "    <img src=\"pictures/kernel.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "  </div>\n",
    "\n",
    "**MUST restart Juypter kernel if automated install dependencies cell is ran**\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/restart.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel will shut down to apply new pip installations, manual restart required.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"Install rag dependencies\"\n",
    "!cd ../../.. && uv pip install --quiet 'llmesh[rag]' ipywidgets\n",
    "# Shut down the kernel so user must restart it to apply new pip installations.\n",
    "# This is a workaround for the fact that Jupyter does not automatically\n",
    "# pick up new installations in the current kernel.\n",
    "!echo \"Kernel will shut down to apply new pip installations, manual restart required.\"\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: atexit handler registered for automated cleanup on kernel exit.\n",
      "Keeping existing .env file.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Setup environment for the lab.\"\"\"\n",
    "from src.notebooks.platform_services.lablib.env_util import set_services_env\n",
    "_, _, _ = set_services_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline for RAG\n",
    "\n",
    "In LLM Agentic Tool Mesh, the RAG process is divided into two main stages: **Data Injection** (where data is injected into the LLM's reasoning) and **Data Retrieval** (where relevant information is retrieved to support generation). \n",
    "\n",
    "### Injection Process\n",
    "\n",
    "The **Injection Process** is a critical step in **RAG**, where data is prepared and integrated into a storage system for efficient retrieval during the generation process. This process ensures that the relevant data is available in a format that the LLM can easily access and use to enhance the quality and relevance of its outputs. In LLM Agentic Tool Mesh, the Injection Process is abstracted into several key steps, including data extraction, transformation, and storage injection.\n",
    "\n",
    "#### 1. Extract\n",
    "\n",
    "- **Data Gathering**: The first step in the injection process involves collecting raw data from various sources. This data can come in different formats such as DOCX, PDF, Excel, or even API responses. The goal is to bring together all the information that may be relevant for the tasks the LLM will perform.\n",
    "  \n",
    "- **Conversion**: Once the data is gathered, it needs to be converted into a common format—typically JSON—to ensure consistency across different data types and sources. This conversion process makes it easier to standardize the subsequent transformation and storage processes.\n",
    "\n",
    "#### 2. Transform\n",
    "\n",
    "- **Clean**: In this stage, irrelevant or redundant information is removed from the data. This might involve eliminating duplicates, irrelevant sections, or noisy data that doesn't contribute to the task at hand. The goal is to focus on the core, useful content.\n",
    "\n",
    "- **Enrich Metadata**: Adding metadata to the data helps improve its searchability and contextual relevance during the retrieval process. Metadata can include information such as the source, keywords, timestamps, and other contextual markers that make it easier to retrieve and use the data efficiently.\n",
    "\n",
    "- **Transform with LLM**: This is where the power of the LLM comes into play. The cleaned data can be transformed into useful formats such as summaries, question-and-answer pairs, or structured outputs. This transformation makes it easier for the LLM to access and use the data in specific ways during the generation process, ensuring that the most relevant information is readily available.\n",
    "\n",
    "#### 3. Load\n",
    "\n",
    "- **Storage Injection**: Once the data is transformed, it is injected into the chosen storage solution. This is typically a **vector database**, which allows for fast and efficient retrieval based on the content's semantic meaning. The vector database stores the transformed data in a format that the LLM can easily access during generation.\n",
    "\n",
    "- **Adaptation**: To optimize data retrieval, the stored data may be further adapted by **chunking** it into smaller, manageable pieces. This process ensures that data is stored in a way that allows the LLM to retrieve only the relevant portions when needed, improving both speed and accuracy during the generation process.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"pictures/ingestion.png\" alt=\"RAG Ingestion\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "\n",
    "This step involves gathering information from a variety of sources and document types, preparing it for further processing in the RAG pipeline. The goal of data extraction is to retrieve relevant information from diverse formats—such as PDFs, DOCX files, or web data—and convert it into a standardized format that can be used efficiently by the system in subsequent stages like transformation, storage, and retrieval.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Multi-Format Support**:\n",
    "   - The Data Extraction service is designed to handle a wide variety of document types, such as **PDF**, **DOCX**, **HTML**, and **XLS** files. This flexibility allows LLM Agentic Tool Mesh to process data from different sources and industries.\n",
    "   - Supports extracting both text and metadata from these files, ensuring that all relevant information is captured for downstream tasks.\n",
    "\n",
    "2. **Standardization**:\n",
    "   - Once the data is extracted from its original format, it is converted into a **standardized format**, typically **JSON**. This uniform format ensures that the data can be seamlessly integrated into the rest of the RAG pipeline, enabling consistency in the way the data is transformed, stored, and retrieved.\n",
    "   - Extracted data can include both text and structural elements like tables, lists, and headers, which are preserved in the standardized format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.package.athon.rag import DataExtractor\n",
    "\n",
    "# Example configuration for the Data Extractor\n",
    "EXTRACTOR_CONFIG = {\n",
    "    'type': 'UnstructuredForSections',\n",
    "    'document_type': 'Docx',\n",
    "    'extraction_type': 'SectionWithHeader'\n",
    "}\n",
    "\n",
    "# Initialize the Data Extractor with the provided configuration\n",
    "data_extractor = DataExtractor.create(EXTRACTOR_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:05:44,981 - ATHON - DEBUG - Parsing document.\n",
      "2025-05-11 16:05:44,982 - ATHON - DEBUG - Partitioning Docx document.\n",
      "2025-05-11 16:05:46,463 - ATHON - WARNING - Image extraction is not implemented for Docx files.\n",
      "2025-05-11 16:05:46,463 - ATHON - DEBUG - Distribution of element types:\n",
      "2025-05-11 16:05:46,464 - ATHON - DEBUG - Header: 1\n",
      "2025-05-11 16:05:46,464 - ATHON - DEBUG - Title: 29\n",
      "2025-05-11 16:05:46,465 - ATHON - DEBUG - Text: 82\n",
      "2025-05-11 16:05:46,465 - ATHON - DEBUG - NarrativeText: 127\n",
      "2025-05-11 16:05:46,465 - ATHON - DEBUG - Table: 3\n",
      "2025-05-11 16:05:46,466 - ATHON - DEBUG - ListItem: 60\n",
      "2025-05-11 16:05:46,466 - ATHON - DEBUG - PageBreak: 15\n",
      "2025-05-11 16:05:46,467 - ATHON - DEBUG - Footer: 1\n",
      "2025-05-11 16:05:46,468 - ATHON - INFO - Parsing all elements.\n",
      "2025-05-11 16:05:46,610 - ATHON - DEBUG - Saving elements to cache.\n",
      "2025-05-11 16:05:46,611 - ATHON - WARNING - Data not saved because the cache is disabled.\n",
      "EXTRACTED # ELEMENTS:\n",
      "316\n",
      "FIRST EXTRACTED ELEMENTS:\n",
      "[{'text': '5.2.8\\tSMF Services', 'metadata': {'category_depth': 2, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'languages': ['eng'], 'last_modified': '2025-05-09T23:58:02', 'page_number': 1, 'parent_id': '0671e7eb28cce9ad755e60c425fc9c18', 'type': 'Title', 'id': 'e836de96dc00ff2305a820293e7abb96', 'header': '5.2.8 SMF Services'}}, {'text': '5.2.8.1\\tGeneral', 'metadata': {'category_depth': 3, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'languages': ['eng'], 'last_modified': '2025-05-09T23:58:02', 'page_number': 1, 'parent_id': 'e836de96dc00ff2305a820293e7abb96', 'type': 'Title', 'id': 'f63d2fd060e4a309bdd460d37b20a0b1', 'header': '5.2.8.1 General'}}]\n"
     ]
    }
   ],
   "source": [
    "# Parse a document file\n",
    "file_path = 'documents/23502-i60-smf.docx'\n",
    "result = data_extractor.parse(file_path)\n",
    "\n",
    "# Handle the extraction result\n",
    "if result.status == \"success\":\n",
    "    print(f\"EXTRACTED # ELEMENTS:\\n{len(result.elements)}\")\n",
    "    print(f\"FIRST EXTRACTED ELEMENTS:\\n{result.elements[:2]}\")\n",
    "    extracted_elements = result.elements\n",
    "else:\n",
    "    print(f\"ERROR:\\n{result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "\n",
    "After the data has been extracted from various sources, the transformation phase ensures that the data is cleaned, optimized, and enriched, making it ready for efficient storage and retrieval. This stage prepares the data in such a way that it can be seamlessly integrated into the retrieval process, allowing language models to access, understand, and use the data effectively during generation.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Data Cleaning**:\n",
    "   - **Remove Redundant Information**: The transformation process begins by cleaning the extracted data. This involves removing irrelevant, duplicate, or noisy content that could confuse or slow down the retrieval and generation processes.\n",
    "   - **Focus on Core Content**: By focusing on essential content, the data is made leaner and more relevant, ensuring that only the most useful and accurate information is retained for use by the LLM.\n",
    "\n",
    "2. **Metadata Enrichment**:\n",
    "   - **Add Contextual Metadata**: Additional metadata, such as keywords, categories, timestamps, and author information, is added to the data during the transformation phase. This enrichment makes the data easier to search and retrieve by improving the system’s ability to match queries with relevant content.\n",
    "   - **Enhance Searchability**: Metadata plays a vital role in making the retrieval process more efficient, ensuring that specific queries lead to precise results. The more enriched the metadata, the more accurate the retrieval, helping the LLM generate contextually relevant responses.\n",
    "\n",
    "3. **Transformation**:\n",
    "   - **Generate Summaries and Q&A Pairs**: The data can also be processed with the help of LLMs to create summaries, question-and-answer pairs, or other useful formats that facilitate quicker retrieval and understanding during the generation phase. This transformation allows the LLM to directly interact with summarized or structured data, improving the quality and relevance of the generated responses.\n",
    "   - **Chunking for Optimization**: The data may also be broken into smaller chunks or sections that allow for quicker, more targeted retrieval during the generation process. Chunking ensures that the language model retrieves only the most relevant pieces of data, improving both response time and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.package.athon.rag import DataTransformer\n",
    "\n",
    "# Example configuration for the Data Transformer\n",
    "TRANSFORMER_CONFIG = {\n",
    "    'type': 'CteActionRunner',\n",
    "    'clean': {\n",
    "        'min_section_length': 100\n",
    "    },\n",
    "    'transform': {\n",
    "        'chunk_size': 1000,\n",
    "        'chunk_overlap': 0,\n",
    "        'token_chunk': 256\n",
    "    },\n",
    "    'enrich': {\n",
    "        'metadata': {\n",
    "            'processed_by': 'WKSHP-LLM'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the Data Transformer with the provided configuration\n",
    "data_transformer = DataTransformer.create(TRANSFORMER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:06:11,743 - ATHON - DEBUG - Performing action: RemoveMultipleSpaces\n",
      "2025-05-11 16:06:11,746 - ATHON - DEBUG - Performing action: ReplaceTabsWithSpaces\n",
      "2025-05-11 16:06:11,746 - ATHON - DEBUG - Performing action: TransformInSectionByHeader\n",
      "2025-05-11 16:06:11,747 - ATHON - DEBUG - Performing action: RemoveTitleElementsOnly\n",
      "2025-05-11 16:06:11,748 - ATHON - DEBUG - Performing action: EnrichMetadata\n",
      "2025-05-11 16:06:11,749 - ATHON - DEBUG - Performing action: TransformInChunk\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db0265dfe6a4c7993e9d0bee0a6e9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b93c5f67e0045ccb620cf7d7e35c34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cc476e9bde4383a40820ecb248a1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab15e647ec8442309ea81888d4ed8d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0703603d776443e29fa3e6454b4e111d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea2723b538548feb3c0caead096ae2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dba727f4d9346ceb27c6d447f9ab413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fd7c106aa24cd495e1d1e2931ffdc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f926326c7a4848889b851476ca73866c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a7e3a4cdbd4bd3b688aa0dc3cd29ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a68b5590e345fbb7885ab954c3d044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMED # ELEMENTS:\n",
      "88\n",
      "FIRST TRANSFORMED ELEMENTS:\n",
      "[{'text': '5. 2. 8. 1 general the following table shows the smf services and smf service operations. table 5. 2. 8. 1 - 1 : nf services provided by the smf service name service operations operation semantics example consumer ( s ) nsmf _ pdusession create request / response v - smf / i - smf update request / response v - smf / i - smf, h - smf release request / response v - smf / i - smf createsmcontext request / response amf updatesmcontext request / response amf releasesmcontext request / response amf smcontextstatusnotify subscribe / notify amf statusnotify subscribe / notify v - smf / i - smf contextrequest request / response amf, v - smf / i - smf, smf contextpush request / response smf sendmodata request / response amf transfermodata request / response v - smf / i - smf transfermtdata request / response smf, h - smf nsmf _ eventexposure subscribe subscribe / notify nef, amf, nwdaf unsubs', 'metadata': {'category_depth': 3, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'languages': ['eng'], 'last_modified': '2025-05-09T23:58:02', 'page_number': 1, 'parent_id': 'e836de96dc00ff2305a820293e7abb96', 'type': 'Title', 'id': 'f63d2fd060e4a309bdd460d37b20a0b1', 'header': '5.2.8.1 General', 'processed_by': 'WKSHP-LLM'}}, {'text': '##cribe nef, amf, nwdaf notify nef, amf, nwdaf apprelocationinfo af nsmf _ nidd delivery request / response nef nsmf _ trafficcorrelation notify subscribe / notify nef', 'metadata': {'category_depth': 3, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'languages': ['eng'], 'last_modified': '2025-05-09T23:58:02', 'page_number': 1, 'parent_id': 'e836de96dc00ff2305a820293e7abb96', 'type': 'Title', 'id': 'f63d2fd060e4a309bdd460d37b20a0b1', 'header': '5.2.8.1 General', 'processed_by': 'WKSHP-LLM'}}]\n"
     ]
    }
   ],
   "source": [
    "# Define the actions to be performed\n",
    "actions = [\n",
    "    'RemoveMultipleSpaces',\n",
    "    'ReplaceTabsWithSpaces',\n",
    "    'TransformInSectionByHeader',\n",
    "    'RemoveTitleElementsOnly',\n",
    "    'EnrichMetadata',\n",
    "    'TransformInChunk',\n",
    "]\n",
    "\n",
    "# Process the elements\n",
    "result = data_transformer.process(actions, extracted_elements)\n",
    "\n",
    "# Handle the transformation result\n",
    "if result.status == \"success\":\n",
    "    print(f\"TRANSFORMED # ELEMENTS:\\n{len(result.elements)}\")\n",
    "    print(f\"FIRST TRANSFORMED ELEMENTS:\\n{result.elements[:2]}\")\n",
    "    trasformed_elements = result.elements\n",
    "else:\n",
    "    print(f\"ERROR:\\n{result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage\n",
    "\n",
    "Once data has been extracted and transformed, it must be stored in a way that allows for fast and accurate retrieval during the generation process. Effective data storage ensures that the system can quickly access the relevant data needed by the LLM to generate contextually appropriate and accurate responses. LLM Agentic Tool Mesh supports a variety of storage strategies, including the use of specialized databases like vector stores, which are optimized for handling semantic search and retrieval.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Efficient Storage Solutions**:\n",
    "   - **Vector Stores**: LLM Agentic Tool Mesh utilizes vector databases to store data as numerical vectors. This storage method is ideal for handling semantic searches, where the meaning of the input query is more important than exact keyword matching. Vector stores enable the system to retrieve data that is most semantically relevant to the user’s query, improving both speed and accuracy.\n",
    "\n",
    "2. **Structured Data Organization**:\n",
    "   - **Metadata-Enhanced Storage**: During storage, the data is organized with enriched metadata, ensuring that specific filters and searches can be applied quickly. This structured organization helps LLM Agentic Tool Mesh narrow down large datasets to retrieve exactly what is needed for the query.\n",
    "   - **Chunking and Indexing**: Data is stored in chunks, and these chunks are indexed for fast lookup. This chunking strategy ensures that the system retrieves only the most relevant portions of data, improving both the accuracy and efficiency of the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:06:43,092 - ATHON - DEBUG - Attempting to get or create Chroma dB collection 'WkshpLlm'.\n",
      "2025-05-11 16:06:45,000 - ATHON - DEBUG - Creating or retrieving collection with arguments: {'name': 'WkshpLlm', 'metadata': {'hnsw:space': 'cosine'}, 'embedding_function': <chromadb.utils.embedding_functions.sentence_transformer_embedding_function.SentenceTransformerEmbeddingFunction object at 0x7f8bb6426810>}\n"
     ]
    }
   ],
   "source": [
    "from src.lib.package.athon.rag import DataStorage\n",
    "\n",
    "# Example configuration for the Data Storage\n",
    "STORAGE_CONFIG = {\n",
    "    'type': 'ChromaCollection',\n",
    "    'path': 'data',\n",
    "    'collection': 'WkshpLlm'\n",
    "}\n",
    "\n",
    "# Initialize the Data Storage with the provided configuration\n",
    "data_storage = DataStorage.create(STORAGE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:06:49,032 - ATHON - DEBUG - Successfully retrieved the collection.\n",
      "COLLECTION RETRIEVED:\n",
      "Collection(name=WkshpLlm)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the data collection\n",
    "result = data_storage.get_collection()\n",
    "\n",
    "# Handle the retrieval result\n",
    "if result.status == \"success\":\n",
    "    print(f\"COLLECTION RETRIEVED:\\n{result.collection}\")\n",
    "    collection = result.collection\n",
    "else:\n",
    "    print(f\"ERROR:\\n{result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "Once data has been extracted, transformed, and prepared for storage, the final step is to load this data into the storage system, making it readily accessible for retrieval and use by LLMs. The Data Loader ensures that all the cleaned and structured data is properly indexed and optimized for retrieval, playing a vital role in the efficiency of the overall RAG pipeline.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Seamless Data Loading**:\n",
    "   - The Data Loader is responsible for moving the processed data into the selected storage solution, such as a vector database or a traditional document store. This ensures that the data is loaded in a format optimized for fast access during the retrieval stage.\n",
    "\n",
    "2. **Data Integrity and Validation**:\n",
    "   - Validation processes ensure that the data is compliant with the expected structure and ready to be indexed for fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.package.athon.rag import DataLoader\n",
    "\n",
    "# Example configuration for the Data Loader\n",
    "LOADER_CONFIG = {\n",
    "    'type': 'ChromaForSentences'\n",
    "}\n",
    "\n",
    "# Initialize the Data Loader with the provided configuration\n",
    "data_loader = DataLoader.create(LOADER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and processing elements: 100%|██████████| 88/88 [00:00<00:00, 318463.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:07:02,722 - ATHON - DEBUG - Inserted 88 documents into the collection.\n",
      "2025-05-11 16:07:02,723 - ATHON - DEBUG - Successfully inserted elements into the collection.\n",
      "Data successfully inserted into the collection.\n"
     ]
    }
   ],
   "source": [
    "# Insert the elements into the collection\n",
    "result = data_loader.insert(collection, trasformed_elements)\n",
    "\n",
    "# Handle the insertion result\n",
    "if result.status == \"success\":\n",
    "    print(\"Data successfully inserted into the collection.\")\n",
    "else:\n",
    "    print(f\"ERROR:\\n{result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Process\n",
    "\n",
    "Once the data has been injected and stored, the **Retrieval Process** comes into play, focusing on fetching the most relevant information based on a given input query. The retrieval process is essential for ensuring that the LLM can access the appropriate data to generate accurate and contextually relevant responses. In LLM Agentic Tool Mesh, the retrieval process utilizes advanced search techniques, metadata filtering, and chunk expansion to optimize the data retrieval for better output generation.\n",
    "\n",
    "#### 1. Search Techniques\n",
    "\n",
    "- **Data Retrieval**: The retrieval process begins by searching the stored data using advanced techniques like **dense retrieval** (leveraging semantic vector search). Dense retrieval methods use the semantic meaning of the query and stored data to find the most relevant results.\n",
    "\n",
    "#### 2. Metadata Filtering\n",
    "\n",
    "- **Refinement**: After an initial retrieval, metadata filtering is applied to refine the search results. By utilizing the metadata added during the **Injection Process**, the system can narrow down the retrieved data to ensure it closely aligns with the specific needs of the query. For example, filters can be applied based on the document source, creation date, or topic tags to return only the most relevant sections of data.\n",
    "\n",
    "#### 3. Chunk Expansion\n",
    "\n",
    "- **Data Expansion**: In some cases, the initial retrieved data may only include small sections of text or partial results. To provide more comprehensive context, the system applies **chunk expansion**, which expands the data around the retrieved sections, either by adding surrounding paragraphs, sections, or related sentences. This ensures that the LLM has access to a more complete and contextually rich dataset when generating responses, leading to more accurate and nuanced outputs.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"Pictures/retrieve.png\" alt=\"RAG Retrieval\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retriever\n",
    "\n",
    "The Data Retriever ensures that the LLM has access to the most contextually appropriate and relevant information to generate high-quality outputs. It does this by not only searching for relevant data but also expanding and refining the results to provide a more comprehensive and accurate set of information for the language model to use.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Search for Relevant Data**:\n",
    "   - The Data Retriever uses advanced search techniques, such as **semantic vector search** or traditional keyword-based search, to identify the most relevant data from the storage based on the user’s query.\n",
    "   - **Semantic Search**: This method leverages vector embeddings to find data that is semantically related to the query, focusing on meaning rather than exact keyword matching. This ensures that the most contextually relevant data is retrieved, even if the exact terms differ.\n",
    "\n",
    "2. **Chunk Expansion**:\n",
    "   - Once the most relevant chunks of data are retrieved, the Data Retriever can **expand** the results by pulling in surrounding text or related sections of the data. This ensures that the language model has access to a comprehensive context, which is critical for generating accurate and meaningful responses.\n",
    "   - Expansion techniques help provide a more complete picture by retrieving additional information beyond just the exact matching chunk, giving the LLM more context to work with when generating responses.\n",
    "\n",
    "3. **Refinement of Results**:\n",
    "   - After retrieving the initial set of data, the Data Retriever applies **refinement techniques** to filter and prioritize the information. This can include using metadata filters, such as document type, date, or author, to ensure that the results are highly relevant to the user’s specific needs.\n",
    "   - Refinement ensures that irrelevant or outdated information is excluded, leaving only the most valuable and contextually appropriate data for the LLM to use during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.package.athon.rag import DataRetriever\n",
    "\n",
    "# Example configuration for the Data Retriever\n",
    "RETRIEVER_CONFIG = {\n",
    "    'type': 'ChromaForSentences',\n",
    "    'n_results': 3,\n",
    "}\n",
    "\n",
    "# Initialize the Data Retriever with the provided configuration\n",
    "data_retriever = DataRetriever.create(RETRIEVER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:07:12,790 - ATHON - DEBUG - Expanding results by section.\n",
      "2025-05-11 16:07:12,796 - ATHON - DEBUG - Successfully retrieved elements from the collection.\n",
      "TEXT:\n",
      "5. 2. 8. 4. 1 general this service is used for nidd transfer between smf and another nf. see clause 4. 25. 5.\n",
      "METADATA:\n",
      "{'category_depth': 4, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'header': '5.2.8.4.1 General', 'id': '88cbf2d0246bb602f5dd9f0d9a3b832c', 'languages': \"['eng']\", 'last_modified': '2025-05-09T23:58:02', 'page_number': 15, 'parent_id': '09358a13f349091f7135b4e62a1766cf', 'processed_by': 'WKSHP-LLM', 'type': 'Title'}\n",
      "\n",
      "TEXT:\n",
      "5. 2. 8. 1 general the following table shows the smf services and smf service operations. table 5. 2. 8. 1 - 1 : nf services provided by the smf service name service operations operation semantics example consumer ( s ) nsmf _ pdusession create request / response v - smf / i - smf update request / response v - smf / i - smf, h - smf release request / response v - smf / i - smf createsmcontext request / response amf updatesmcontext request / response amf releasesmcontext request / response amf smcontextstatusnotify subscribe / notify amf statusnotify subscribe / notify v - smf / i - smf contextrequest request / response amf, v - smf / i - smf, smf contextpush request / response smf sendmodata request / response amf transfermodata request / response v - smf / i - smf transfermtdata request / response smf, h - smf nsmf _ eventexposure subscribe subscribe / notify nef, amf, nwdaf unsubs ##cribe nef, amf, nwdaf notify nef, amf, nwdaf apprelocationinfo af nsmf _ nidd delivery request / response nef nsmf _ trafficcorrelation notify subscribe / notify nef\n",
      "METADATA:\n",
      "{'category_depth': 3, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'header': '5.2.8.1 General', 'id': 'f63d2fd060e4a309bdd460d37b20a0b1', 'languages': \"['eng']\", 'last_modified': '2025-05-09T23:58:02', 'page_number': 1, 'parent_id': 'e836de96dc00ff2305a820293e7abb96', 'processed_by': 'WKSHP-LLM', 'type': 'Title'}\n",
      "\n",
      "TEXT:\n",
      "5. 2. 8. 2. 3 nsmf _ pdusession _ update service operation service operation name : nsmf _ pdusession _ update. description : update the established pdu session. this service operation is invoked by the v - smf towards the h - smf in the case of ue or serving network requested pdu session modification in order for the v - smf to transfer the pdu session modification request. it can also be invoked by the v - smf to indicate to the h - smf that the access type of the pdu session can be changed. this service operation is also invoked by the v - smf to insert or remove ul cl or bp controlled by the v - smf. this service operation is invoked by the i - smf towards the smf in the case of ue or serving network requested pdu session modification in order for the i - smf to transfer the pdu session modification request. it can also be invoked by the i - smf to indicate to the smf that the access type of the pdu session can be changed. this service operation is also invoked by the i - smf towards the smf to insert or remove ulcl or bp controlled by the i - smf or to report usage offloaded via ul cl or bp controlled by i - smf. this service operation is invoked by the h - smf towards the v - smf for both ue initiated and hplmn initiated pdu session modification and pdu session release cases in order to have the sm pdu session modification request or sm pdu session release request sent to the ue. it can also be invoked by the h - smf towards the v - smf to release the 5gc and 5g - an resources in e. g. handover from 5gc - n3iwf to eps and from 5gs to epc / epdg, wherein the ue is not notified. this service operation is invoked by the smf towards the i - smf for both ue initiated and smf / pcf initiated pdu session modification and pdu session release cases in order to have the sm pdu session modification request or sm pdu session release request sent to the ue. it can also be invoked by the smf towards the i - smf to release the 5gc and 5g - an resources in e. g. handover from 5gc - n3iwf to eps and from 5gs to epc / epdg, wherein the ue is not notified. this service operation is also invoked by the smf towards the i - smf to provide updated n4 information or updated dnai list of interest for this pdu session when smf receives updated pcc rules. this service operation is invoked by the v - smf or i - smf and the h - smf or smf in the case of pdu session establishment authentication / authorization by a dn - aaa server defined in clause 4. 3. 2. 3 : it is used to carry dn request container information between the dn - aaa server and the ue. input, required : sm context id. input, optional : ue location information ( uli ), ue time zone, an type, indication of pdu session release, h - smf sm context id ( from h - smf to v - smf ) or smf sm context id ( from smf to i - smf ), qos rule and qos flow level qos parameters if any for the qos flow associated with the qos rule ( from h - smf to v - smf or from smf to i - smf ), eps bearer context ( s ) and linked ebi ( from h - smf to v - smf or from smf to i - smf ), n9 tunnel info ( from v - smf to h - smf or from i - smf to smf ), information requested by ue for e. g . qos ( from v - smf to h - smf or from i - smf to smf ), 5gsm core network capability, information necessary for v - smf or i - smf to build sm message towards the ue ( from h - smf to v - smf or from smf to i - smf ), trigger pdu release indication ( v - smf to h - smf or from i - smf to smf ), start pause of charging indication, stop pause of charging indication, dn request container information, indication that the ue shall not be notified, ebi allocation parameters ( arp list ), secondary rat usage data, indication that the access type of the pdu session can be changed ( v - smf to h - smf or from i - smf to smf ) or from smf to i - smf ), extended nas - sm timer indication, dnai list supported by i - smf ( from i - smf to smf ), indication of multi - homing support ( from smf to i - smf ), indication of ulcl or bp insertion ( from i - smf to smf ), indication of ulcl or bp removal ( from i - smf to smf ), ipv6 prefix @ local psa ( from i - smf to smf ), dnai ( s ) supported by local psa ( from i - smf to smf ), tunnel info of ulcl or bp ( from i - smf to smf ), n4 information ( from i - smf to smf or from smf to i - smf ), handover complete indication, relocation cancel indication . ma pdu request indication, ma pdu network - upgrade allowed indication, indication on whether the ue is registered in both accesses, ma pdu session accepted indication, access for ma pdu session release, access type for gbr qos flow, indication of access unavailability ( with access type ), qos monitoring indication ( from smf to i - smf ), qos monitoring reporting frequency ( from smf to i - smf ), qos monitoring policy ( from smf to i - smf ), qos monitoring result from ( i - smf to smf ), notification of the sm policy association establishment and termination, pcf binding information, satellite backhaul category, n9 forwarding tunnel to support the eas session continuity required ( from smf to i - smf ), traffic filter for n9 forwarding ( from smf to i - smf ), value of the timer to detect the end of activity on the n9 forwarding tunnel to support the eas session continuity ( from smf to i - smf ), eas rediscovery indication, eas information to be refreshed for eas re - discovery, ecs address configuration information, alternative hplmn s - nssai, hr - sbo authorization result, vplmn specific offloading information for hr - sbo, offload identifier ( s ), hplmn address information, vplmn easdf / local dns server / resolver ip address, dns security information of v - easdf / local dns server / resolver, dns server address provided by hplmn, af traffic influence information ( from h - smf to v - smf in case af interacts with hplmn to influence hr - sbo session at vplmn ), indication of ue supports non - 3gpp access path switching, [ ursp rule enforcement reports ]. output, required : result indication, < arp, cause > pair. output, optional : ue location information, an type, sm information from ue ( from v - smf to h - smf or from i - smf to smf ), list of rejected qos flows ( from v - smf to h - smf or from i - smf to smf ), a list of < arp, ebi > pair, secondary rat usage data, dnai ( s ) of interest for this pdu session ( from smf to i - smf ), n4 information ( from smf to i - smf ), qfi ( s ), qos profile ( s ), session - ambr, qos rule ( s ), qos flow level qos parameters if any for the qos flow ( s ) associated with the qos rule ( s ), eps bearer context ( s ), linked ebi, dnai ( s ) of interest for this pdu session, hr - sbo authorization result, vplmn specific offloading information for hr - sbo, offload identifier ( s ), hplmn address information, dns server address provided by hplmn ( e. g. local dns server / resolver address in vplmn ), internal group identifier ( s ). the h - smf sm context id in the input provides addressing information allocated by the h - smf ( to be used for service operations towards the h - smf for this pdu session ). the smf sm context id in the input provides addressing information allocated by the smf ( to be used for service operations towards the smf for this pdu session ). see clause 4. 3. 3. 3 for an example usage of this service operation. see clauses 4. 22. 6. 3, 4. 22. 7, 4. 22. 8. 3 and 4. 22. 10. 3 for detailed usage of this service operation for atsss. see clause 6. 7. 3 of ts 23. 548 [ 74 ] for detailed usage of this service for eas re - discovery.\n",
      "METADATA:\n",
      "{'category_depth': 4, 'file_directory': 'documents', 'filename': '23502-i60-smf.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'header': '5.2.8.2.3 Nsmf_PDUSession_Update service operation', 'id': '409640ad44bf6139993356920bf5ca6f', 'languages': \"['eng']\", 'last_modified': '2025-05-09T23:58:02', 'page_number': 3, 'processed_by': 'WKSHP-LLM', 'type': 'Title'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example query to search within the collection\n",
    "query = \"What are the services of SMF?\"\n",
    "\n",
    "# Retrieve the relevant data based on the query\n",
    "result = data_retriever.select(collection, query)\n",
    "\n",
    "# Handle the retrieval result\n",
    "if result.status == \"success\":\n",
    "    for element in result.elements:\n",
    "        print(f\"TEXT:\\n{element['text']}\\nMETADATA:\\n{element['metadata']}\\n\")\n",
    "else:\n",
    "    print(f\"ERROR:\\n{result.error_message}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
