{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1051b6",
   "metadata": {},
   "source": [
    "# Lab: Model Context Protocol (MCP)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Model Context Protocol (MCP) provides a standardized approach for communication AI powered applications, particularly Large Language Models (LLMs). As AI models become more sophisticated, capable of understanding complex **prompts**, utilizing external **tools**, and accessing diverse **resources**, MCP aims to simplify these interactions by offering a consistent interface.\n",
    "\n",
    "This lab explores MCP's fundamental concepts, including its typical architecture and core components for managing the flow of **prompts**, data, and instructions for tool use. You'll learn how MCP functions over different transport mechanisms to facilitate structured and capable interactions with AI models.\n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "\n",
    "MCP is designed to define a clear structure for managing LLM **prompts**, manage contextual information (which can include references to **tools** and **resources**), and handle data streams when an application (a client) communicates with a model service (a server). This is not the complete specification of the protocol, but a simplified version to illustrate the concepts of the lab.\n",
    "\n",
    "Key objectives and potential benefits of using a protocol like MCP include:\n",
    "\n",
    "* **Improved Interoperability**: Facilitating connections between applications and a variety of model backends.\n",
    "* **Standardized Patterns**: Promoting common communication methods for leveraging **prompts**, requesting **tool** executions, and receiving model-generated content.\n",
    "* **Streaming Capabilities**: Supporting the efficient flow of data, useful for real-time or interactive model responses.\n",
    "* **Rich Context & Instruction Handling**: Providing defined ways to pass not just conversational history, but also detailed **prompts**, and specifications for how models should use available **tools** or access **resources**.\n",
    "* **Extensibility**: Allowing for future enhancements or specialized data exchanges within the protocol's framework.\n",
    "\n",
    "MCP often draws on established communication principles while adapting them to the specific needs of AI model interactions, such as managing conversational context, interpreting detailed **prompts**, orchestrating **tool** use, and streaming data.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "MCP interactions generally use a client-server architecture:\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/mcp-arch.png\" alt=\"MCP Architecture\" width=\"700\">\n",
    "</div>\n",
    "\n",
    "1.  **Client**: An application that consumes model services. It forms and sends requests (containing **prompts**, data, and potentially **tool** calls) to the MCP server and processes the received responses.\n",
    "2.  **Server**: A service that exposes one or more AI models via MCP. It listens for client requests, interprets **prompts**, manages **tool** execution or **resource** access as defined by the protocol, interacts with the model(s), and returns responses according to the protocol.\n",
    "3.  **Transport Layer**: The underlying mechanism for message transmission. MCP defines the *what* (message structure and interaction flow) more than the *how* of transmission. This lab demonstrates MCP over:\n",
    "    * Standard Input/Output (STDIO)\n",
    "    * Server-Sent Events (SSE)\n",
    "    * Streamable HTTP\n",
    "\n",
    "This separation allows flexibility in choosing a transport suitable for the application's environment.\n",
    "\n",
    "#### Components\n",
    "\n",
    "MCP interactions involve several key elements and types of information:\n",
    "\n",
    "1.  **Messages**: Defined units of information exchanged. Common types are:\n",
    "    * `ContextRequest`: From client to server. This is the primary message for conveying the user's intent, including:\n",
    "        * **Prompts**: The specific instructions or queries for the model.\n",
    "        * Contextual Data: Relevant background information, conversation history, or data to be processed.\n",
    "        * **Tool Calls/Resource Requests**: Specifications for any tools the model should use or resources it needs to access (if the MCP implementation supports this directly or via a structured format).\n",
    "        * Parameters: Configuration for the model's response generation (e.g., temperature, max tokens).\n",
    "    * `ModelResponseStart`: From server to client, signaling the beginning of a response stream, possibly with metadata.\n",
    "    * `ModelResponseChunk`: From server to client, containing a part of the model's output (e.g., text, structured data, tool results), enabling streaming.\n",
    "    * `ModelResponseEnd`: From server to client, indicating the end of the model's output for that request.\n",
    "    * `ErrorResponse`: From server to client, used to communicate issues or errors.\n",
    "\n",
    "2.  **Prompts**: The core input that guides the model's behavior. MCP facilitates the clear transmission of simple or complex prompts from the client to the server.\n",
    "\n",
    "3.  **Tools & Resources**: Mechanisms by which a model can interact with external systems or data:\n",
    "    * **Tools**: Pre-defined functions or capabilities that the model can be instructed to use (e.g., a code interpreter, a search engine API, a database query function). MCP messages can carry requests to invoke such tools and return their outputs.\n",
    "    * **Resources**: External data sources or knowledge bases that the model might need to access to fulfill a request. MCP can facilitate referencing or providing these resources.\n",
    "    * *(The specifics of how tools and resources are defined and invoked can vary between MCP implementations, sometimes handled through structured data within messages or via specific message types if the protocol is highly tool-oriented.)*\n",
    "\n",
    "4.  **Streaming**: A core feature where the model's response (which might include text, or results from tool use) can be sent back in parts (chunks) as it's generated.\n",
    "\n",
    "#### Protocol (Interaction Flow)\n",
    "\n",
    "A typical interaction sequence in MCP includes:\n",
    "\n",
    "1.  **Connection**: The client connects to the MCP server via the chosen transport.\n",
    "2.  **Request**: The client sends a `ContextRequest` message with the **prompt**, relevant context, parameters, and any **tool** or **resource** specifications.\n",
    "    *Below is an illustrative structure for data within a request (often JSON):*\n",
    "    ```json\n",
    "    {\n",
    "      \"context\": {\n",
    "        \"prompt\": \"What's the weather in London and summarize the top news from there?\",\n",
    "        \"history\": []\n",
    "      },\n",
    "      \"tools\": [ // Example of how tool specification might look\n",
    "        {\n",
    "          \"type\": \"function\",\n",
    "          \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather for a city\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\"}}}\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"function\",\n",
    "          \"function\": {\n",
    "            \"name\": \"get_news_summary\",\n",
    "            \"description\": \"Get top news for a city\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\"}}}\n",
    "          }\n",
    "        }\n",
    "      ],\n",
    "      \"params\": { // Model parameters\n",
    "        \"temperature\": 0.5\n",
    "      }\n",
    "    }\n",
    "    ```\n",
    "3.  **Response Initiation (Optional but common for streaming)**: The server may send a `ModelResponseStart` message.\n",
    "4.  **Data Streaming / Tool Execution**: The server processes the request. This may involve invoking tools as specified. It sends `ModelResponseChunk` messages as parts of the response (text from the model, results from tools) become available.\n",
    "5.  **Response Completion**: The server sends a `ModelResponseEnd` message once all data for the request has been sent.\n",
    "6.  **Error Reporting**: If an issue arises, the server sends an `ErrorResponse`.\n",
    "7.  **Disconnection**: The connection may be closed or kept alive for subsequent interactions.\n",
    "\n",
    "This structured approach helps create more predictable and maintainable integrations with AI models, encompassing complex interactions involving prompts, tools, and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8976a",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    "- Understand the core principles of the Model Context Protocol (MCP) and why it's valuable for LLM applications\n",
    "- Explore different transport mechanisms for MCP implementation (STDIO, SSE, and HTTP)\n",
    "- Implement basic MCP clients and servers using Python\n",
    "- Learn how to integrate LLMs with MCP\n",
    "- Gain practical experience with standardized protocols for AI communication\n",
    "\n",
    "This lab provides hands-on experience with an emerging standard in the AI ecosystem, giving you the skills to build more interoperable and flexible LLM-powered applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e09592",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "The following bullets must be ran prior to executing notebooks for running this lab:\n",
    "  1. uv installed and available on PATH with python 3.12 venv\n",
    "\n",
    "      - Linux/MacOS:\n",
    "          - `curl -sSL https://get.uv.dev | sh`\n",
    "          - `source ~/.bashrc`\n",
    "          - `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
    "          - `source $HOME/.local/bin/env`\n",
    "          - `uv python install 3.12`\n",
    "          - `uv venv -p 3.12`\n",
    "          - `source .venv/bin/activate`\n",
    "          - `uv pip install ipykernel`\n",
    "\n",
    "      - Windows:\n",
    "          - TODO\n",
    "  - Select the venv as the notebook kernel\n",
    "  <div align=\"left\">\n",
    "    <img src=\"pictures/kernel.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "  </div>\n",
    "\n",
    "**MUST restart Juypter kernel if automated install dependencies cell is ran**\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/restart.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafda488",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Install platform chat & mcp dependencies.\"\"\"\n",
    "!cd ../../.. && uv pip install --quiet -e .[chat] mcp\n",
    "# Shut down the kernel so user must restart it to apply new pip installations.\n",
    "# This is a workaround for the fact that Jupyter does not automatically\n",
    "# pick up new installations in the current kernel.\n",
    "!echo \"Kernel will shut down to apply new pip installations, manual restart required.\"\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup background process manager for sse/http server demos.\"\"\"\n",
    "from src.notebooks.platform_services.lablib.bg_util import start_background_process, kill_background_process, kill_all_background_processes\n",
    "from time import sleep\n",
    "import subprocess as sp\n",
    "print(\"lablib.util process management functions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set up the lab chat model.\"\"\"\n",
    "from src.notebooks.platform_services.lablib.env_util import set_services_env\n",
    "\n",
    "_, _, _ = set_services_env()\n",
    "\n",
    "print(\"Chat env initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdd7bf",
   "metadata": {},
   "source": [
    "## STDIO Transport\n",
    "\n",
    "The STDIO (Standard Input/Output) transport is the simplest MCP implementation. It uses standard input and output streams to exchange MCP messages between client and server processes.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Simplicity**: Easy to implement with minimal dependencies\n",
    "- **Process-to-Process**: Ideal for communication between parent and child processes\n",
    "- **Piping Support**: Works well with Unix-style pipes and redirects\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Client sends JSON-formatted MCP requests to the server's standard input\n",
    "2. Server processes the requests and responds via its standard output\n",
    "3. Client reads and parses the JSON responses from the server's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run the STDIO demo\"\"\"\n",
    "stdio_client_command = \"uv run --active lablib/mcp/stdio/client.py\"\n",
    "client_result = sp.run(stdio_client_command, shell=True, capture_output=True, text=True)\n",
    "print(\"--- STDIO Client Output ---\")\n",
    "print(client_result.stdout)\n",
    "if client_result.stderr:\n",
    "    print(\"--- STDIO Client Errors ---\")\n",
    "    print(client_result.stderr)\n",
    "print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdfe8d",
   "metadata": {},
   "source": [
    "## SSE Transport\n",
    "\n",
    "Server-Sent Events (SSE) is a transport mechanism that enables servers to push updates to clients over HTTP connections.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **One-Way Streaming**: Server can push multiple events to the client over a single connection\n",
    "- **Simple HTTP-Based**: Uses standard HTTP, making it firewall-friendly and easy to implement\n",
    "- **Automatic Reconnection**: Browsers handle connection interruptions automatically\n",
    "- **Event Typing**: Supports different event types for structured communication\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Client establishes an HTTP connection to the server requesting an SSE stream\n",
    "2. Server keeps the connection open and sends events as they occur\n",
    "3. Each event is formatted as a text block with `event`, `data`, and `id` fields\n",
    "4. Client processes these events as they arrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2029b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run the SSE demo\"\"\"\n",
    "sse_server_command = \"uv run lablib/mcp/sse/server.py\"\n",
    "sse_client_command = \"uv run lablib/mcp/sse/client.py\"\n",
    "\n",
    "print(f\"Starting SSE server: {sse_server_command}\")\n",
    "server_proc = start_background_process(\"sse_server\", sse_server_command)\n",
    "\n",
    "if server_proc and server_proc.poll() is None:\n",
    "    print(f\"Running SSE client: {sse_client_command}\")\n",
    "    client_result = sp.run(sse_client_command, shell=True, capture_output=True, text=True)\n",
    "    print(\"--- SSE Client Output ---\")\n",
    "    print(client_result.stdout)\n",
    "    if client_result.stderr:\n",
    "        print(\"--- SSE Client Errors ---\")\n",
    "        print(client_result.stderr)\n",
    "    print(\"-------------------------\")\n",
    "else:\n",
    "    print(\"SSE server failed to start or exited prematurely. Client will not run.\")\n",
    "\n",
    "print(\"Killing SSE server...\")\n",
    "kill_background_process(\"sse_server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run the SSE Copilot demo\"\"\"\n",
    "sse_server_command = \"uv run lablib/mcp/sse/server.py\"\n",
    "\n",
    "print(f\"Starting SSE server: {sse_server_command}\")\n",
    "server_proc = start_background_process(\"sse_server\", sse_server_command)\n",
    "\n",
    "output = \"\"\"\n",
    "        \"my-test-mcp\": {\n",
    "            \"url\": \"http://localhost:8000/math/sse\"\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Add the following to your llmesh/.vscode/mcp.json file:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf418fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cleanup sse process\"\"\"\n",
    "print(\"Manually killing sse server...\")\n",
    "kill_background_process(\"sse_server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb4c98",
   "metadata": {},
   "source": [
    "## Streamable-HTTP Transport\n",
    "\n",
    "The Streamable-HTTP transport is the go forward replacement for SSE.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Universal Compatibility**: Works with standard HTTP clients and servers\n",
    "- **RESTful Design**: Follows standard RESTful API conventions\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Client sends an HTTP POST request with an MCP request object\n",
    "2. Server responds with chunked transfer encoding (HTTP 1.1)\n",
    "3. Each chunk contains a complete MCP event object (usually a delta update)\n",
    "4. Client processes chunks as they arrive and assembles the complete response\n",
    "\n",
    "**Note**: There is currently no python sdk client for this transport hence the typescript client is used via the mcp_inspector.\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/inspector.png\" alt=\"Inspector screen shot\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf05e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run the HTTP demo\"\"\"\n",
    "\n",
    "# Ensure background processes are killed before starting new ones\n",
    "kill_all_background_processes()\n",
    "\n",
    "# Start the HTTP server\n",
    "start_background_process(\"mcp_server\", \"uv run lablib/mcp/streamable/server.py\")\n",
    "\n",
    "# Start the mcp inspector\n",
    "start_background_process(\"mcp_inspector\", \"npx @modelcontextprotocol/inspector\")\n",
    "\n",
    "print(\"MCP inspector UI available at http://localhost:6274/\")\n",
    "print(\"MCP Streamable HTTP demo server available at http://localhost:8000/math/mcp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeaba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually kill streamable-http server and MCP inspector\n",
    "# Will be automatically killed on kernel restart/exit\n",
    "print(\"Manually terminating MCP servers...\")\n",
    "kill_background_process(\"mcp_server\")\n",
    "kill_background_process(\"mcp_inspector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To kill ALL processes managed by lablib.util (if you started others):\n",
    "print(\"Manually terminating ALL lablib-managed background processes...\")\n",
    "kill_all_background_processes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
