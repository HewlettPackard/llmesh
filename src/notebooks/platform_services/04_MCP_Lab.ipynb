{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1051b6",
   "metadata": {},
   "source": [
    "# Lab: Model Context Protocol (MCP)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Model Context Protocol (MCP) provides a standardized approach for communication between AI-powered applications, particularly Large Language Models (LLMs). This is to facilitate easier integrations between various frameworks, applications, and model services as they continue to grow in scale/complexity. \n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "\n",
    "MCP is designed to define a clear structure for managing LLM **prompts**, contextual information (which can include references to **tools** and **resources**), and data exchange when an application (a client) communicates with a model service (a server). This is not the complete specification of the protocol, but a simplified overview to illustrate the key concepts.\n",
    "\n",
    "Key objectives and potential benefits of using a protocol like MCP include:\n",
    "\n",
    "* **Improved Interoperability**: Facilitating connections between applications and a variety of model backends.\n",
    "* **Standardized Patterns**: Promoting common communication methods for leveraging **prompts**, requesting **tool** executions, and receiving model-generated content.\n",
    "* **Rich Context & Instruction Handling**: Providing defined ways to pass not just conversational history, but also detailed **prompts**, and specifications for how models should use available **tools** or access **resources**.\n",
    "* **Extensibility**: Allowing for future enhancements or specialized data exchanges within the protocol's framework.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "MCP interactions use a client-server architecture:\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/mcp-arch.png\" alt=\"MCP Architecture\" width=\"700\">\n",
    "</div>\n",
    "\n",
    "1.  **Client**: An application that consumes model services. It forms and sends requests (containing **prompts**, data, and potentially **tool** calls) to the MCP server and processes the received responses.\n",
    "2.  **Server**: A service that exposes one or more AI models via MCP. It listens for client requests, interprets **prompts**, manages **tool** execution or **resource** access as defined by the protocol, interacts with the model(s), and returns responses according to the protocol.\n",
    "3.  **Transport Layer**: The underlying mechanism for message transmission. MCP defines the structure and interaction flow, with flexibility in transport mechanisms. MCP supports:\n",
    "    * Standard Input/Output (STDIO)\n",
    "    * Streamable HTTP (with optional Server-Sent Events)\n",
    "\n",
    "This separation allows flexibility in choosing a transport suitable for the application's environment.\n",
    "\n",
    "**Note:** The protocol itself does not define this but MCP Hosts are discrete components actually implementing the protocol. A MCP Host is the entity that includes server(s) and/or client(s) and can be implemented in any programming language. Servers and client connections can/often-are maintained within the same MCP host.\n",
    "\n",
    "#### Components\n",
    "\n",
    "MCP interactions involve several key elements and types of information:\n",
    "\n",
    "1.  **Messages**: Defined units of information exchanged between client and server using JSON-RPC format. Common patterns include:\n",
    "    * Request messages from client to server\n",
    "    * Response messages from server to client  \n",
    "    * Notification messages that don't require responses\n",
    "    * Error messages for communicating issues\n",
    "\n",
    "2.  **Prompts**: The core input that guides the model's behavior. MCP facilitates powerful prompt management.\n",
    "\n",
    "3.  **Tools & Resources**: Mechanisms by which a model can interact with external systems or data:\n",
    "    * **Tools**: Pre-defined functions or capabilities that the model can be instructed to use (e.g., calculators, search engines, database query functions)\n",
    "    * **Resources**: External data sources or knowledge bases that the model might need to access to fulfill a request\n",
    "    * *Note: The specifics of how tools and resources are defined and invoked can vary*\n",
    "\n",
    "#### Protocol (Interaction Flow)\n",
    "\n",
    "A typical interaction sequence in MCP includes:\n",
    "\n",
    "1.  **Connection**: The client connects to the MCP server via the chosen transport mechanism.\n",
    "2.  **Initialization**: Client and server exchange initialization messages to establish capabilities and protocol version.\n",
    "3.  **Request**: The client sends a request message with the **prompt**, relevant context, parameters, and any **tool** or **resource** specifications.\n",
    "4.  **Processing**: The server processes the request, which may involve:\n",
    "    * Interpreting the prompt\n",
    "    * Invoking tools as specified\n",
    "    * Accessing resources as needed\n",
    "    * Generating model responses\n",
    "5.  **Response**: The server returns the response, either as a complete message or (if using Streamable HTTP) potentially as a stream of partial responses.\n",
    "6.  **Error Handling**: If issues arise, appropriate error messages are exchanged.\n",
    "7.  **Disconnection**: The connection may be closed or kept alive for subsequent interactions, depending on the transport and use case.\n",
    "\n",
    "This structured approach helps create more predictable and maintainable integrations with AI models, encompassing complex interactions involving prompts, tools, and resources while maintaining flexibility in implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8976a",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    "- Understand the core principles of the Model Context Protocol (MCP) and why it's valuable for LLM applications\n",
    "- Explore different transport mechanisms for MCP implementation (STDIO, SSE, and HTTP)\n",
    "- Implement basic MCP clients and servers using Python\n",
    "- Learn how to integrate LLMs with MCP\n",
    "- Gain practical experience with standardized protocols for AI communication\n",
    "\n",
    "This lab provides hands-on experience with an emerging standard in the AI ecosystem, giving you the skills to build more interoperable and flexible LLM-powered applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e09592",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "The following bullets must be ran prior to executing notebooks for running this lab:\n",
    "  1. uv installed and available on PATH\n",
    "      - Linux/MacOS:\n",
    "          - `curl -sSL https://get.uv.dev | sh`\n",
    "          - `source ~/.bashrc`\n",
    "          - `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
    "          - `source $HOME/.local/bin/env`\n",
    "\n",
    "      - Windows:\n",
    "          - `powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"`\n",
    "          - Completley close and re-open VSCode, essentailly just restart terminal.\n",
    "              - In my experience doing a terminal restart without closing VSCode does not work.\n",
    "\n",
    "  2. Python 3.12 installed and venv created\n",
    "      - `uv python install 3.12`\n",
    "      - `uv venv -p 3.12`\n",
    "      - Activate the venv:\n",
    "          - Linux/MacOS:\n",
    "              - `source .venv/bin/activate`\n",
    "          - Windows:\n",
    "              - `.\\.venv\\Scripts\\activate`\n",
    "      - `uv pip install ipykernel`\n",
    "\n",
    "  3. (Optional) In order to run the final labe section you need npx installed which requires Node.js (and npm if on linux)\n",
    "      - Windows:\n",
    "          - Simply use installer and include extra tools checkbox when installing Node.js https://nodejs.org/en/download.\n",
    "      - Linux/MacOS:\n",
    "          - Google how to install Node.js on your OS as there are many different ways to do it.\n",
    "\n",
    "  4. Select the venv as the notebook kernel\n",
    "  <div align=\"left\">\n",
    "    <img src=\"pictures/kernel.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "  </div>\n",
    "\n",
    "  \n",
    "\n",
    "**MUST restart Juypter kernel if automated install dependencies cell is ran**\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/restart.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eafda488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel will shut down to apply new pip installations, manual restart required.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"Install platform chat & mcp dependencies.\"\"\"\n",
    "!cd ../../.. && uv pip install --quiet -e .[chat] mcp\n",
    "# Shut down the kernel so user must restart it to apply new pip installations.\n",
    "# This is a workaround for the fact that Jupyter does not automatically\n",
    "# pick up new installations in the current kernel.\n",
    "!echo \"Kernel will shut down to apply new pip installations, manual restart required.\"\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda5fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: atexit handler registered for automated cleanup on kernel exit.\n",
      "lablib.util process management functions are ready.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Setup background process manager for sse/http server demos.\"\"\"\n",
    "from src.notebooks.platform_services.lablib.bg_util import clear_port, start_background_process, kill_background_process, kill_all_background_processes\n",
    "from time import sleep\n",
    "import subprocess as sp\n",
    "print(\"lablib.util process management functions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6018be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping existing .env file.\n",
      "Chat env initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set up the lab chat model.\"\"\"\n",
    "from src.notebooks.platform_services.lablib.env_util import set_services_env\n",
    "\n",
    "_, _, _ = set_services_env()\n",
    "\n",
    "print(\"Chat env initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdd7bf",
   "metadata": {},
   "source": [
    "## STDIO Transport\n",
    "\n",
    "The STDIO (Standard Input/Output) transport is the primary MCP implementation that clients should support. It uses standard input and output streams to exchange MCP messages between client and server processes.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Primary Transport**: Clients SHOULD support STDIO whenever possible\n",
    "- **Subprocess Communication**: The client launches the MCP server as a subprocess\n",
    "- **Newline-Delimited Messages**: JSON-RPC messages are separated by newlines\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Process Launch**: The client launches the MCP server as a subprocess\n",
    "2. **Message Exchange**: \n",
    "   - The server reads JSON-RPC messages from its standard input (stdin)\n",
    "   - The server sends messages to its standard output (stdout)\n",
    "   - Messages are delimited by newlines and MUST NOT contain embedded newlines\n",
    "3. **Message Types**: Messages may be JSON-RPC requests, notifications, responses, or batched messages containing multiple requests/notifications\n",
    "4. **Logging**: The server MAY write UTF-8 strings to its standard error (stderr) for logging purposes, which clients MAY capture, forward, or ignore\n",
    "5. **Protocol Compliance**: \n",
    "   - The server MUST NOT write anything to stdout that is not a valid MCP message\n",
    "   - The client MUST NOT write anything to the server's stdin that is not a valid MCP message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef00f934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-12 16:50:03,557 - ATHON - DEBUG - Selected Langchain ChatOpenAI\n",
      "==================================================\n",
      "MCP Client-Server Demonstration\n",
      "==================================================\n",
      "\n",
      "==================== STEP: Initialization ====================\n",
      "Description: Establishing connection with MCP server\n",
      "\n",
      "‚úÖ Successfully connected to MCP server\n",
      "\n",
      "==================== STEP: Discovery ====================\n",
      "Description: Finding what the server offers\n",
      "\n",
      "üîß Found 2 tools: ['add', 'get_weather']\n",
      "üìÑ Found 1 resources: [AnyUrl('config://settings')]\n",
      "üí¨ Found 1 prompts: ['system_prompt']\n",
      "\n",
      "==================== STEP: Tool Usage ====================\n",
      "Description: Calling different MCP tools to show functionality\n",
      "\n",
      "\n",
      "üîß Using tool: add\n",
      "   Description: Add two numbers\n",
      "\n",
      "--- Add Tool Response ---\n",
      "{'content': [{'annotations': None, 'text': '8.5', 'type': 'text'}],\n",
      " 'isError': False,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Basic addition using MCP tool', 'operation': '5 + 3.5'}\n",
      "--- End of Add Tool ---\n",
      "\n",
      "üîß Using tool: get_weather\n",
      "   Description: Get current weather for a city\n",
      "\n",
      "--- Weather Tool Response ---\n",
      "{'content': [{'annotations': None,\n",
      "              'text': 'Error executing tool get_weather: 1 validation error '\n",
      "                      'for get_weatherArguments\\n'\n",
      "                      'city\\n'\n",
      "                      '  Field required [type=missing, '\n",
      "                      \"input_value={'location': 'San Francisco'}, \"\n",
      "                      'input_type=dict]\\n'\n",
      "                      '    For further information visit '\n",
      "                      'https://errors.pydantic.dev/2.10/v/missing',\n",
      "              'type': 'text'}],\n",
      " 'isError': True,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Getting weather information', 'location': 'San Francisco'}\n",
      "--- End of Weather Tool ---\n",
      "\n",
      "==================== STEP: Resource Access ====================\n",
      "Description: Reading data from MCP resources\n",
      "\n",
      "\n",
      "üìÑ Accessing resource: config://settings\n",
      "   Name: config://settings\n",
      "   Description: None\n",
      "\n",
      "--- Config Resource Response ---\n",
      "{'contents': [{'mimeType': 'text/plain',\n",
      "               'text': '{\\n'\n",
      "                       '  \"type\": \"text\",\\n'\n",
      "                       '  \"text\": \"{\\\\\"version\\\\\": \\\\\"1.0\\\\\", \\\\\"status\\\\\": '\n",
      "                       '\\\\\"active\\\\\"}\",\\n'\n",
      "                       '  \"annotations\": null\\n'\n",
      "                       '}',\n",
      "               'uri': AnyUrl('config://settings')}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Configuration data resource'}\n",
      "--- End of Config Resource ---\n",
      "\n",
      "==================== STEP: Prompt Usage ====================\n",
      "Description: Getting and using MCP prompts\n",
      "\n",
      "\n",
      "üí¨ Using prompt: system_prompt\n",
      "   Description: System prompt template for chat responses for one of two modes.\n",
      "\n",
      "        Mode 1: Translate user query to pirate language.\n",
      "        Mode 2: Answer the user query in pirate language.\n",
      "\n",
      "        Args:\n",
      "            user_query (str): The user's query.\n",
      "            mode (str): The mode of operation. Can be 'translate' or 'answer'.\n",
      "\n",
      "        Returns:\n",
      "            A list containing system instruction string and message objects.\n",
      "        \n",
      "\n",
      "   Trying mode: translate\n",
      "\n",
      "--- System Prompt (mode=translate) Response ---\n",
      "{'description': None,\n",
      " 'messages': [{'content': {'annotations': None,\n",
      "                           'text': 'You are a helpful assistant who is fluent '\n",
      "                                   \"in pirate language. Please 'translate' the \"\n",
      "                                   'following text.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'Hello, how are you today?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'Ahoy there, how be ye this fine day, '\n",
      "                                   'matey?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'I need to find a restaurant nearby.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': \"Arr, I be needin' to find a place to fill \"\n",
      "                                   'me belly nearby, savvy?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': \"What's the weather like today?\",\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Prompt in translate mode',\n",
      " 'message_count': 6,\n",
      " 'parameters': {'mode': 'translate',\n",
      "                'user_query': \"What's the weather like today?\"}}\n",
      "--- End of System Prompt (mode=translate) ---\n",
      "\n",
      "==================== STEP: LLM Integration ====================\n",
      "Description: Using MCP data with Language Model\n",
      "\n",
      "2025-05-12 16:50:05,558 - ATHON - DEBUG - Prompt generated Arrr, matey! The treasure ye seek be 50 doubloons! A shiny haul indeed! ‚öìÔ∏èü™ô\n",
      "\n",
      "--- LLM Integration Example Response ---\n",
      "{'explanation': 'Shows how MCP data can be enhanced with LLM processing',\n",
      " 'llm_enhancement': 'Arrr, matey! The treasure ye seek be 50 doubloons! A '\n",
      "                    'shiny haul indeed! ‚öìÔ∏èü™ô',\n",
      " 'mcp_result': {'content': [{'annotations': None,\n",
      "                             'text': '50.0',\n",
      "                             'type': 'text'}],\n",
      "                'isError': False,\n",
      "                'meta': None}}\n",
      "--- End of LLM Integration Example ---\n",
      "\n",
      "==================================================\n",
      "‚úÖ MCP Demo Complete!\n",
      "==================================================\n",
      "\n",
      "--- Server Logs ---\n",
      "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 25ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 78ms\u001b[0m\u001b[0m\n",
      "[05/12/25 16:50:04] INFO     Processing request of type            server.py:545\n",
      "                             ListToolsRequest                                   \n",
      "                    INFO     Processing request of type            server.py:545\n",
      "                             ListResourcesRequest                               \n",
      "                    INFO     Processing request of type            server.py:545\n",
      "                             ListPromptsRequest                                 \n",
      "                    INFO     Processing request of type            server.py:545\n",
      "                             CallToolRequest                                    \n",
      "                    INFO     Processing request of type            server.py:545\n",
      "                             CallToolRequest                                    \n",
      "                    INFO     Processing request of type            server.py:545\n",
      "                             ReadResourceRequest                                \n",
      "                    INFO     Processing request of type            server.py:545\n",
      "                             GetPromptRequest                                   \n",
      "                    INFO     Processing request of type            server.py:545\n",
      "                             CallToolRequest                                    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the STDIO demo\"\"\"\n",
    "stdio_client_command = \"uv run lablib/mcp/stdio/client.py\"\n",
    "client_result = sp.run(stdio_client_command, shell=True, capture_output=True, text=True)\n",
    "print(client_result.stdout)\n",
    "if client_result.stderr:\n",
    "    print(\"--- Server Logs ---\")\n",
    "    print(client_result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdfe8d",
   "metadata": {},
   "source": [
    "## SSE Transport (Deprecated)\n",
    "\n",
    "Server-Sent Events (SSE) was originally used in MCP as part of the \"HTTP+SSE transport\" protocol, which has since been deprecated and replaced by the Streamable HTTP transport.\n",
    "\n",
    "### Important Note\n",
    "\n",
    "- The standalone \"HTTP+SSE transport\" from protocol version 2024-11-05 has been **deprecated**\n",
    "- SSE functionality has been **absorbed into** the new Streamable HTTP transport (as of protocol version 2025-03-26)\n",
    "- The Python SDK currently only supports SSE transport\n",
    "- This section covers the SSE implementation as used in current Python MCP implementations\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Server-to-Client Streaming**: Enables servers to push updates to clients over HTTP\n",
    "- **Event-Based Communication**: Uses SSE event format with `event`, `data`, and `id` fields\n",
    "- **HTTP-Compatible**: Works over standard HTTP connections\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Client establishes an HTTP connection requesting SSE stream\n",
    "2. Server sends initial `endpoint` event with connection details\n",
    "3. Server streams JSON-RPC messages as SSE events\n",
    "4. Client processes messages as they arrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2029b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Clearing port 8000...\n",
      "BackgroundProcessManager: No processes found using port 8000\n",
      "BackgroundProcessManager: Port 8000 was already clear.\n",
      "Starting SSE server: uv run lablib/mcp/sse/server.py\n",
      "BackgroundProcessManager: Started 'sse_server' (PID: 23841, PGID: 23841). Command: uv run lablib/mcp/sse/server.py\n",
      "BackgroundProcessManager: Process 'sse_server' started successfully. PID: 23841\n",
      "Running SSE client: uv run lablib/mcp/sse/client.py\n",
      "--- SSE Client Output ---\n",
      "2025-05-12 16:50:23,864 - ATHON - DEBUG - Selected Langchain ChatOpenAI\n",
      "Connecting to SSE server at http://localhost:8000/math/sse...\n",
      "==================================================\n",
      "MCP Client-Server Demonstration\n",
      "==================================================\n",
      "\n",
      "==================== STEP: Initialization ====================\n",
      "Description: Establishing connection with MCP server\n",
      "\n",
      "‚úÖ Successfully connected to MCP server\n",
      "\n",
      "==================== STEP: Discovery ====================\n",
      "Description: Finding what the server offers\n",
      "\n",
      "üîß Found 3 tools: ['add', 'get_weather', 'add_two']\n",
      "üìÑ Found 1 resources: [AnyUrl('config://settings')]\n",
      "üí¨ Found 1 prompts: ['system_prompt']\n",
      "\n",
      "==================== STEP: Tool Usage ====================\n",
      "Description: Calling different MCP tools to show functionality\n",
      "\n",
      "\n",
      "üîß Using tool: add\n",
      "   Description: Add two numbers\n",
      "\n",
      "--- Add Tool Response ---\n",
      "{'content': [{'annotations': None, 'text': '8.5', 'type': 'text'}],\n",
      " 'isError': False,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Basic addition using MCP tool', 'operation': '5 + 3.5'}\n",
      "--- End of Add Tool ---\n",
      "\n",
      "üîß Using tool: get_weather\n",
      "   Description: Get current weather for a city\n",
      "\n",
      "--- Weather Tool Response ---\n",
      "{'content': [{'annotations': None,\n",
      "              'text': 'Error executing tool get_weather: 1 validation error '\n",
      "                      'for get_weatherArguments\\n'\n",
      "                      'city\\n'\n",
      "                      '  Field required [type=missing, '\n",
      "                      \"input_value={'location': 'San Francisco'}, \"\n",
      "                      'input_type=dict]\\n'\n",
      "                      '    For further information visit '\n",
      "                      'https://errors.pydantic.dev/2.10/v/missing',\n",
      "              'type': 'text'}],\n",
      " 'isError': True,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Getting weather information', 'location': 'San Francisco'}\n",
      "--- End of Weather Tool ---\n",
      "\n",
      "üîß Using tool: add_two\n",
      "   Description: A simple add tool\n",
      "\n",
      "--- Add Two Tool Response ---\n",
      "{'content': [{'annotations': None, 'text': '2 + 2 = 4', 'type': 'text'}],\n",
      " 'isError': False,\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Another addition tool variant', 'operation': '10 + 7'}\n",
      "--- End of Add Two Tool ---\n",
      "\n",
      "==================== STEP: Resource Access ====================\n",
      "Description: Reading data from MCP resources\n",
      "\n",
      "\n",
      "üìÑ Accessing resource: config://settings\n",
      "   Name: config://settings\n",
      "   Description: None\n",
      "\n",
      "--- Config Resource Response ---\n",
      "{'contents': [{'mimeType': 'text/plain',\n",
      "               'text': '{\\n'\n",
      "                       '  \"type\": \"text\",\\n'\n",
      "                       '  \"text\": \"{\\\\\"version\\\\\": \\\\\"1.0\\\\\", \\\\\"status\\\\\": '\n",
      "                       '\\\\\"active\\\\\"}\",\\n'\n",
      "                       '  \"annotations\": null\\n'\n",
      "                       '}',\n",
      "               'uri': AnyUrl('config://settings')}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Configuration data resource'}\n",
      "--- End of Config Resource ---\n",
      "\n",
      "==================== STEP: Prompt Usage ====================\n",
      "Description: Getting and using MCP prompts\n",
      "\n",
      "\n",
      "üí¨ Using prompt: system_prompt\n",
      "   Description: System prompt template for chat responses for one of two modes.\n",
      "\n",
      "        Mode 1: Translate user query to pirate language.\n",
      "        Mode 2: Answer the user query in pirate language.\n",
      "\n",
      "        Args:\n",
      "            user_query (str): The user's query.\n",
      "            mode (str): The mode of operation. Can be 'translate' or 'answer'.\n",
      "\n",
      "        Returns:\n",
      "            A list containing system instruction string and message objects.\n",
      "        \n",
      "\n",
      "   Trying mode: translate\n",
      "\n",
      "--- System Prompt (mode=translate) Response ---\n",
      "{'description': None,\n",
      " 'messages': [{'content': {'annotations': None,\n",
      "                           'text': 'You are a helpful assistant who is fluent '\n",
      "                                   \"in pirate language. Please 'translate' the \"\n",
      "                                   'following text.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'Hello, how are you today?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'Ahoy there, how be ye this fine day, '\n",
      "                                   'matey?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': 'I need to find a restaurant nearby.',\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': \"Arr, I be needin' to find a place to fill \"\n",
      "                                   'me belly nearby, savvy?',\n",
      "                           'type': 'text'},\n",
      "               'role': 'assistant'},\n",
      "              {'content': {'annotations': None,\n",
      "                           'text': \"What's the weather like today?\",\n",
      "                           'type': 'text'},\n",
      "               'role': 'user'}],\n",
      " 'meta': None}\n",
      "\n",
      "--- Additional Information ---\n",
      "{'explanation': 'Prompt in translate mode',\n",
      " 'message_count': 6,\n",
      " 'parameters': {'mode': 'translate',\n",
      "                'user_query': \"What's the weather like today?\"}}\n",
      "--- End of System Prompt (mode=translate) ---\n",
      "\n",
      "==================== STEP: LLM Integration ====================\n",
      "Description: Using MCP data with Language Model\n",
      "\n",
      "2025-05-12 16:50:25,425 - ATHON - DEBUG - Prompt generated Arrr, matey! The treasure ye seek be a grand total of 50 doubloons! Avast, count 'em well, for they be worth their weight in gold!\n",
      "\n",
      "--- LLM Integration Example Response ---\n",
      "{'explanation': 'Shows how MCP data can be enhanced with LLM processing',\n",
      " 'llm_enhancement': 'Arrr, matey! The treasure ye seek be a grand total of 50 '\n",
      "                    \"doubloons! Avast, count 'em well, for they be worth their \"\n",
      "                    'weight in gold!',\n",
      " 'mcp_result': {'content': [{'annotations': None,\n",
      "                             'text': '50.0',\n",
      "                             'type': 'text'}],\n",
      "                'isError': False,\n",
      "                'meta': None}}\n",
      "--- End of LLM Integration Example ---\n",
      "\n",
      "==================================================\n",
      "‚úÖ MCP Demo Complete!\n",
      "==================================================\n",
      "\n",
      "-------------------------\n",
      "Killing SSE server...\n",
      "BackgroundProcessManager: Attempting to terminate 'sse_server' (PID: 23841)...\n",
      "BackgroundProcessManager: Process group for 'sse_server' (PGID: 23841) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'sse_server'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the SSE demo\"\"\"\n",
    "# Clear port 8000 before starting the server\n",
    "clear_port(8000)\n",
    "\n",
    "sse_server_command = \"uv run lablib/mcp/sse/server.py\"\n",
    "sse_client_command = \"uv run lablib/mcp/sse/client.py\"\n",
    "\n",
    "print(f\"Starting SSE server: {sse_server_command}\")\n",
    "server_proc = start_background_process(\"sse_server\", sse_server_command)\n",
    "\n",
    "if server_proc and server_proc.poll() is None:\n",
    "    print(f\"Running SSE client: {sse_client_command}\")\n",
    "    client_result = sp.run(sse_client_command, shell=True, capture_output=True, text=True)\n",
    "    print(\"--- SSE Client Output ---\")\n",
    "    print(client_result.stdout)\n",
    "    if client_result.stderr:\n",
    "        print(\"--- SSE Client Errors ---\")\n",
    "        print(client_result.stderr)\n",
    "    print(\"-------------------------\")\n",
    "else:\n",
    "    print(\"SSE server failed to start or exited prematurely. Client will not run.\")\n",
    "\n",
    "print(\"Killing SSE server...\")\n",
    "kill_background_process(\"sse_server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d436b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SSE server: uv run --active lablib/mcp/sse/server.py\n",
      "BackgroundProcessManager: Started 'sse_server' (PID: 24019, PGID: 24019). Command: uv run --active lablib/mcp/sse/server.py\n",
      "BackgroundProcessManager: Process 'sse_server' started successfully. PID: 24019\n",
      "Add the following to your llmesh/.vscode/mcp.json file:\n",
      "\n",
      "        \"my-test-mcp\": {\n",
      "            \"url\": \"http://localhost:8000/math/sse\"\n",
      "        }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the SSE Copilot demo\"\"\"\n",
    "sse_server_command = \"uv run --active lablib/mcp/sse/server.py\"\n",
    "\n",
    "print(f\"Starting SSE server: {sse_server_command}\")\n",
    "server_proc = start_background_process(\"sse_server\", sse_server_command)\n",
    "\n",
    "output = \"\"\"\n",
    "        \"my-test-mcp\": {\n",
    "            \"url\": \"http://localhost:8000/math/sse\"\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Add the following to your llmesh/.vscode/mcp.json file:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaf418fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually killing sse server...\n",
      "BackgroundProcessManager: Attempting to terminate 'sse_server' (PID: 24019)...\n",
      "BackgroundProcessManager: Process group for 'sse_server' (PGID: 24019) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'sse_server'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cleanup sse process\"\"\"\n",
    "print(\"Manually killing sse server...\")\n",
    "kill_background_process(\"sse_server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb4c98",
   "metadata": {},
   "source": [
    "## Streamable HTTP Transport\n",
    "\n",
    "The Streamable HTTP transport is the current standard MCP transport mechanism, replacing the deprecated HTTP+SSE transport.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Current Standard**: The recommended transport mechanism for MCP\n",
    "- **HTTP-Based**: Uses standard HTTP POST and GET requests\n",
    "- **Optional Streaming**: Server can optionally use Server-Sent Events (SSE) for streaming responses\n",
    "- **Stateless or Stateful**: Supports both basic stateless servers and more feature-rich servers with sessions\n",
    "- **Multiple Connections**: Clients can maintain multiple concurrent connections\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Single Endpoint**: Server provides one HTTP endpoint that supports both POST and GET methods\n",
    "2. **Sending Messages**: \n",
    "   - Client sends JSON-RPC messages via HTTP POST to the MCP endpoint\n",
    "   - Client must include `Accept` header with both `application/json` and `text/event-stream`\n",
    "3. **Server Response Options**:\n",
    "   - **Simple Response**: Return `Content-Type: application/json` with a single JSON object\n",
    "   - **Streaming Response**: Return `Content-Type: text/event-stream` to initiate SSE streaming\n",
    "4. **Receiving Messages**: \n",
    "   - Client can issue HTTP GET to open SSE streams for server-initiated messages\n",
    "   - Server may send JSON-RPC requests and notifications via SSE\n",
    "5. **Session Management**: Server may optionally assign session IDs for stateful interactions\n",
    "\n",
    "**Note**: The MCP Inspector (shown below) provides a TypeScript client for testing Streamable HTTP connections.\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/inspector.png\" alt=\"Inspector screen shot\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10bf05e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackgroundProcessManager: Manually killing all registered processes...\n",
      "BackgroundProcessManager: Manual cleanup attempt complete.\n",
      "BackgroundProcessManager: Clearing port 8000...\n",
      "BackgroundProcessManager: No processes found using port 8000\n",
      "BackgroundProcessManager: Port 8000 was already clear.\n",
      "BackgroundProcessManager: Clearing port 6274...\n",
      "BackgroundProcessManager: No processes found using port 6274\n",
      "BackgroundProcessManager: Port 6274 was already clear.\n",
      "BackgroundProcessManager: Started 'mcp_server' (PID: 24050, PGID: 24050). Command: uv run lablib/mcp/streamable/server.py\n",
      "BackgroundProcessManager: Process 'mcp_server' started successfully. PID: 24050\n",
      "BackgroundProcessManager: Started 'mcp_inspector' (PID: 24052, PGID: 24052). Command: npx @modelcontextprotocol/inspector\n",
      "BackgroundProcessManager: Process 'mcp_inspector' started successfully. PID: 24052\n",
      "Starting MCP inspector and server...\n",
      "MCP inspector UI available at http://localhost:6274/\n",
      "MCP Streamable HTTP demo server available at http://localhost:8000/math/mcp\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run the HTTP demo\"\"\"\n",
    "\n",
    "# Ensure background processes are killed before starting new ones\n",
    "kill_all_background_processes()\n",
    "\n",
    "# Ensure ports are clear before starting the server\n",
    "clear_port(8000)\n",
    "clear_port(6274)\n",
    "\n",
    "# Start the HTTP server\n",
    "start_background_process(\"mcp_server\", \"uv run lablib/mcp/streamable/server.py\")\n",
    "\n",
    "# Start the mcp inspector\n",
    "start_background_process(\"mcp_inspector\", \"npx @modelcontextprotocol/inspector\")\n",
    "\n",
    "print(\"Starting MCP inspector and server...\")\n",
    "sleep(5)\n",
    "print(\"MCP inspector UI available at http://localhost:6274/\")\n",
    "print(\"MCP Streamable HTTP demo server available at http://localhost:8000/math/mcp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfeaba8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually terminating MCP servers...\n",
      "BackgroundProcessManager: Attempting to terminate 'mcp_server' (PID: 24050)...\n",
      "BackgroundProcessManager: Process group for 'mcp_server' (PGID: 24050) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'mcp_server'.\n",
      "BackgroundProcessManager: Attempting to terminate 'mcp_inspector' (PID: 24052)...\n",
      "BackgroundProcessManager: Process group for 'mcp_inspector' (PGID: 24052) terminated gracefully (SIGTERM).\n",
      "BackgroundProcessManager: Successfully initiated termination for 'mcp_inspector'.\n"
     ]
    }
   ],
   "source": [
    "# Manually kill streamable-http server and MCP inspector\n",
    "# Will be automatically killed on kernel restart/exit\n",
    "print(\"Manually terminating MCP servers...\")\n",
    "kill_background_process(\"mcp_server\")\n",
    "kill_background_process(\"mcp_inspector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a075311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually terminating ALL lablib-managed background processes...\n",
      "BackgroundProcessManager: Manually killing all registered processes...\n",
      "BackgroundProcessManager: Manual cleanup attempt complete.\n"
     ]
    }
   ],
   "source": [
    "# To kill ALL processes managed by lablib.util (if you started others):\n",
    "print(\"Manually terminating ALL lablib-managed background processes...\")\n",
    "kill_all_background_processes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
