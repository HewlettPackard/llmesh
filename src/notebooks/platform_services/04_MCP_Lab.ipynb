{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1051b6",
   "metadata": {},
   "source": [
    "# Lab: Model Context Protocol (MCP)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Model Context Protocol (MCP) provides a standardized approach for communication between AI-powered applications, particularly Large Language Models (LLMs). As LLM models become more sophisticated, capable of understanding complex **prompts**, utilizing external **tools**, and accessing diverse **resources**, MCP aims to simplify these interactions by offering a consistent interface.\n",
    "\n",
    "This lab explores MCP's fundamental concepts, including its architecture and core components for managing the flow of **prompts**, data, and instructions for tool use. You'll learn how MCP functions over different transport mechanisms to facilitate structured and capable interactions with AI models.\n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "\n",
    "MCP is designed to define a clear structure for managing LLM **prompts**, contextual information (which can include references to **tools** and **resources**), and data exchange when an application (a client) communicates with a model service (a server). This is not the complete specification of the protocol, but a simplified overview to illustrate the key concepts.\n",
    "\n",
    "Key objectives and potential benefits of using a protocol like MCP include:\n",
    "\n",
    "* **Improved Interoperability**: Facilitating connections between applications and a variety of model backends.\n",
    "* **Standardized Patterns**: Promoting common communication methods for leveraging **prompts**, requesting **tool** executions, and receiving model-generated content.\n",
    "* **Rich Context & Instruction Handling**: Providing defined ways to pass not just conversational history, but also detailed **prompts**, and specifications for how models should use available **tools** or access **resources**.\n",
    "* **Extensibility**: Allowing for future enhancements or specialized data exchanges within the protocol's framework.\n",
    "\n",
    "MCP adapts established communication principles to the specific needs of AI model interactions, such as managing conversational context, interpreting detailed **prompts**, orchestrating **tool** use, and optionally streaming data.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "MCP interactions use a client-server architecture:\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/mcp-arch.png\" alt=\"MCP Architecture\" width=\"700\">\n",
    "</div>\n",
    "\n",
    "1.  **Client**: An application that consumes model services. It forms and sends requests (containing **prompts**, data, and potentially **tool** calls) to the MCP server and processes the received responses.\n",
    "2.  **Server**: A service that exposes one or more AI models via MCP. It listens for client requests, interprets **prompts**, manages **tool** execution or **resource** access as defined by the protocol, interacts with the model(s), and returns responses according to the protocol.\n",
    "3.  **Transport Layer**: The underlying mechanism for message transmission. MCP defines the structure and interaction flow, with flexibility in transport mechanisms. MCP supports:\n",
    "    * Standard Input/Output (STDIO)\n",
    "    * Streamable HTTP (with optional Server-Sent Events)\n",
    "\n",
    "This separation allows flexibility in choosing a transport suitable for the application's environment.\n",
    "\n",
    "#### Components\n",
    "\n",
    "MCP interactions involve several key elements and types of information:\n",
    "\n",
    "1.  **Messages**: Defined units of information exchanged between client and server using JSON-RPC format. Common patterns include:\n",
    "    * Request messages from client to server\n",
    "    * Response messages from server to client  \n",
    "    * Notification messages that don't require responses\n",
    "    * Error messages for communicating issues\n",
    "\n",
    "2.  **Prompts**: The core input that guides the model's behavior. MCP facilitates the clear transmission of prompts from the client to the server.\n",
    "\n",
    "3.  **Tools & Resources**: Mechanisms by which a model can interact with external systems or data:\n",
    "    * **Tools**: Pre-defined functions or capabilities that the model can be instructed to use (e.g., calculators, search engines, database query functions)\n",
    "    * **Resources**: External data sources or knowledge bases that the model might need to access to fulfill a request\n",
    "    * *Note: The specifics of how tools and resources are defined and invoked can vary between MCP implementations*\n",
    "\n",
    "#### Protocol (Interaction Flow)\n",
    "\n",
    "A typical interaction sequence in MCP includes:\n",
    "\n",
    "1.  **Connection**: The client connects to the MCP server via the chosen transport mechanism.\n",
    "2.  **Initialization**: Client and server exchange initialization messages to establish capabilities and protocol version.\n",
    "3.  **Request**: The client sends a request message with the **prompt**, relevant context, parameters, and any **tool** or **resource** specifications.\n",
    "4.  **Processing**: The server processes the request, which may involve:\n",
    "    * Interpreting the prompt\n",
    "    * Invoking tools as specified\n",
    "    * Accessing resources as needed\n",
    "    * Generating model responses\n",
    "5.  **Response**: The server returns the response, either as a complete message or (if using Streamable HTTP) potentially as a stream of partial responses.\n",
    "6.  **Error Handling**: If issues arise, appropriate error messages are exchanged.\n",
    "7.  **Disconnection**: The connection may be closed or kept alive for subsequent interactions, depending on the transport and use case.\n",
    "\n",
    "This structured approach helps create more predictable and maintainable integrations with AI models, encompassing complex interactions involving prompts, tools, and resources while maintaining flexibility in implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8976a",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    "- Understand the core principles of the Model Context Protocol (MCP) and why it's valuable for LLM applications\n",
    "- Explore different transport mechanisms for MCP implementation (STDIO, SSE, and HTTP)\n",
    "- Implement basic MCP clients and servers using Python\n",
    "- Learn how to integrate LLMs with MCP\n",
    "- Gain practical experience with standardized protocols for AI communication\n",
    "\n",
    "This lab provides hands-on experience with an emerging standard in the AI ecosystem, giving you the skills to build more interoperable and flexible LLM-powered applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e09592",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "The following bullets must be ran prior to executing notebooks for running this lab:\n",
    "  1. uv installed and available on PATH\n",
    "      - Linux/MacOS:\n",
    "          - `curl -sSL https://get.uv.dev | sh`\n",
    "          - `source ~/.bashrc`\n",
    "          - `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
    "          - `source $HOME/.local/bin/env`\n",
    "\n",
    "      - Windows:\n",
    "          - `powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"`\n",
    "          - Completley close and re-open VSCode, essentailly just restart terminal.\n",
    "              - In my experience doing a terminal restart without closing VSCode does not work.\n",
    "\n",
    "  2. Python 3.12 installed and venv created\n",
    "      - `uv python install 3.12`\n",
    "      - `uv venv -p 3.12`\n",
    "      - Activate the venv:\n",
    "          - Linux/MacOS:\n",
    "              - `source .venv/bin/activate`\n",
    "          - Windows:\n",
    "              - `.\\.venv\\Scripts\\activate`\n",
    "      - `uv pip install ipykernel`\n",
    "\n",
    "  3. (Optional) In order to run the final labe section you need npx installed which requires Node.js (and npm if on linux)\n",
    "      - Windows:\n",
    "          - Simply use installer and include extra tools checkbox when installing Node.js https://nodejs.org/en/download.\n",
    "      - Linux/MacOS:\n",
    "          - Google how to install Node.js on your OS as there are many different ways to do it.\n",
    "\n",
    "  4. Select the venv as the notebook kernel\n",
    "  <div align=\"left\">\n",
    "    <img src=\"pictures/kernel.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "  </div>\n",
    "\n",
    "  \n",
    "\n",
    "**MUST restart Juypter kernel if automated install dependencies cell is ran**\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/restart.png\" alt=\"VSCode Juypter UI hint\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafda488",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Install platform chat & mcp dependencies.\"\"\"\n",
    "!cd ../../.. && uv pip install --quiet -e .[chat] mcp\n",
    "# Shut down the kernel so user must restart it to apply new pip installations.\n",
    "# This is a workaround for the fact that Jupyter does not automatically\n",
    "# pick up new installations in the current kernel.\n",
    "!echo \"Kernel will shut down to apply new pip installations, manual restart required.\"\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup background process manager for sse/http server demos.\"\"\"\n",
    "from src.notebooks.platform_services.lablib.bg_util import clear_port, start_background_process, kill_background_process, kill_all_background_processes\n",
    "from time import sleep\n",
    "import subprocess as sp\n",
    "print(\"lablib.util process management functions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set up the lab chat model.\"\"\"\n",
    "from src.notebooks.platform_services.lablib.env_util import set_services_env\n",
    "\n",
    "_, _, _ = set_services_env()\n",
    "\n",
    "print(\"Chat env initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdd7bf",
   "metadata": {},
   "source": [
    "## STDIO Transport\n",
    "\n",
    "The STDIO (Standard Input/Output) transport is the primary MCP implementation that clients should support. It uses standard input and output streams to exchange MCP messages between client and server processes.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Primary Transport**: Clients SHOULD support STDIO whenever possible\n",
    "- **Subprocess Communication**: The client launches the MCP server as a subprocess\n",
    "- **Newline-Delimited Messages**: JSON-RPC messages are separated by newlines\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Process Launch**: The client launches the MCP server as a subprocess\n",
    "2. **Message Exchange**: \n",
    "   - The server reads JSON-RPC messages from its standard input (stdin)\n",
    "   - The server sends messages to its standard output (stdout)\n",
    "   - Messages are delimited by newlines and MUST NOT contain embedded newlines\n",
    "3. **Message Types**: Messages may be JSON-RPC requests, notifications, responses, or batched messages containing multiple requests/notifications\n",
    "4. **Logging**: The server MAY write UTF-8 strings to its standard error (stderr) for logging purposes, which clients MAY capture, forward, or ignore\n",
    "5. **Protocol Compliance**: \n",
    "   - The server MUST NOT write anything to stdout that is not a valid MCP message\n",
    "   - The client MUST NOT write anything to the server's stdin that is not a valid MCP message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run the STDIO demo\"\"\"\n",
    "stdio_client_command = \"uv run lablib/mcp/stdio/client.py\"\n",
    "client_result = sp.run(stdio_client_command, shell=True, capture_output=True, text=True)\n",
    "print(client_result.stdout)\n",
    "if client_result.stderr:\n",
    "    print(\"--- Server Logs ---\")\n",
    "    print(client_result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdfe8d",
   "metadata": {},
   "source": [
    "## SSE Transport (Deprecated)\n",
    "\n",
    "Server-Sent Events (SSE) was originally used in MCP as part of the \"HTTP+SSE transport\" protocol, which has since been deprecated and replaced by the Streamable HTTP transport.\n",
    "\n",
    "### Important Note\n",
    "\n",
    "- The standalone \"HTTP+SSE transport\" from protocol version 2024-11-05 has been **deprecated**\n",
    "- SSE functionality has been **absorbed into** the new Streamable HTTP transport (as of protocol version 2025-03-26)\n",
    "- The Python SDK currently only supports SSE transport\n",
    "- This section covers the SSE implementation as used in current Python MCP implementations\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Server-to-Client Streaming**: Enables servers to push updates to clients over HTTP\n",
    "- **Event-Based Communication**: Uses SSE event format with `event`, `data`, and `id` fields\n",
    "- **HTTP-Compatible**: Works over standard HTTP connections\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Client establishes an HTTP connection requesting SSE stream\n",
    "2. Server sends initial `endpoint` event with connection details\n",
    "3. Server streams JSON-RPC messages as SSE events\n",
    "4. Client processes messages as they arrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2029b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run the SSE demo\"\"\"\n",
    "# Clear port 8000 before starting the server\n",
    "clear_port(8000)\n",
    "\n",
    "sse_server_command = \"uv run lablib/mcp/sse/server.py\"\n",
    "sse_client_command = \"uv run lablib/mcp/sse/client.py\"\n",
    "\n",
    "print(f\"Starting SSE server: {sse_server_command}\")\n",
    "server_proc = start_background_process(\"sse_server\", sse_server_command)\n",
    "\n",
    "if server_proc and server_proc.poll() is None:\n",
    "    print(f\"Running SSE client: {sse_client_command}\")\n",
    "    client_result = sp.run(sse_client_command, shell=True, capture_output=True, text=True)\n",
    "    print(\"--- SSE Client Output ---\")\n",
    "    print(client_result.stdout)\n",
    "    if client_result.stderr:\n",
    "        print(\"--- SSE Client Errors ---\")\n",
    "        print(client_result.stderr)\n",
    "    print(\"-------------------------\")\n",
    "else:\n",
    "    print(\"SSE server failed to start or exited prematurely. Client will not run.\")\n",
    "\n",
    "print(\"Killing SSE server...\")\n",
    "kill_background_process(\"sse_server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run the SSE Copilot demo\"\"\"\n",
    "sse_server_command = \"uv run --active lablib/mcp/sse/server.py\"\n",
    "\n",
    "print(f\"Starting SSE server: {sse_server_command}\")\n",
    "server_proc = start_background_process(\"sse_server\", sse_server_command)\n",
    "\n",
    "output = \"\"\"\n",
    "        \"my-test-mcp\": {\n",
    "            \"url\": \"http://localhost:8000/math/sse\"\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Add the following to your llmesh/.vscode/mcp.json file:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf418fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cleanup sse process\"\"\"\n",
    "print(\"Manually killing sse server...\")\n",
    "kill_background_process(\"sse_server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb4c98",
   "metadata": {},
   "source": [
    "## Streamable HTTP Transport\n",
    "\n",
    "The Streamable HTTP transport is the current standard MCP transport mechanism, replacing the deprecated HTTP+SSE transport.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Current Standard**: The recommended transport mechanism for MCP\n",
    "- **HTTP-Based**: Uses standard HTTP POST and GET requests\n",
    "- **Optional Streaming**: Server can optionally use Server-Sent Events (SSE) for streaming responses\n",
    "- **Stateless or Stateful**: Supports both basic stateless servers and more feature-rich servers with sessions\n",
    "- **Multiple Connections**: Clients can maintain multiple concurrent connections\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Single Endpoint**: Server provides one HTTP endpoint that supports both POST and GET methods\n",
    "2. **Sending Messages**: \n",
    "   - Client sends JSON-RPC messages via HTTP POST to the MCP endpoint\n",
    "   - Client must include `Accept` header with both `application/json` and `text/event-stream`\n",
    "3. **Server Response Options**:\n",
    "   - **Simple Response**: Return `Content-Type: application/json` with a single JSON object\n",
    "   - **Streaming Response**: Return `Content-Type: text/event-stream` to initiate SSE streaming\n",
    "4. **Receiving Messages**: \n",
    "   - Client can issue HTTP GET to open SSE streams for server-initiated messages\n",
    "   - Server may send JSON-RPC requests and notifications via SSE\n",
    "5. **Session Management**: Server may optionally assign session IDs for stateful interactions\n",
    "\n",
    "**Note**: The MCP Inspector (shown below) provides a TypeScript client for testing Streamable HTTP connections.\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"pictures/inspector.png\" alt=\"Inspector screen shot\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf05e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run the HTTP demo\"\"\"\n",
    "\n",
    "# Ensure background processes are killed before starting new ones\n",
    "kill_all_background_processes()\n",
    "\n",
    "# Ensure ports are clear before starting the server\n",
    "clear_port(8000)\n",
    "clear_port(6274)\n",
    "\n",
    "# Start the HTTP server\n",
    "start_background_process(\"mcp_server\", \"uv run lablib/mcp/streamable/server.py\")\n",
    "\n",
    "# Start the mcp inspector\n",
    "start_background_process(\"mcp_inspector\", \"npx @modelcontextprotocol/inspector\")\n",
    "\n",
    "print(\"Starting MCP inspector and server...\")\n",
    "sleep(5)\n",
    "print(\"MCP inspector UI available at http://localhost:6274/\")\n",
    "print(\"MCP Streamable HTTP demo server available at http://localhost:8000/math/mcp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeaba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually kill streamable-http server and MCP inspector\n",
    "# Will be automatically killed on kernel restart/exit\n",
    "print(\"Manually terminating MCP servers...\")\n",
    "kill_background_process(\"mcp_server\")\n",
    "kill_background_process(\"mcp_inspector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To kill ALL processes managed by lablib.util (if you started others):\n",
    "print(\"Manually terminating ALL lablib-managed background processes...\")\n",
    "kill_all_background_processes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
